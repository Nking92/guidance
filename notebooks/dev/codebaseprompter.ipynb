{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "import os\n",
    "import tiktoken\n",
    "import re\n",
    "from guidance import models, gen, any_char, any_char_but, regex, substring, substring_no_empty, with_temperature, system, user, assistant\n",
    "from typing import Optional\n",
    "import nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str):\n",
    "    # Encoding for GPT-3 and later models\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text, disallowed_special=())\n",
    "    num_tokens = len(tokens)\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral = models.LlamaCpp(\"/Users/nicholasking/code/models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf\", n_gpu_layers=-1, n_ctx=4096)\n",
    "\n",
    "azure_model = os.getenv(\"AZUREAI_CHAT_MODEL\", \"Please set the model\")\n",
    "azure_endpoint = os.getenv(\"AZUREAI_CHAT_ENDPOINT\", \"Please set the endpoint\")\n",
    "azure_api_key=os.getenv(\"AZUREAI_CHAT_KEY\", \"Please set API key\")\n",
    "\n",
    "gpt4 = models.AzureOpenAI(\n",
    "    model=azure_model,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=azure_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_ipynb(notebook_file):\n",
    "    nb = nbformat.read(notebook_file, as_version=4)\n",
    "    extracted_text = \"\"\n",
    "    for cell in nb['cells']:\n",
    "        if cell['cell_type'] == 'code':\n",
    "            extracted_text += \"```python\\n\" + cell['source'] + \"\\n```\\n\\n\"\n",
    "        elif cell['cell_type'] == 'markdown':\n",
    "            extracted_text += \"```ipynb\\n\" + cell['source'] + \"\\n```\\n\\n\"\n",
    "    return extracted_text\n",
    "\n",
    "def include_exclude_check(file_name, include_file_regex=None, exclude_file_regex=None):\n",
    "    includes = include_file_regex is None or re.search(include_file_regex, file_name) is not None\n",
    "    excludes = exclude_file_regex is None or re.search(exclude_file_regex, file_name) is None\n",
    "    # print(\"include_exclude_check\", file_name, includes, excludes)\n",
    "    return includes and excludes\n",
    "\n",
    "def walk_and_match_files(start_path, include_file_regex=None, exclude_file_regex=None):\n",
    "    \"\"\"Walk through directories starting from start_path and collect files that match include_file_regex and don't match exclude_file_regex.\"\"\"\n",
    "    matched_files = []\n",
    "    for root, _, files in os.walk(start_path):\n",
    "        for file_name in files:\n",
    "            if include_exclude_check(file_name, include_file_regex, exclude_file_regex):\n",
    "                matched_files.append(os.path.join(root, file_name))\n",
    "    return matched_files\n",
    "\n",
    "def list_and_match_files(dir_path, include_file_regex=None, exclude_file_regex=None):\n",
    "    \"\"\"List files in dir_path and collect files that match include_file_regex and don't match exclude_file_regex.\"\"\"\n",
    "    matched_files = []\n",
    "    for file_name in os.listdir(dir_path):\n",
    "        if include_exclude_check(file_name, include_file_regex, exclude_file_regex):\n",
    "            matched_files.append(os.path.join(dir_path, file_name))\n",
    "    return matched_files\n",
    "\n",
    "def read_files(file_paths):\n",
    "    \"\"\"Read the contents of the files.\"\"\"\n",
    "    file_contents = {}\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            if file_path.endswith('.ipynb'):\n",
    "                file_contents[file_path] = extract_text_from_ipynb(file_path)\n",
    "            else:\n",
    "                file_contents[file_path] = f.read()\n",
    "    return file_contents\n",
    "\n",
    "def format_for_analysis(file_contents):\n",
    "    \"\"\"Format the contents for model prompting.\"\"\"\n",
    "    formatted_string = \"\"\n",
    "    for file_path, content in file_contents.items():\n",
    "        formatted_string += f\"## File: {file_path}\\n```{file_path.split('.')[-1]}\\n{content}\\n```\\n\\n\"\n",
    "    return formatted_string\n",
    "\n",
    "# Orchestrator\n",
    "def build_code_prompt(repo_tree_paths=None, repo_dir_paths=None, repo_file_paths=None, include_file_regex=None, exclude_file_regex=None):\n",
    "    \"\"\"Orchestrate the analysis of a repository.\"\"\"\n",
    "    all_file_paths = []\n",
    "    if repo_tree_paths is not None:\n",
    "        for start_path in repo_tree_paths:\n",
    "            all_file_paths.extend(walk_and_match_files(start_path, include_file_regex, exclude_file_regex))\n",
    "\n",
    "    if repo_dir_paths is not None:\n",
    "        for dir_path in repo_dir_paths:\n",
    "            all_file_paths.extend(list_and_match_files(dir_path, include_file_regex, exclude_file_regex))\n",
    "    \n",
    "    if repo_file_paths is not None:\n",
    "        all_file_paths.extend(repo_file_paths)\n",
    "    all_file_contents = read_files(all_file_paths)\n",
    "    formatted_code = format_for_analysis(all_file_contents)\n",
    "    prompt = f\"\"\"# Code Analysis\n",
    "Please analyze the code provided below.\n",
    "\n",
    "{formatted_code}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "@guidance\n",
    "def analyze_code(lm, code_prompt: str, system_prompt: Optional[str] = None, user_message: Optional[str] = None, **kwargs):\n",
    "    kwargs.setdefault('temperature', 0.8)\n",
    "    kwargs.setdefault('max_tokens', 1000)\n",
    "    if isinstance(lm, models.Chat):\n",
    "        if system_prompt is None:\n",
    "            system_prompt = \"Act as an expert software architect. Provide insights into code quality, potential issues, and suggestions for improvement. Answer the user's questions.\"\n",
    "        with system():\n",
    "            lm += system_prompt\n",
    "        with user():\n",
    "            lm += code_prompt\n",
    "            if user_message is not None:\n",
    "                lm += f\"\\n# User Message\\n{user_message}\"\n",
    "        with assistant():\n",
    "            lm += gen(**kwargs)\n",
    "    else:\n",
    "        lm += code_prompt\n",
    "        if system_prompt is not None:\n",
    "            lm += f\"\\n# Instructions\\n{system_prompt}\"\n",
    "        if user_message is not None:\n",
    "            lm += f\"\\n# User Message\\n{user_message}\"\n",
    "        # Set default temperature and max_tokens in kwargs\n",
    "        lm += gen(**kwargs)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code prompt has 81639 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "repo_tree_paths = ['/Users/nicholasking/code/ms/guidance/guidance']\n",
    "doc_paths = ['/Users/nicholasking/code/ms/guidance/README.md', '/Users/nicholasking/code/ms/guidance/notebooks/api_examples/models/OpenAI.ipynb', '/Users/nicholasking/code/ms/guidance/notebooks/tutorials/intro_to_guidance.ipynb', '/Users/nicholasking/code/ms/guidance/notebooks/anachronism.ipynb', '/Users/nicholasking/code/ms/guidance/notebooks/art_of_prompt_design/prompt_boundaries_and_token_healing.ipynb', '/Users/nicholasking/code/ms/guidance/notebooks/proverb.ipynb']\n",
    "match_file_regex = r'\\.(py|cpp|ipynb|md)$'\n",
    "exclude_file_regex = r'\\.(pyc|so|dll)$'\n",
    "\n",
    "code_prompt = build_code_prompt(repo_tree_paths=repo_tree_paths, repo_file_paths=doc_paths, include_file_regex=match_file_regex, exclude_file_regex=exclude_file_regex)\n",
    "\n",
    "print(f\"Code prompt has {count_tokens(code_prompt)} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Code Analysis\n",
      "Please analyze the code provided below.\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/_json_schema_to_grammar.py\n",
      "```py\n",
      "import json\n",
      "from typing import Dict\n",
      "\n",
      "from ._grammar import Byte, GrammarFunction, Join, Select, select\n",
      "from .library._char_range import char_range\n",
      "\n",
      "_QUOTE = Byte(b'\"')\n",
      "_SAFE_STRING = select(\n",
      "    [\n",
      "        char_range(\"a\", \"z\"),\n",
      "        char_range(\"A\", \"Z\"),\n",
      "        char_range(\"0\", \"9\"),\n",
      "        *[c for c in \"-_' ,.!?/[]{}():;\"],\n",
      "        \"\\\\n\",\n",
      "        \"\\\\t\",\n",
      "        \"\\\\\\\\\",\n",
      "    ],\n",
      "    recurse=True,\n",
      ")\n",
      "_OPEN_BRACE = Byte(b\"{\")\n",
      "_CLOSE_BRACE = Byte(b\"}\")\n",
      "_OPEN_BRACKET = Byte(b\"[\")\n",
      "_CLOSE_BRACKET = Byte(b\"]\")\n",
      "_COMMA = Byte(b\",\")\n",
      "_COLON = Byte(b\":\")\n",
      "\n",
      "\n",
      "def _make_optional(f: GrammarFunction) -> GrammarFunction:\n",
      "    return select([\"\", f])\n",
      "\n",
      "\n",
      "def _process_int() -> GrammarFunction:\n",
      "    return Join([select([\"-\", \"\"]), select([char_range(\"0\", \"9\")], recurse=True)])\n",
      "\n",
      "\n",
      "def _process_number() -> GrammarFunction:\n",
      "    mantissa_int = _process_int()\n",
      "    mantissa_frac = _make_optional(\n",
      "        Join([Byte(b\".\"), select([char_range(\"0\", \"9\")], recurse=True)])\n",
      "    )\n",
      "    exponent = _make_optional(\n",
      "        Join(\n",
      "            [\n",
      "                \"e\",\n",
      "                # Since the exponent can contain a '+', can't just reuse\n",
      "                # _process_int() here\n",
      "                select([\"\", \"-\", \"+\"]),\n",
      "                select([char_range(\"0\", \"9\")], recurse=True),\n",
      "            ]\n",
      "        )\n",
      "    )\n",
      "    return Join(\n",
      "        [\n",
      "            mantissa_int,\n",
      "            mantissa_frac,\n",
      "            exponent,\n",
      "        ],\n",
      "    )\n",
      "\n",
      "\n",
      "def _process_object(schema_properties: Dict[str, any]) -> GrammarFunction:\n",
      "    properties = []\n",
      "    for name, nxt_node in schema_properties.items():\n",
      "        nxt = Join(\n",
      "            [\n",
      "                Join([_QUOTE, name, _QUOTE]),\n",
      "                _COLON,\n",
      "                _process_node(nxt_node),\n",
      "                _COMMA if len(properties) + 1 < len(schema_properties) else \"\",\n",
      "            ]\n",
      "        )\n",
      "        properties.append(nxt)\n",
      "    return Join([_OPEN_BRACE, *properties, _CLOSE_BRACE])\n",
      "\n",
      "\n",
      "def _process_array(item_node: Dict[str, any]) -> GrammarFunction:\n",
      "    return Join(\n",
      "        [\n",
      "            _OPEN_BRACKET,\n",
      "            _make_optional(\n",
      "                # One or more items\n",
      "                Join(\n",
      "                    [\n",
      "                        select(\n",
      "                            [\"\", Join([_process_node(item_node), _COMMA])],\n",
      "                            recurse=True,\n",
      "                        ),\n",
      "                        _process_node(item_node),\n",
      "                    ]\n",
      "                )\n",
      "            ),\n",
      "            _CLOSE_BRACKET,\n",
      "        ]\n",
      "    )\n",
      "\n",
      "\n",
      "def _process_node(node: Dict[str, any]) -> GrammarFunction:\n",
      "    if node[\"type\"] == \"null\":\n",
      "        # Not completely sure about this\n",
      "        return Select([\"null\"])\n",
      "    elif node[\"type\"] == \"string\":\n",
      "        return Join([_QUOTE, _SAFE_STRING, _QUOTE])\n",
      "    elif node[\"type\"] == \"boolean\":\n",
      "        return select([\"true\", \"false\"])\n",
      "    elif node[\"type\"] == \"integer\":\n",
      "        return _process_int()\n",
      "    elif node[\"type\"] == \"number\":\n",
      "        return _process_number()\n",
      "    elif node[\"type\"] == \"object\":\n",
      "        return _process_object(node[\"properties\"])\n",
      "    elif node[\"type\"] == \"array\":\n",
      "        item_node = dict(type=node[\"items\"][\"type\"])\n",
      "        if item_node[\"type\"] == \"object\":\n",
      "            item_node[\"properties\"] = node[\"items\"][\"properties\"]\n",
      "        return _process_array(item_node)\n",
      "    else:\n",
      "        raise ValueError(f\"Unsupported type in schema: {node['type']}\")\n",
      "\n",
      "\n",
      "def json_schema_to_grammar(schema: str) -> GrammarFunction:\n",
      "    schema_obj = json.loads(schema)\n",
      "\n",
      "    return _process_node(schema_obj)\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/__init__.py\n",
      "```py\n",
      "__version__ = \"0.1.11\"\n",
      "\n",
      "import functools\n",
      "import sys\n",
      "import types\n",
      "import inspect\n",
      "\n",
      "from . import models\n",
      "from ._grammar import (Placeholder, RawFunction, GrammarFunction,\n",
      "                       Terminal, replace_grammar_node, string)\n",
      "from ._utils import strip_multiline_string_indents\n",
      "from ._server import Server\n",
      "\n",
      "newline = \"\\n\"\n",
      "\n",
      "# This makes the guidance module callable\n",
      "class Guidance(types.ModuleType):\n",
      "    def __call__(self, f=None, *, stateless=False, cache=None, dedent=True, model=models.Model):\n",
      "        return _decorator(f, stateless=stateless, cache=cache, dedent=dedent, model=model)\n",
      "sys.modules[__name__].__class__ = Guidance\n",
      "    \n",
      "_null_grammar = string('')\n",
      "\n",
      "def _decorator(f, *, stateless, cache, dedent, model):\n",
      "    \n",
      "    # if we are not yet being used as a decorator, then save the args\n",
      "    if f is None:\n",
      "        return functools.partial(_decorator, stateless=stateless, cache=cache, dedent=dedent, model=model)\n",
      "    \n",
      "    # if we are being used as a decorator then return the decorated function\n",
      "    else:\n",
      "\n",
      "        # this strips out indentation in multiline strings that aligns with the current python indentation\n",
      "        if dedent is True or dedent == 'python':\n",
      "            f = strip_multiline_string_indents(f)\n",
      "\n",
      "        # we cache if requested\n",
      "        if cache:\n",
      "            f = functools.cache(f)\n",
      "\n",
      "        @functools.wraps(f)\n",
      "        def wrapped(*args, **kwargs):\n",
      "\n",
      "            # make a stateless grammar if we can\n",
      "            if stateless is True or (callable(stateless) and stateless(*args, **kwargs)):\n",
      "                \n",
      "                # if we have a placeholder set then we must be in a recursive definition and so we return the placeholder\n",
      "                placeholder = getattr(f, \"_self_call_placeholder_\", None)\n",
      "                if placeholder is not None:\n",
      "                    return placeholder\n",
      "                \n",
      "                # otherwise we call the function to generate the grammar\n",
      "                else:\n",
      "                    \n",
      "                    # set a placeholder for recursive calls (only if we don't have arguments that might make caching a bad idea)\n",
      "                    no_args = len(args) + len(kwargs) == 0\n",
      "                    if no_args:\n",
      "                        f._self_call_placeholder_ = Placeholder()\n",
      "\n",
      "                    # call the function to get the grammar node\n",
      "                    node = f(_null_grammar, *args, **kwargs)\n",
      "                    if not isinstance(node, (Terminal, str)):\n",
      "                        node.name = f.__name__\n",
      "\n",
      "                    # replace all the placeholders with our generated node\n",
      "                    if no_args:\n",
      "                        replace_grammar_node(node, f._self_call_placeholder_, node)\n",
      "                        del f._self_call_placeholder_\n",
      "\n",
      "                    return node\n",
      "\n",
      "            # otherwise must be stateful (which means we can't be inside a select() call)\n",
      "            else:\n",
      "                return RawFunction(f, args, kwargs)\n",
      "\n",
      "        # Remove the first argument from the wrapped function\n",
      "        signature = inspect.signature(f)\n",
      "        params = list(signature.parameters.values())\n",
      "        params.pop(0)\n",
      "        wrapped.__signature__ = signature.replace(parameters=params)\n",
      "        \n",
      "        # attach this as a method of the model class (if given)\n",
      "        # if model is not None:\n",
      "        #     setattr(model, f.__name__, f)\n",
      "        \n",
      "        return wrapped\n",
      "\n",
      "# we expose all the library functions at the top level of the module\n",
      "from .library import *\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/selectors.py\n",
      "```py\n",
      "import random\n",
      "\n",
      "class Random():\n",
      "    def __init__(self, items, k=1):\n",
      "        ''' Create a selector that chooses a random set of instances.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        k : int\n",
      "            The number of instances to choose.\n",
      "        '''\n",
      "        self.items = items\n",
      "        self.k = k\n",
      "    \n",
      "    def __call__(self):\n",
      "        ''' Selects a random set of instances.\n",
      "        '''\n",
      "        return random.choice(self.items, k=self.k)\n",
      "    \n",
      "class NGramOverlap():\n",
      "    def __init__(self, items, k=1):\n",
      "        ''' Create a selector that chooses a random set of instances.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        k : int\n",
      "            The number of instances to choose.\n",
      "        '''\n",
      "        # TODO: Implement this.\n",
      "        # It would be nice just use let the LangChain option cover this, but they have some odd dependencies on an example template.\n",
      "        raise NotImplementedError(\"NGramOverlap is not implemented yet.\")\n",
      "    \n",
      "    def __call__(self):\n",
      "        ''' Selects a random set of instances.\n",
      "        '''\n",
      "        return self.items\n",
      "\n",
      "def _word_tokenizer(text):\n",
      "    ''' Tokenizes a string by white space.\n",
      "    '''\n",
      "    return text.split()\n",
      "\n",
      "class TokenLimit():\n",
      "    \n",
      "    def __init__(self, items, max_tokens=20):\n",
      "        ''' Create a selector that limits the number of tokens in a list of items.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        items : list\n",
      "            A list of items to select from.\n",
      "        max_tokens : int\n",
      "            The maximum number of tokens to allow.\n",
      "        '''\n",
      "\n",
      "        self.items = items\n",
      "        self.max_tokens = max_tokens\n",
      "    \n",
      "    def __call__(self, template_context=None):\n",
      "        ''' Filters a list of items to a maximum number of tokens.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        template_context : dict (optional)\n",
      "            A dictionary of template context variables to use for token counting.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        A list of items that fit within the token limit.\n",
      "        '''\n",
      "        \n",
      "        if template_context is not None and \"@tokenizer\" in template_context:\n",
      "            token_encoder = template_context[\"@tokenizer\"].encode\n",
      "        else:\n",
      "            token_encoder = _word_tokenizer\n",
      "        total_length = 0\n",
      "        out = []\n",
      "        for item in self.items:\n",
      "            if template_context is not None and \"@block_text\" in template_context:\n",
      "                context_new = template_context[\"@block_text\"]\n",
      "                if isinstance(item, dict):\n",
      "                    for k in item:\n",
      "                        context_new = context_new.replace(\"{{this.\"+k+\"}}\", item[k])\n",
      "                else:\n",
      "                    context_new = context_new.replace(\"{{this}}\", item)\n",
      "            else:\n",
      "                context_new = \" \".join([item[k] for k in item])\n",
      "            new_length = len(token_encoder(context_new))\n",
      "            if total_length + new_length <= self.max_tokens:\n",
      "                total_length += new_length\n",
      "                out.append(item)\n",
      "        return out\n",
      "    \n",
      "\n",
      "class LangChain():\n",
      "    def __init__(self, selector):\n",
      "        ''' Create a selector from a LangChain ExampleSelector object.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        selector : ExampleSelector\n",
      "            A LangChain ExampleSelector object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        A selector that selects examples using a LangChain ExampleSelector object.\n",
      "        '''\n",
      "        self.selector = selector\n",
      "    \n",
      "    def __call__(self, **kwargs):\n",
      "        ''' Select examples using a LangChain ExampleSelector object.\n",
      "\n",
      "        Note that we use keyword arguments here instead of a single dictionary.\n",
      "        '''\n",
      "        out = self.selector.select_examples(kwargs)\n",
      "        return out\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/_grammar.py\n",
      "```py\n",
      "import base64\n",
      "import uuid\n",
      "import json\n",
      "import inspect\n",
      "import types\n",
      "import re\n",
      "\n",
      "from typing import List, TypeVar, Union\n",
      "\n",
      "from . import _serialization_pb2\n",
      "from . import _parser\n",
      "\n",
      "_T = TypeVar(\"_T\")\n",
      "\n",
      "# to support the embedding of guidance functions inside Python f-strings we use tags with these delimiters\n",
      "tag_start = \"{{G|\" # start of a call tag\n",
      "tag_end = \"|G}}\" # end of a call tag\n",
      "_call_pool = {} # the functions associated with the call tags\n",
      "_tag_pattern = re.compile(re.escape(tag_start) + r\"([^\\|]+)\" + re.escape(tag_end)) # the pattern for matching call tags\n",
      "\n",
      "class StatefulException(Exception):\n",
      "    '''This is raised when we try and use the state of a grammar object like it was a live model.\n",
      "    \n",
      "    Note that eventually it would be nice to support stateful parser/grammar constructs directly, but\n",
      "    such \"parser combinators\" cannot be run effciently in Python. So we use a traditional parser and\n",
      "    grammar separation (hence the need for this exception).'''\n",
      "    pass\n",
      "\n",
      "class Function():\n",
      "    ''' This is the abstract class representing all guidance functions.\n",
      "    \n",
      "    There are two main subclasses: GrammarFunction and RawFunction. GrammarFunctions\n",
      "    represent guidance grammars that can be serialized and sent across the wire, while\n",
      "    RawFunctions represent unconstrained native Python functions.\n",
      "    '''\n",
      "    \n",
      "    def __init__(self, name, value=None) -> None:\n",
      "        self.name = name\n",
      "        self.value = value\n",
      "\n",
      "    def __str__(self):\n",
      "        '''Creates a string tag that can be used to retrieve this object.'''\n",
      "    \n",
      "        # save the call in our call pool, ready to be run when it is attached to an LM object\n",
      "        str_id = str(id(self))\n",
      "        if str_id not in _call_pool:\n",
      "            _call_pool[str_id] = self\n",
      "\n",
      "        # return a string representation of this call so it can be combined with other strings/calls\n",
      "        return tag_start + str_id + tag_end\n",
      "    \n",
      "    def serialize(self):\n",
      "        raise NotImplementedError()\n",
      "    \n",
      "    @classmethod\n",
      "    def deserialize(cls, serialized_grammar):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "\n",
      "class RawFunction(Function):\n",
      "    __slots__ = (\"f\", \"args\", \"kwargs\")\n",
      "\n",
      "    def __init__(self, f, args, kwargs):\n",
      "        self.f = f\n",
      "        self.args = args\n",
      "        self.kwargs = kwargs\n",
      "    \n",
      "    def __call__(self, model):\n",
      "        return self.f(model, *self.args, **self.kwargs)\n",
      "    \n",
      "    def __add__(self, other):\n",
      "        \n",
      "        # if we are joining with a string we use the string representation for ourselves\n",
      "        if isinstance(other, str):\n",
      "            return str(self) + other\n",
      "        \n",
      "        def __add__(model):\n",
      "            model = self(model)\n",
      "            if model is None:\n",
      "                raise Exception(f\"The guidance function `{self.f.__name__}` did not return a model object! You need to return an updated model object at the end of your guidance function.\")\n",
      "            if isinstance(other, GrammarFunction):\n",
      "                return model + other\n",
      "            else:\n",
      "                return other(model)\n",
      "        return RawFunction(__add__, [], {})\n",
      "    \n",
      "    def __radd__(self, other):\n",
      "        \n",
      "        # if we are joining with a string we use the string representation for ourselves\n",
      "        if isinstance(other, str):\n",
      "            return other + str(self)\n",
      "        \n",
      "        def __radd__(model):\n",
      "            if isinstance(other, GrammarFunction):\n",
      "                model += other\n",
      "            else:\n",
      "                model = other(model)\n",
      "            return self(model)\n",
      "        return RawFunction(__radd__, [], {})\n",
      "\n",
      "class Match:\n",
      "    def __init__(self, captures, log_probs, partial):\n",
      "        self.captures = captures\n",
      "        self.log_probs = log_probs\n",
      "        self.partial = partial\n",
      "    \n",
      "    def __getitem__(self, key):\n",
      "        return self.captures[key]\n",
      "    \n",
      "    def __len__(self):\n",
      "        return len(self.captures)\n",
      "    \n",
      "    def __bool__(self):\n",
      "        return True\n",
      "    \n",
      "    def __str__(self):\n",
      "        return str(self.captures)\n",
      "    \n",
      "    def __repr__(self):\n",
      "        return \"<guidance.Match object; captures=\"+str(self.captures)+\"; partial=\"+str(self.partial)+\">\"\n",
      "\n",
      "class GrammarFunction(Function):\n",
      "    num_used_names = 0\n",
      "\n",
      "    def __add__(self, value):\n",
      "\n",
      "        # see if we have a string with calls or a simple string \n",
      "        if isinstance(value, str) or isinstance(value, bytes):\n",
      "            if isinstance(value, str) and re.search(_tag_pattern, value):\n",
      "                return str(self) + value\n",
      "            else:\n",
      "                value = string(value) \n",
      "        \n",
      "        # see if we can keep building a stateless grammar\n",
      "        if isinstance(value, GrammarFunction):\n",
      "            return Join([self, value])\n",
      "        \n",
      "        # otherwise we let the stateful object handle things\n",
      "        else:\n",
      "            return value.__radd__(self)\n",
      "    \n",
      "    def __radd__(self, value):\n",
      "\n",
      "        # see if we have a string with calls or a simple string \n",
      "        if isinstance(value, str) or isinstance(value, bytes):\n",
      "            if isinstance(value, str) and re.search(_tag_pattern, value):\n",
      "                return value + str(self)\n",
      "            else:\n",
      "                value = string(value) \n",
      "        \n",
      "        # see if we can keep building a stateless grammar\n",
      "        if isinstance(value, GrammarFunction):\n",
      "            return Join([value, self])\n",
      "        \n",
      "        # otherwise we let the stateful object handle things\n",
      "        else:\n",
      "            return value.__add__(self)\n",
      "    \n",
      "    def __getitem__(self, value):\n",
      "        raise StatefulException(\"GrammarFunctions can't access state!\")\n",
      "    \n",
      "    def match(self, byte_string: Union[str, bytes], allow_partial: bool=False, raise_exceptions: bool=False) -> Union[Match, None]:\n",
      "        if isinstance(byte_string, str):\n",
      "            byte_string = byte_string.encode()\n",
      "        parser = _parser.EarleyCommitParser(self)\n",
      "\n",
      "        for i in range(len(byte_string)):\n",
      "            try:\n",
      "                parser.consume_byte(byte_string[i:i+1])\n",
      "            except _parser.ParserException:\n",
      "                if raise_exceptions:\n",
      "                    raise\n",
      "                else:\n",
      "                    return None\n",
      "        \n",
      "        if not allow_partial and not parser.matched():\n",
      "            return None\n",
      "        else:\n",
      "            return Match(*parser.get_captures(), partial=not parser.matched())\n",
      "    \n",
      "    @staticmethod\n",
      "    def _new_name():\n",
      "        num_used = GrammarFunction.num_used_names\n",
      "\n",
      "        a_ord = ord('a')\n",
      "\n",
      "        # name the name in base 26 letter notation\n",
      "        name = chr(a_ord + num_used % 26)\n",
      "        if num_used >= 26:\n",
      "            name = chr(a_ord + (num_used % 676) // 26) + name\n",
      "            if num_used >= 676:\n",
      "                name = chr(a_ord + (num_used % 17576) // 676) + name\n",
      "                if num_used >= 17576:\n",
      "                    name = chr(a_ord + (num_used % 456976) // 17576) + name\n",
      "\n",
      "        GrammarFunction.num_used_names += 1\n",
      "        \n",
      "        return name\n",
      "    \n",
      "    def gbnf_string(self):\n",
      "        used_names = set()\n",
      "        names = {}\n",
      "        lines = []\n",
      "        root_name = self._rec_gbnf_string(lines, used_names, names)\n",
      "        lines.append(\"root ::= \" + root_name)\n",
      "        return \"\\n\".join(lines)\n",
      "    \n",
      "    def serialize(self):\n",
      "        g = _serialization_pb2.Grammar()\n",
      "        index_map = {}\n",
      "        nodes = {}\n",
      "        self._rec_create_index_map(index_map) # gives all the nodes an index\n",
      "        self._rec_serialize(index_map, nodes) # nodes is filled in (as is index_map)\n",
      "        g.nodes.extend(list(nodes.values()))\n",
      "        return g.SerializeToString()\n",
      "    \n",
      "    def _rec_create_index_map(self, index_map):\n",
      "        if self not in index_map:\n",
      "            index_map[self] = len(index_map)\n",
      "            if hasattr(self, \"values\"):\n",
      "                for value in self.values:\n",
      "                    value._rec_create_index_map(index_map)\n",
      "    \n",
      "    def _rec_serialize(self, index_map, nodes):\n",
      "        if self not in nodes:\n",
      "            v = self._to_proto(index_map)\n",
      "            node = _serialization_pb2.GrammarFunction()\n",
      "            if isinstance(self, Byte):\n",
      "                node.byte.CopyFrom(v)\n",
      "            elif isinstance(self, ByteRange):\n",
      "                node.byte_range.CopyFrom(v)\n",
      "            elif isinstance(self, Select):\n",
      "                node.select.CopyFrom(v)\n",
      "            elif isinstance(self, Join):\n",
      "                node.join.CopyFrom(v)\n",
      "            elif isinstance(self, ModelVariable):\n",
      "                node.model_variable.CopyFrom(v)\n",
      "            else:\n",
      "                raise Exception(\"Unknown node type\")\n",
      "            nodes[self] = node\n",
      "            if hasattr(self, \"values\"):\n",
      "                for value in self.values:\n",
      "                    value._rec_serialize(index_map, nodes)\n",
      "    \n",
      "    @classmethod\n",
      "    def deserialize(cls, serialized_grammar):\n",
      "        g = _serialization_pb2.Grammar()\n",
      "        g.ParseFromString(serialized_grammar)\n",
      "\n",
      "        # create the list of objects\n",
      "        values = []\n",
      "        for node in g.nodes:\n",
      "            if node.HasField(\"byte\"):\n",
      "                node = Byte._from_proto(node.byte)\n",
      "            elif node.HasField(\"byte_range\"):\n",
      "                node = ByteRange._from_proto(node.byte_range)\n",
      "            elif node.HasField(\"select\"):\n",
      "                node = Select._from_proto(node.select)\n",
      "            elif node.HasField(\"join\"):\n",
      "                node = Join._from_proto(node.join)\n",
      "            elif node.HasField(\"model_variable\"):\n",
      "                node = ModelVariable._from_proto(node.model_variable)\n",
      "            else:\n",
      "                raise Exception(\"Unknown node type\")\n",
      "            values.append(node)\n",
      "\n",
      "        # fill in the values pointers now that we have the full list of objects\n",
      "        for v in values:\n",
      "            if hasattr(v, \"values\"):\n",
      "                for i, index in enumerate(v.values):\n",
      "                    v.values[i] = values[index]\n",
      "\n",
      "        return values[0] # the first element in the root node of the grammar\n",
      "\n",
      "class Terminal(GrammarFunction):\n",
      "    def match_byte(self, byte):\n",
      "        pass # abstract\n",
      "\n",
      "    @property\n",
      "    def max_tokens(self):\n",
      "        return 1000000000000\n",
      "\n",
      "class Byte(Terminal):\n",
      "    __slots__ = (\"byte\", \"hidden\", \"commit_point\", \"capture_name\", \"temperature\")\n",
      "\n",
      "    def __init__(self, byte):\n",
      "        assert isinstance(byte, bytes)\n",
      "        assert len(byte) == 1\n",
      "        self.byte = byte\n",
      "        self.hidden = False\n",
      "        self.commit_point = False\n",
      "        self.capture_name = None\n",
      "        self.temperature = -1\n",
      "\n",
      "    @property\n",
      "    def name(self):\n",
      "        return str(self.byte)\n",
      "    \n",
      "    def __hash__(self):\n",
      "        return self.byte[0]\n",
      "    \n",
      "    def __eq__(self, other):\n",
      "        return isinstance(other, Byte) and self.byte[0] == other.byte[0]\n",
      "    \n",
      "    def __repr__(self) -> str:\n",
      "        return str(self.byte)\n",
      "    \n",
      "    def __len__(self):\n",
      "        return 1\n",
      "    \n",
      "    def match_byte(self, byte):\n",
      "        return byte == self.byte\n",
      "    \n",
      "    @property\n",
      "    def nullable(self):\n",
      "        return False\n",
      "    \n",
      "    def _to_proto(self, index_map):\n",
      "        data = _serialization_pb2.Byte()\n",
      "        data.byte = self.byte\n",
      "        data.hidden = self.hidden\n",
      "        data.commit_point = self.commit_point\n",
      "        data.capture_name = \"\" if self.capture_name is None else self.capture_name\n",
      "        data.temperature = self.temperature\n",
      "        return data\n",
      "    \n",
      "    @staticmethod\n",
      "    def _from_proto(data):\n",
      "        out = Byte(data.byte)\n",
      "        out.hidden = data.hidden\n",
      "        out.commit_point = data.commit_point\n",
      "        out.capture_name = None if data.capture_name == \"\" else data.capture_name\n",
      "        out.temperature = data.temperature\n",
      "        return out\n",
      "\n",
      "class ByteRange(Terminal):\n",
      "    __slots__ = (\"byte_range\", \"hidden\", \"commit_point\", \"capture_name\", \"temperature\")\n",
      "\n",
      "    def __init__(self, byte_range):\n",
      "        assert isinstance(byte_range, bytes)\n",
      "        assert len(byte_range) == 2\n",
      "        self.byte_range = byte_range\n",
      "        self.hidden = False\n",
      "        self.commit_point = False\n",
      "        self.capture_name = None\n",
      "        self.temperature = -1 # -1 means not set\n",
      "\n",
      "    def match_byte(self, byte):\n",
      "        return self.byte_range[0] <= byte[0] <= self.byte_range[1]\n",
      "\n",
      "    @property\n",
      "    def name(self):\n",
      "        return str(self.byte_range)\n",
      "    @name.setter\n",
      "    def name(self, value):\n",
      "        pass # we ignore name changes\n",
      "    \n",
      "    @property\n",
      "    def nullable(self):\n",
      "        return False\n",
      "    \n",
      "    def __hash__(self):\n",
      "        return self.byte_range[0] + 256 * self.byte_range[1]\n",
      "    \n",
      "    def __eq__(self, other):\n",
      "        return isinstance(other, ByteRange) and self.byte_range[0] == other.byte_range[0] and self.byte_range[1] == other.byte_range[1]\n",
      "    \n",
      "    def __repr__(self) -> str:\n",
      "        return str(self.byte_range)\n",
      "    \n",
      "    def __len__(self):\n",
      "        return 1\n",
      "\n",
      "    def _to_proto(self, index_map):\n",
      "        data = _serialization_pb2.ByteRange()\n",
      "        data.byte_range = self.byte_range\n",
      "        data.hidden = self.hidden\n",
      "        data.commit_point = self.commit_point\n",
      "        data.capture_name = \"\" if self.capture_name is None else self.capture_name\n",
      "        data.temperature = self.temperature\n",
      "        return data\n",
      "\n",
      "    @staticmethod\n",
      "    def _from_proto(data):\n",
      "        out = ByteRange(data.byte_range)\n",
      "        out.hidden = data.hidden\n",
      "        out.commit_point = data.commit_point\n",
      "        out.capture_name = None if data.capture_name == \"\" else data.capture_name\n",
      "        out.temperature = data.temperature\n",
      "        return out\n",
      "\n",
      "class Null():\n",
      "    __slots__ = (\"name\", \"hidden\", \"commit_point\", \"capture_name\")\n",
      "\n",
      "    nullable = True\n",
      "    def __init__(self):\n",
      "        self.name = None\n",
      "        self.hidden = False\n",
      "        self.commit_point = False\n",
      "        self.capture_name = None\n",
      "\n",
      "    def __add__(self, other):\n",
      "        # see if we have a string with calls or a simple string\n",
      "        if isinstance(other, bytes):\n",
      "            return string(other)\n",
      "        elif isinstance(other, str):\n",
      "            return str_to_grammar(other)\n",
      "        \n",
      "        # otherwise we return unchanged\n",
      "        else:\n",
      "            return other\n",
      "        \n",
      "    def __radd__(self, other):\n",
      "        return self.__add__(other) # left vs right makes no difference since we are null\n",
      "        \n",
      "class ModelVariable(GrammarFunction):\n",
      "    '''This represents a variable that will be read from the model object when this grammar is executed.\n",
      "    \n",
      "    Note that the name is the name of the attribute on the model object this node\n",
      "    will get replaced with.\n",
      "    '''\n",
      "    __slots__ = (\"name\", \"hidden\", \"commit_point\", \"capture_name\")\n",
      "\n",
      "    def __init__(self, name):\n",
      "        self.name = name\n",
      "        self.hidden = False\n",
      "        self.commit_point = False\n",
      "        self.capture_name = None\n",
      "        self.nullable = False\n",
      "\n",
      "    def _to_proto(self, index_map):\n",
      "        data = _serialization_pb2.ModelVariable()\n",
      "        data.hidden = self.hidden\n",
      "        data.name = self.name\n",
      "        data.commit_point = self.commit_point\n",
      "        data.capture_name = \"\" if self.capture_name is None else self.capture_name\n",
      "        return data\n",
      "\n",
      "    @staticmethod\n",
      "    def _from_proto(data):\n",
      "        out = ModelVariable(data.name)\n",
      "        out.hidden = data.hidden\n",
      "        out.commit_point = data.commit_point\n",
      "        out.capture_name = None if data.capture_name == \"\" else data.capture_name\n",
      "        return out\n",
      "\n",
      "def replace_grammar_node(grammar, target, replacement):\n",
      "    # Use a stack to keep track of the nodes to be visited\n",
      "    stack = [grammar]\n",
      "    visited_set = set()  # use set for O(1) lookups\n",
      "\n",
      "    while stack:\n",
      "        current = stack.pop()\n",
      "        \n",
      "        # Check if we have already visited this node\n",
      "        if current in visited_set:\n",
      "            continue\n",
      "        visited_set.add(current)\n",
      "\n",
      "        # We are done with this node if it's a terminal\n",
      "        if isinstance(current, (Terminal, ModelVariable)):\n",
      "            continue\n",
      "        \n",
      "        # Iterate through the node's values and replace target with replacement\n",
      "        for i, value in enumerate(current.values):\n",
      "            if value == target:\n",
      "                current.values[i] = replacement\n",
      "            else:\n",
      "                stack.append(value)\n",
      "\n",
      "# def replace_grammar_node(grammar, target, replacement, visited_set={}):\n",
      "    \n",
      "#     # see if we have already visited this node\n",
      "#     if grammar in visited_set:\n",
      "#         return\n",
      "#     else:\n",
      "#         visited_set[grammar] = True\n",
      "   \n",
      "#     # we are done if this is a terminal\n",
      "#     if isinstance(grammar, (Terminal, ModelVariable)):\n",
      "#         return\n",
      "    \n",
      "#     # replace all matching sub-nodes\n",
      "#     for i,value in enumerate(grammar.values):\n",
      "#         if value == target:\n",
      "#             grammar.values[i] = replacement\n",
      "#         else:\n",
      "#             replace_grammar_node(value, target, replacement, visited_set)\n",
      "\n",
      "def replace_model_variables(grammar, model, allowed_vars=None):\n",
      "    '''Replace all the ModelVariable nodes with their values in an iterative manner.'''\n",
      "    visited_set = set()\n",
      "    stack = [(grammar, None, None)]  # Stack stores tuples of (node, parent_node, child_index)\n",
      "    replacements = []\n",
      "\n",
      "    while stack:\n",
      "        current, parent, child_index = stack.pop()\n",
      "\n",
      "        # This node is being visited for the first time\n",
      "        if current not in visited_set:\n",
      "            visited_set.add(current)\n",
      "\n",
      "            # If it's a terminal node, skip it\n",
      "            if isinstance(current, Terminal):\n",
      "                continue\n",
      "\n",
      "            # Process non-terminal nodes in reverse order to maintain the depth-first order\n",
      "            for i in reversed(range(len(current.values))):\n",
      "                value = current.values[i]\n",
      "                if isinstance(value, ModelVariable):\n",
      "                    if allowed_vars is not None and value.name not in allowed_vars:\n",
      "                        raise Exception(f\"Invalid model variable name: {value.name}\")\n",
      "                    # Replace the ModelVariable with its value from 'model' (or the tokenizer if model does not have it)\n",
      "                    # note we skip over attrs we don't have since we may be run twice, once on the model and once for the engine\n",
      "                    if hasattr(model, value.name):\n",
      "                        obj = model\n",
      "                    elif hasattr(model, \"tokenizer\") and hasattr(model.tokenizer, value.name):\n",
      "                        obj = model.tokenizer\n",
      "                    else:\n",
      "                        obj = None\n",
      "                    if obj is not None:\n",
      "                        replacement_value = _wrap_as_grammar(getattr(obj, value.name))\n",
      "                        if value.commit_point:\n",
      "                            replacement_value = commit_point(replacement_value, hidden=value.hidden)\n",
      "                        replacements.append((current, i, value))  # Record the replacement\n",
      "                        current.values[i] = replacement_value  # Perform the replacement\n",
      "                else:\n",
      "                    # If not ModelVariable, push onto the stack to process later\n",
      "                    stack.append((value, current, i))\n",
      "\n",
      "    return replacements\n",
      "\n",
      "# def replace_model_variables(grammar, model, visited_set={}):\n",
      "#     '''Replace all the ModelVariable nodes with their values.'''\n",
      "    \n",
      "#     # see if we have already visited this node\n",
      "#     if grammar in visited_set:\n",
      "#         return []\n",
      "#     else:\n",
      "#         visited_set[grammar] = True\n",
      "   \n",
      "#     # we are done if this is a terminal\n",
      "#     if isinstance(grammar, Terminal):\n",
      "#         return []\n",
      "    \n",
      "#     # replace all matching sub-nodes\n",
      "#     replacements = []\n",
      "#     for i,value in enumerate(grammar.values):\n",
      "#         if isinstance(value, ModelVariable):\n",
      "#             g = _wrap_as_grammar(getattr(model, value.name))\n",
      "#             if value.commit_point:\n",
      "#                 g = commit_point(g, hidden=value.hidden)\n",
      "#             replacements.append((grammar, i, value))\n",
      "#             grammar.values[i] = g\n",
      "#         else:\n",
      "#             replacements.extend(replace_model_variables(value, model, visited_set))\n",
      "#     return replacements\n",
      "\n",
      "def unreplace_model_variables(replacements):\n",
      "    '''This restores a grammar back to its original state, ready for another execution.'''\n",
      "    for grammar,i,orig_value in replacements:\n",
      "        grammar.values[i] = orig_value\n",
      "\n",
      "def _wrap_as_grammar(value):\n",
      "    '''This takes whatever value was given and tries to turn in into a guidance grammar.'''\n",
      "\n",
      "    # if it is already a valid grammar we have no need to wrap it\n",
      "    if isinstance(value, GrammarFunction):\n",
      "        return value\n",
      "    \n",
      "    # if it is already a valid grammar we have no need to wrap it\n",
      "    if value is None:\n",
      "        return Null() \n",
      "    \n",
      "    # we have a constant value\n",
      "    if isinstance(value, (str, bytes)):\n",
      "        return string(value)\n",
      "    \n",
      "    raise Exception(\"Can't wrap as a grammar!\")\n",
      "\n",
      "def commit_point(value, hidden=False):\n",
      "    '''Force the grammar to commit to a parse that includes this node once it can.\n",
      "    \n",
      "    Not that commit point nodes can be optionally hidden (in fact they are the only\n",
      "    nodes that can be hidden since they are by definition not impacted by multiple possible\n",
      "    inconsistent parses.)'''\n",
      "    # TODO: assert that value is not empty since we don't yet support that\n",
      "    if isinstance(value, str):\n",
      "        value = string(value)\n",
      "    if isinstance(value, Terminal):\n",
      "        value = Join([value]) # commit points should be full nodes (otherwise we can't hide them) TODO: decide if we want to do this even for non-hidden commit points\n",
      "    value.commit_point = True\n",
      "    if hidden:\n",
      "        _rec_hide(value)\n",
      "    return value\n",
      "\n",
      "def _rec_hide(grammar):\n",
      "    if not grammar.hidden:\n",
      "        grammar.hidden = True\n",
      "        if hasattr(grammar, \"values\"):\n",
      "            for g in grammar.values:\n",
      "                _rec_hide(g)\n",
      "\n",
      "class Placeholder(GrammarFunction):\n",
      "    __slots__ = tuple(\"nullable\")\n",
      "    def __init__(self):\n",
      "        self.nullable = False\n",
      "\n",
      "\n",
      "class Join(GrammarFunction):\n",
      "    __slots__ = (\"nullable\", \"values\", \"name\", \"hidden\", \"commit_point\", \"capture_name\", \"max_tokens\")\n",
      "\n",
      "    def __init__(self, values, name: Union[str, None]=None, max_tokens=100000000) -> None:\n",
      "        values = [string(v) if isinstance(v, (str, bytes)) else v for v in values] # wrap raw strings\n",
      "        self.nullable = all(getattr(v, \"nullable\", False) for v in values)\n",
      "        self.values = [v for v in values if not isinstance(v, Null)]\n",
      "        self.name = name if name is not None else GrammarFunction._new_name()\n",
      "        self.hidden = False\n",
      "        self.commit_point = False\n",
      "        self.capture_name = None\n",
      "        self.max_tokens = max_tokens\n",
      "\n",
      "    def __repr__(self, indent=\"\", done=None):\n",
      "        if done is None:\n",
      "            done = set()\n",
      "        s = self.name.ljust(20) + \" <- \" + \" \".join([v.name for v in self.values])\n",
      "        s += \"        \" + (\"hidden \" if self.hidden else \"\") + (\"commit_point \" if self.commit_point else \"\") + (f\"capture_name={self.capture_name} \" if self.capture_name else \"\") + (f\"max_tokens={self.max_tokens}\" if self.max_tokens < 100000 else \"\") +\"\\n\"\n",
      "        done.add(self)\n",
      "        for v in self.values:\n",
      "            if v not in done and (isinstance(v, Join) or isinstance(v, Select)):\n",
      "                s += v.__repr__(indent, done)\n",
      "        return s\n",
      "    \n",
      "    def _to_proto(self, index_map):\n",
      "        data = _serialization_pb2.Join()\n",
      "        data.nullable = self.nullable\n",
      "        for v in self.values:\n",
      "            data.values.append(index_map[v])\n",
      "        data.name = self.name\n",
      "        data.hidden = self.hidden\n",
      "        data.commit_point = self.commit_point\n",
      "        data.capture_name = \"\" if self.capture_name is None else self.capture_name\n",
      "        data.max_tokens = self.max_tokens\n",
      "        return data\n",
      "\n",
      "    @staticmethod\n",
      "    def _from_proto(data):\n",
      "        out = Join(\n",
      "            data.values, # we put ints in that will be replaced later by the deserialize method\n",
      "            name=data.name,\n",
      "            max_tokens=data.max_tokens\n",
      "        )\n",
      "        out.nullable = data.nullable\n",
      "        out.hidden = data.hidden\n",
      "        out.commit_point = data.commit_point\n",
      "        out.capture_name = None if data.capture_name == \"\" else data.capture_name\n",
      "        return out\n",
      "\n",
      "\n",
      "class Select(GrammarFunction):\n",
      "    __slots__ = (\"nullable\", \"_values\", \"name\", \"hidden\", \"commit_point\", \"capture_name\", \"max_tokens\", \"recursive\")\n",
      "\n",
      "    def __init__(self, values, capture_name=None, name=None, max_tokens=10000000, recursive=False) -> None:\n",
      "        self.values = values\n",
      "        self.name = name if name is not None else GrammarFunction._new_name()\n",
      "        self.hidden = False\n",
      "        self.commit_point = False\n",
      "        self.capture_name = capture_name\n",
      "        self.max_tokens = max_tokens\n",
      "        self.recursive = recursive\n",
      "\n",
      "    @property\n",
      "    def values(self):\n",
      "        return self._values\n",
      "    @values.setter\n",
      "    def values(self, vals):\n",
      "        self._values = [string(v) if isinstance(v, (str, bytes)) else v for v in vals]\n",
      "        self.nullable = any(getattr(v, \"nullable\", False) for v in self._values)\n",
      "        self._values = [v for v in self._values if not isinstance(v, Null)]\n",
      "\n",
      "    def __repr__(self, indent=\"\", done=None):\n",
      "        if done is None:\n",
      "            done = set()\n",
      "        s = self.name.ljust(20) + \" <- \" + \" | \".join([v.name for v in self.values])\n",
      "        s += \"        \" + (\"hidden \" if self.hidden else \"\") + (\"commit_point \" if self.commit_point else \"\") + (f\"max_tokens={self.max_tokens}\" if self.max_tokens < 100000 else \"\") +\"\\n\"\n",
      "        done.add(self)\n",
      "        for v in self.values:\n",
      "            if v not in done and (isinstance(v, Join) or isinstance(v, Select)):\n",
      "                s += v.__repr__(indent, done)\n",
      "        return s\n",
      "    \n",
      "    def _to_proto(self, index_map):\n",
      "        data = _serialization_pb2.Select()\n",
      "        data.nullable = self.nullable\n",
      "        for v in self.values:\n",
      "            data.values.append(index_map[v])\n",
      "        data.name = self.name\n",
      "        data.hidden = self.hidden\n",
      "        data.commit_point = self.commit_point\n",
      "        data.capture_name = \"\" if self.capture_name is None else self.capture_name\n",
      "        data.max_tokens = self.max_tokens\n",
      "        data.recursive = self.recursive\n",
      "\n",
      "        return data\n",
      "\n",
      "    @staticmethod\n",
      "    def _from_proto(data):\n",
      "        out = Select(\n",
      "            data.values, # we put ints in that will be replaced later by the deserialize method\n",
      "            name=data.name,\n",
      "            max_tokens=data.max_tokens\n",
      "        )\n",
      "        out.nullable = data.nullable\n",
      "        out.hidden = data.hidden\n",
      "        out.commit_point = data.commit_point\n",
      "        out.capture_name = None if data.capture_name == \"\" else data.capture_name\n",
      "        out.recursive = data.recursive\n",
      "        return out\n",
      "\n",
      "def string(value) -> Union[str, bytes, Null, Byte, Join]:\n",
      "    if isinstance(value, str):\n",
      "        b = bytes(value, encoding=\"utf8\")\n",
      "    elif isinstance(value, bytes):\n",
      "        b = value\n",
      "    else:\n",
      "        raise Exception(\"Must pass bytes or str to the string() function!\")\n",
      "    if len(value) == 0:\n",
      "        return Null()\n",
      "    elif len(b) == 1:\n",
      "        return Byte(b)\n",
      "    else:\n",
      "        return Join([Byte(b[i:i+1]) for i in range(len(b))], name=str(b))\n",
      "    \n",
      "def select(options: List[_T], name=None, list_append=False, recurse=False, skip_checks=False) -> Union[Select, _T]:\n",
      "    # TODO: allow for returning the probabilites of the selected item\n",
      "    # TODO: also the full probabilites distribution over all items. We can implement this using the prob of the selected item by repeating the call, removing the selected item each time\n",
      "    if not skip_checks:\n",
      "        for i, value in enumerate(options):\n",
      "            assert not isinstance(value, RawFunction), \"You cannot select between stateful functions in the current guidance implementation!\"\n",
      "            assert not isinstance(value, types.FunctionType), \"Did you pass a function without calling it to select? You need to pass the results of a called guidance function to select.\"\n",
      "            if isinstance(value, int) or isinstance(value, float):\n",
      "                options[i] = str(value)\n",
      "\n",
      "    # set up list append var saving if requested\n",
      "    if list_append:\n",
      "        name = \"__LIST_APPEND:\" + name\n",
      "\n",
      "    if recurse:\n",
      "        node = Select([], capture_name=name, recursive=True)\n",
      "        node.values = [node + v for v in options if v != \"\"] + options\n",
      "        return node\n",
      "    else:\n",
      "        if len(options) == 1 and name is None:\n",
      "            return options[0]\n",
      "        else:\n",
      "            return Select(options, capture_name=name, recursive=False)\n",
      "        \n",
      "def byte_range(low, high) -> ByteRange:\n",
      "    return ByteRange(low + high)\n",
      "\n",
      "# def ignore_placeholders(value):\n",
      "#     if not isinstance(value, Join): # don't double wrap\n",
      "#         value = Join([value]) # this ensures we capture what we want, and not something surprisingly self_recursive\n",
      "#     value.ignore_placeholders = True\n",
      "#     return value\n",
      "\n",
      "def capture(value, name):\n",
      "    # if log_probs:\n",
      "    #     name += \":__LOG_PROBS\"\n",
      "    if not (isinstance(value, Join) and len(value.values) == 1): # don't double wrap\n",
      "        value = Join([value]) # this ensures we capture what we want, and not something surprisingly self_recursive\n",
      "    value.capture_name = name\n",
      "    return value\n",
      "\n",
      "def token_limit(value, max_tokens: int):\n",
      "    _rec_token_limit(value, max_tokens)\n",
      "    return value\n",
      "\n",
      "def _rec_token_limit(grammar, max_tokens: int):\n",
      "    if grammar.max_tokens > max_tokens and not isinstance(grammar, Terminal):\n",
      "        if getattr(grammar, \"recursive\", False): # only restrict recursive selects, otherwise we would block all ways to complete the grammar\n",
      "            grammar.max_tokens = max_tokens\n",
      "            for value in getattr(grammar, \"values\", []): # restrict recursive selects recursive nodes\n",
      "                if not isinstance(value, Terminal):\n",
      "                    value.max_tokens = max_tokens\n",
      "        if hasattr(grammar, \"values\"):\n",
      "            for g in grammar.values:\n",
      "                _rec_token_limit(g, max_tokens)\n",
      "\n",
      "def with_temperature(value, temperature):\n",
      "    '''This sets the sampling temperature to be used for the given portion of the grammar.\n",
      "    \n",
      "    Note that if the grammar passed to us already has some portions with a temperature\n",
      "    setting in place, those settings will not be overridden.\n",
      "    '''\n",
      "    _re_with_temperature(value, temperature, {})\n",
      "    return value\n",
      "\n",
      "def _re_with_temperature(grammar, temperature, visited_set):\n",
      "    \n",
      "    # don't go down the same path twice\n",
      "    if grammar in visited_set:\n",
      "        return\n",
      "    visited_set[grammar] = True\n",
      "\n",
      "    # if getattr(grammar, \"temperature\", 100000000) > temperature:\n",
      "    if isinstance(grammar, Terminal) and grammar.temperature < 0: # only need to set temp for terminals\n",
      "        grammar.temperature = temperature\n",
      "    elif getattr(grammar, \"temperature\", 100000000) > temperature and hasattr(grammar, \"values\"):\n",
      "        for g in grammar.values:\n",
      "            _re_with_temperature(g, temperature, visited_set)\n",
      "\n",
      "# def model_variable(name):\n",
      "#     return ModelVariable(name)\n",
      "\n",
      "def active_role_end() -> ModelVariable:\n",
      "    return ModelVariable('active_role_end')\n",
      "\n",
      "def eos_token() -> ModelVariable:\n",
      "    return ModelVariable('eos_token')\n",
      "\n",
      "def bos_token() -> ModelVariable:\n",
      "    return ModelVariable('bos_token')\n",
      "\n",
      "_null_grammar = string('')\n",
      "# def char_range(low, high):\n",
      "#     low_bytes = bytes(low, encoding=\"utf8\")\n",
      "#     high_bytes = bytes(high, encoding=\"utf8\")\n",
      "#     if len(low_bytes) > 1 or len(high_bytes) > 1:\n",
      "#         raise Exception(\"We don't yet support multi-byte character ranges!\")\n",
      "#     return ByteRange(low_bytes + high_bytes)\n",
      "def str_to_grammar(value: str):\n",
      "    is_id = False\n",
      "    parts = re.split(_tag_pattern, value)\n",
      "    \n",
      "    # we have no embedded objects\n",
      "    if len(parts) == 1:\n",
      "        return string(value)\n",
      "    \n",
      "    # if we have embedded objects we have to convert the string to a grammar tree\n",
      "    else:\n",
      "        partial_grammar = _null_grammar\n",
      "        # lm.suffix = \"\"\n",
      "        for i,part in enumerate(parts):\n",
      "            # if i < len(parts) - 1:\n",
      "            #     lm.suffix = parts[i+1]\n",
      "            if is_id:\n",
      "                call = _call_pool[part]\n",
      "                if isinstance(call, GrammarFunction):\n",
      "                    partial_grammar += _call_pool[part]\n",
      "                else:\n",
      "                    partial_grammar = RawFunction(lambda lm, g, call: call(lm + g), partial_grammar, _call_pool[part])\n",
      "                    # lm += partial_grammar\n",
      "                    # lm = _call_pool[part](lm)\n",
      "                    # partial_grammar = _null_grammar\n",
      "            elif part != \"\":\n",
      "                partial_grammar += string(part)\n",
      "            is_id = not is_id\n",
      "    return partial_grammar\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/_parser.py\n",
      "```py\n",
      "from sys import stderr\n",
      "import numpy as np\n",
      "from ordered_set import OrderedSet\n",
      "from ._grammar import Join, Select, Terminal, Null, Byte, ByteRange\n",
      "\n",
      "\n",
      "class ParserException(Exception):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        self.current_byte = kwargs.pop(\"current_byte\", None)\n",
      "        self.allowed_bytes = kwargs.pop(\"allowed_bytes\", None)\n",
      "        super().__init__(*args, **kwargs)\n",
      "\n",
      "\n",
      "class EarleyItem:\n",
      "    __slots__ = (\"node\", \"values\", \"start\", \"pos\", \"log_prob\", \"children\", \"hidden_start\")\n",
      "\n",
      "    def __init__(self, node, values, pos, start, log_prob, hidden_start):\n",
      "        self.node = node\n",
      "        self.values = values\n",
      "        self.start = start\n",
      "        self.pos = pos\n",
      "        self.log_prob = log_prob\n",
      "        self.children = None\n",
      "        self.hidden_start = hidden_start\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        return isinstance(other, EarleyItem) and \\\n",
      "               self.start == other.start and \\\n",
      "               self.pos == other.pos and \\\n",
      "               self.node == other.node and \\\n",
      "               self.values == other.values and \\\n",
      "               self.log_prob == other.log_prob\n",
      "    \n",
      "    def __hash__(self):\n",
      "        return hash((self.node, self.values, self.start, self.pos))\n",
      "    \n",
      "    def __repr__(self):\n",
      "        if isinstance(self.node, Join):\n",
      "            s = f\"{self.node.name:20} -> \"\n",
      "            rs = \"\"\n",
      "            for i,v in enumerate(self.values):\n",
      "                if self.pos == i:\n",
      "                    rs += \"\"\n",
      "                rs += v.name + \" \"\n",
      "            if self.pos == len(self.values):\n",
      "                rs += \"\"\n",
      "        elif isinstance(self.node, Select):\n",
      "            s = f\"{self.node.name:20} -> \"\n",
      "            rs = \"\"\n",
      "            if self.pos == 0:\n",
      "                rs += \"\"\n",
      "            rs += self.values[0].name\n",
      "            if self.pos == 1:\n",
      "                rs += \"\"\n",
      "        else:\n",
      "            assert False\n",
      "        return s + f\"{rs:40} ({self.start}) {'nullable' if self.node.nullable else ''}\"\n",
      "\n",
      "class Parser:\n",
      "    '''An abstract base class for guidance parsers.'''\n",
      "    pass\n",
      "\n",
      "class EarleyCommitParser(Parser):\n",
      "    def __init__(self, grammar):\n",
      "\n",
      "        # we can't have a terminal as the root\n",
      "        if isinstance(grammar, Terminal):\n",
      "            grammar = Join([grammar])\n",
      "        \n",
      "        self.grammar = grammar\n",
      "        self.bytes = b''\n",
      "        self.state_sets = [OrderedSet()] # the list of Earley items for each byte\n",
      "        self.token_counts = [] # used to track how many tokens have been used\n",
      "        self.state_set_pos = 0\n",
      "        self.shadow_pos = 0\n",
      "        self._add_node(self.grammar, 0, 0.0, 1000000000)\n",
      "        self._inner_loop(self.state_set_pos)\n",
      "\n",
      "    @property\n",
      "    def pos(self):\n",
      "        return self.shadow_pos\n",
      "    @pos.setter\n",
      "    def pos(self, new_pos):\n",
      "\n",
      "        # do nothing if we aren't moving\n",
      "        if new_pos == self.state_set_pos:\n",
      "            return\n",
      "        elif new_pos > self.state_set_pos:\n",
      "            raise ParserException(\"Can't move the parser position forward! (only backward)\")\n",
      "        \n",
      "        # check if we are just moving the shadow position\n",
      "        if new_pos >= self.shadow_pos:\n",
      "            self.shadow_pos = new_pos\n",
      "            return\n",
      "        \n",
      "        # actually reset our position if we need to\n",
      "        self.state_sets = self.state_sets[:new_pos+1] + [OrderedSet()]\n",
      "        self.token_counts = self.token_counts[:new_pos+2]\n",
      "        self.bytes = self.bytes[:new_pos]\n",
      "        self.state_set_pos = new_pos\n",
      "        self.shadow_pos = new_pos\n",
      "        self._inner_loop(self.state_set_pos)\n",
      "\n",
      "    def _add_item(self, state_set_pos, new_item):\n",
      "        state_set = self.state_sets[state_set_pos]\n",
      "        if new_item not in state_set:\n",
      "            state_set.append(new_item)\n",
      "        else:\n",
      "            existing_item = state_set.items[state_set.map[new_item]]\n",
      "            existing_item.hidden_start = min(existing_item.hidden_start, new_item.hidden_start)\n",
      "\n",
      "    def _add_node(self, grammar, state_set_pos, log_prob, hidden_start):\n",
      "        if isinstance(grammar, Terminal):\n",
      "            new_item = EarleyItem(grammar, tuple(), 0, state_set_pos, log_prob, hidden_start)\n",
      "            self._add_item(state_set_pos, new_item)\n",
      "            \n",
      "        elif isinstance(grammar, Join):\n",
      "            new_item = EarleyItem(grammar, tuple(grammar.values), 0, state_set_pos, log_prob, hidden_start)\n",
      "            self._add_item(state_set_pos, new_item)\n",
      "        \n",
      "        elif isinstance(grammar, Select):\n",
      "            for value in grammar.values:\n",
      "                new_item = EarleyItem(grammar, (value,), 0, state_set_pos, log_prob, hidden_start)\n",
      "                self._add_item(state_set_pos, new_item)\n",
      "\n",
      "    def _inner_loop(self, state_set_pos, start_pos=0):\n",
      "        curr_state_set = self.state_sets[state_set_pos]\n",
      "        if len(self.state_sets) == state_set_pos + 1:\n",
      "            self.state_sets.append(OrderedSet())\n",
      "            self.token_counts.append(self.token_counts[-1] if len(self.token_counts) > 0 else 0)\n",
      "        next_state_set = self.state_sets[state_set_pos + 1]\n",
      "        pos = start_pos\n",
      "        while len(curr_state_set) > pos:\n",
      "            item = curr_state_set[pos]\n",
      "\n",
      "            # completion\n",
      "            if item.pos == len(item.values):\n",
      "                \n",
      "                # if we complete an item that is a \"commit point\" then we eliminate all other possible\n",
      "                # parses so that we are \"committed\" to using this item\n",
      "                # we do this by removing any unprocessed items in the current state set and clearing the next state set\n",
      "                if item.node.commit_point:\n",
      "                    while len(curr_state_set) > pos:\n",
      "\n",
      "                        # if we find another valid commit point that starts earlier we use that instead\n",
      "                        # this causes us to pick the longest matching valid commit point\n",
      "                        end_item = curr_state_set[-1]\n",
      "                        if end_item.node.commit_point and end_item.pos == len(end_item.values) and end_item.start < item.start:\n",
      "                            item = end_item\n",
      "                        \n",
      "                        curr_state_set.pop()\n",
      "                    curr_state_set.append(item) # we append the current item again (we do this since we may have swapped it out above)\n",
      "                    next_state_set.clear()\n",
      "                \n",
      "                # advance all the parents that our completion impacts\n",
      "                token_span = self.token_counts[state_set_pos] - self.token_counts[item.start]\n",
      "                start_state_set = self.state_sets[item.start]\n",
      "                for start_item in start_state_set:\n",
      "                    if start_item.pos < len(start_item.values) and start_item.values[start_item.pos] == item.node:\n",
      "                        \n",
      "                        # if item.node.max_tokens <= token_span and any(start_item.node == v and len(v.values) > 1 for v in item.node.values):\n",
      "                        #     continue # skip advancing parents that are also children (recursion) once we are past the token limit\n",
      "\n",
      "                        curr_state_set.append(EarleyItem(\n",
      "                            start_item.node,\n",
      "                            start_item.values,\n",
      "                            start_item.pos + 1,\n",
      "                            start_item.start,\n",
      "                            start_item.log_prob + item.log_prob, # increment the log prob by the child value,\n",
      "                            start_item.hidden_start\n",
      "                        ))\n",
      "            \n",
      "            # don't advance past our max token limit\n",
      "            elif item.node.max_tokens > self.token_counts[state_set_pos] - self.token_counts[item.start]:\n",
      "\n",
      "                # scan (note we only scan forward when we have more max token headroom left)\n",
      "                next_item_node = item.values[item.pos]\n",
      "                hidden_start = item.hidden_start\n",
      "                if next_item_node.hidden:\n",
      "                    hidden_start = min(state_set_pos, hidden_start)\n",
      "                if isinstance(next_item_node, Terminal):# and item.node.max_tokens > self.token_counts[state_set_pos] - self.token_counts[item.start]:\n",
      "                    next_state_set.append(EarleyItem(item.node, item.values, item.pos + 1, item.start, item.log_prob, hidden_start)) # the log prob will get incremented when consume_bytes is called\n",
      "                \n",
      "                # prediction\n",
      "                else:\n",
      "                    self._add_node(next_item_node, state_set_pos, 0.0, hidden_start) # the log probs will get incremented by children later\n",
      "\n",
      "                # handle nullable items by advancing them automatically (since we know we can)\n",
      "                if next_item_node.nullable:\n",
      "                    new_item = EarleyItem(item.node, item.values, item.pos + 1, item.start, item.log_prob, item.hidden_start)\n",
      "                    if new_item not in self.state_sets[state_set_pos]:\n",
      "                        self.state_sets[state_set_pos].append(new_item)\n",
      "            pos += 1\n",
      "\n",
      "    def earliest_hidden_start(self, state_pos=None):\n",
      "        '''The earliest that a hidden node might match.\n",
      "        \n",
      "        This is useful because it tells us which bytes may end being hidden.\n",
      "        '''\n",
      "        if state_pos is None:\n",
      "            state_pos = self.state_set_pos\n",
      "        earliest_pos = 10000000000\n",
      "        for item in self.state_sets[state_pos]:\n",
      "            earliest_pos = min(earliest_pos, item.hidden_start)\n",
      "        return earliest_pos\n",
      "    \n",
      "    def matched(self):\n",
      "        '''Checks if the parser has completely matched the grammar.'''\n",
      "        if self.shadow_pos != self.state_set_pos:\n",
      "            return False\n",
      "        for item in self.state_sets[self.state_set_pos]:\n",
      "            if item.node == self.grammar and item.pos == len(item.values):\n",
      "                return True\n",
      "        return False\n",
      "    \n",
      "    def shadow_rewind(self, new_pos):\n",
      "        if new_pos == self.state_set_pos:\n",
      "            return\n",
      "        self.shadow_pos = new_pos\n",
      "    \n",
      "    def commit_and_collapse_item(self, item):\n",
      "        '''This collapses the item into zero width and rewinds the parser position accordingly.\n",
      "        \n",
      "        Note we assume the item is in the current state set.\n",
      "        '''\n",
      "\n",
      "        # trim off the state sets that matches this item\n",
      "        self.state_sets = self.state_sets[:item.start + 1]\n",
      "        self.token_counts = self.token_counts[:item.start + 1]\n",
      "        self.bytes = self.bytes[:item.start]\n",
      "        self.state_set_pos = item.start\n",
      "        self.shadow_pos = item.start\n",
      "\n",
      "        # add this state to its start point (making it a zero length match with no values)\n",
      "        self.state_sets[item.start].append(EarleyItem(item.node, tuple(), 0, item.start, item.log_prob, item.hidden_start))\n",
      "\n",
      "        # expand from this state\n",
      "        self._inner_loop(item.start, len(self.state_sets[item.start]) - 1)\n",
      "\n",
      "    def mark_new_token(self):\n",
      "        # TODO: we allow ourselves to go one past our max token limit when we hit a one-byte token\n",
      "        #       because we don't know if we are continuing or extending a new token when we parse\n",
      "        #       the first byte of the token. We could fix this by rerunning the inner_loop after each\n",
      "        #       token, but we skip that for now since max_tokens is not a hard garuntee anyway when you\n",
      "        #       have patterns.\n",
      "        \n",
      "        self.token_counts[-1] += 1\n",
      "\n",
      "    def consume_byte(self, byte, log_prob=0.0):\n",
      "        '''Advances the parser by the given byte.'''\n",
      "\n",
      "        # see if we need to advance our shadow position...\n",
      "        if self.shadow_pos < self.state_set_pos:\n",
      "            assert byte == self.bytes[self.shadow_pos:self.shadow_pos+1], \"Attempted to consume a byte by advancing shadow_pos but the byte didn't match!\"\n",
      "            self.shadow_pos += 1\n",
      "            return\n",
      "\n",
      "        # ...if not, we extend our bytes\n",
      "        self.bytes += byte\n",
      "\n",
      "        # filter out all the extensions that don't match this byte\n",
      "        new_next_state_set = []\n",
      "        found_valid = False\n",
      "        found_invalid = False\n",
      "        hidden_start = 10000000000\n",
      "        for item in self.state_sets[self.state_set_pos + 1]:\n",
      "            token_span = self.token_counts[-1] - self.token_counts[item.start]\n",
      "            if item.node.max_tokens <= token_span:\n",
      "                found_invalid = True\n",
      "                continue\n",
      "            elif item.pos > 0 and isinstance(item.values[item.pos - 1], Terminal):\n",
      "                last_inner_node = item.values[item.pos - 1]\n",
      "                if not last_inner_node.match_byte(byte):\n",
      "                    found_invalid = True\n",
      "                    continue\n",
      "                else:\n",
      "                    found_valid = True\n",
      "                    if last_inner_node.commit_point:\n",
      "                        item.log_prob += log_prob\n",
      "                        new_next_state_set = [item]\n",
      "                        hidden_start = min(hidden_start, item.hidden_start)\n",
      "                        found_invalid = True # we make everything else invalid, so that means we found something invalid\n",
      "                        break\n",
      "            item.log_prob += log_prob # update the probability of the item by the probability of choosing this byte\n",
      "            new_next_state_set.append(item)\n",
      "            hidden_start = min(hidden_start, item.hidden_start)\n",
      "        if not found_valid:\n",
      "            raise ParserException(\"Attempted to consume a byte that the grammar does not accept!\",current_byte=byte)\n",
      "        if found_invalid: # only update if we changed the set\n",
      "            self.state_sets[self.state_set_pos + 1] = OrderedSet(new_next_state_set)\n",
      "\n",
      "        # advance the parser one position\n",
      "        self.state_set_pos += 1\n",
      "        self.shadow_pos += 1\n",
      "        self._inner_loop(self.state_set_pos)\n",
      "\n",
      "        # look for a commit point node\n",
      "        commit_point = None\n",
      "        for item in self.state_sets[self.state_set_pos]:\n",
      "            if item.node.commit_point and item.pos == len(item.values) or (item.pos > 0 and item.values[item.pos-1].commit_point):\n",
      "                commit_point = item\n",
      "                break # TODO: consider how we might need to prioritize multiple commit point nodes (an uncommon scenario I think)\n",
      "        # hidden_start, \n",
      "        return commit_point\n",
      "\n",
      "    def valid_next_bytes(self):\n",
      "        '''A list of Byte and ByteRange objects representing the next valid bytes.'''\n",
      "        valid_items = set()\n",
      "        next_state_set = self.state_sets[self.state_set_pos + 1]\n",
      "        for item in next_state_set:\n",
      "            token_span = self.token_counts[-1] - self.token_counts[item.start]\n",
      "            if item.node.max_tokens <= token_span:\n",
      "                continue\n",
      "            elif item.pos > 0 and isinstance(item.values[item.pos - 1], Terminal):\n",
      "                v = item.values[item.pos - 1]\n",
      "                if v not in valid_items:\n",
      "                    valid_items.add(v)\n",
      "        return valid_items\n",
      "    \n",
      "    def next_byte_temperature(self):\n",
      "        '''The maximum temperature over all the next bytes, or -1 if no temperature is set.'''\n",
      "        max_temp = -1\n",
      "        next_state_set = self.state_sets[self.state_set_pos + 1]\n",
      "        for item in next_state_set:\n",
      "            if item.pos > 0 and isinstance(item.values[item.pos - 1], Terminal):\n",
      "                v = item.values[item.pos - 1]\n",
      "                max_temp = max(max_temp, v.temperature)\n",
      "        return max_temp\n",
      "    \n",
      "    def next_byte_mask(self):\n",
      "        '''A mask version of the `valid_next_bytes` method.'''\n",
      "        \n",
      "        mask = np.zeros(256, dtype=bool)\n",
      "\n",
      "        # if we are shadow rewound then we just force those bytes again\n",
      "        if self.shadow_pos < self.state_set_pos:\n",
      "            mask[self.bytes[self.shadow_pos]] = True\n",
      "        \n",
      "        # otherwise we compute the valid bytes from the grammar\n",
      "        else:\n",
      "            valid_items = self.valid_next_bytes()\n",
      "            for item in valid_items:\n",
      "                if isinstance(item, Byte):\n",
      "                    mask[item.byte[0]] = True\n",
      "                elif isinstance(item, ByteRange):\n",
      "                    mask[item.byte_range[0]:item.byte_range[1]+1] = True\n",
      "                else:\n",
      "                    raise ParserException(\"Unknown Terminal Type: \"  + str(type(item)), )\n",
      "        return mask\n",
      "\n",
      "    def __repr__(self, state_sets=None) -> str:\n",
      "        s = \"\"\n",
      "        if state_sets is None:\n",
      "            state_sets = self.state_sets\n",
      "        for i,states in enumerate(state_sets):\n",
      "            s += f\"\\n=== {i} ===\"\n",
      "            if self.state_set_pos == i:\n",
      "                s += \" (state_set_pos)\"\n",
      "            s += \"\\n\"\n",
      "            for state in states:\n",
      "                if isinstance(state.node, Join):\n",
      "                    s += f\"{state.node.name:20} -> \"\n",
      "                    rs = \"\"\n",
      "                    for i,v in enumerate(state.values):\n",
      "                        if state.pos == i:\n",
      "                            rs += \"\"\n",
      "                        rs += v.name + \" \"\n",
      "                    if state.pos == len(state.values):\n",
      "                        rs += \"\"\n",
      "                elif isinstance(state.node, Select):\n",
      "                    s += f\"{state.node.name:20} -> \"\n",
      "                    rs = \"\"\n",
      "                    if state.pos == 0:\n",
      "                       rs += \"\"\n",
      "                    if len(state.values) == 0:\n",
      "                        rs += \"NO_VALUES!\"\n",
      "                    else:\n",
      "                        rs += state.values[0].name\n",
      "                        if state.pos == 1:\n",
      "                            rs += \"\"\n",
      "                else:\n",
      "                    assert False\n",
      "                s += f\"{rs:40} ({state.start}) {'nullable' if state.node.nullable else ''}\\n\"\n",
      "        return s\n",
      "    \n",
      "    def _reversed_state_sets(self):\n",
      "        new_state_sets = [OrderedSet([]) for _ in range(len(self.state_sets))]\n",
      "        for i,states in enumerate(self.state_sets):\n",
      "            for state in states:\n",
      "                # if state.node.name == \"__call___c\":\n",
      "                #     pass\n",
      "                new_state_sets[state.start].append(EarleyItem(state.node, state.values, state.pos, i, state.log_prob, state.hidden_start))\n",
      "        \n",
      "        return new_state_sets\n",
      "    \n",
      "    def parse_tree(self):\n",
      "        reversed_state_sets = self._reversed_state_sets()\n",
      "        root_item = None\n",
      "\n",
      "        # find the matching root state\n",
      "        for item in reversed_state_sets[0]:\n",
      "            if item.node == self.grammar and item.start == len(self.bytes) and item.pos == len(item.values): # note that \".start\" mean end because items are reversed\n",
      "                root_item = item\n",
      "        if root_item is None:\n",
      "            return None\n",
      "        self._compute_parse_tree(0, root_item, reversed_state_sets)\n",
      "        return root_item\n",
      "\n",
      "    def get_captures(self, data=None, log_prob_data=None):\n",
      "        root_node = self.parse_tree()\n",
      "        if data is None:\n",
      "            data = {}\n",
      "        if log_prob_data is None:\n",
      "            log_prob_data = {}\n",
      "        if root_node is not None:\n",
      "            # parse complete, so we can get the captures\n",
      "            self._record_captures_from_root(root_node, data, log_prob_data)\n",
      "            return data, log_prob_data\n",
      "        # compute on partially parsed tree\n",
      "        self._record_captures_partial(data, log_prob_data)\n",
      "        return data, log_prob_data\n",
      "\n",
      "    def _record_captures_partial(self, data, log_prob_data):\n",
      "        byte_data = self.bytes\n",
      "\n",
      "        for item in self.state_sets[self.state_set_pos]:\n",
      "            cname = item.node.capture_name\n",
      "            if cname is None:\n",
      "                continue\n",
      "            captured_value = byte_data[item.start:self.earliest_hidden_start()]\n",
      "            if captured_value.endswith(b'<'):\n",
      "                print(\"WARNING: Captured value ends with '<' which is a special character in the parser!\", file=stderr)\n",
      "            data[cname] = captured_value\n",
      "            log_prob_data[cname] = item.log_prob\n",
      "\n",
      "    def _record_captures_from_root(self, initial_item, data, log_prob_data):\n",
      "        byte_data = self.bytes\n",
      "        stack = [(initial_item, 0)]\n",
      "        used_names = set() # track which capture names have been used so self-recursive children don't overwrite their parents\n",
      "        \n",
      "        while stack:\n",
      "            item, byte_pos = stack.pop()\n",
      "            # terminal nodes\n",
      "            if isinstance(item, Terminal):\n",
      "\n",
      "                # if we are at a capture group node then we save the matched terminal byte\n",
      "                if item.capture_name is not None:\n",
      "                    data[item.capture_name] = item.byte\n",
      "                    log_prob_data[item.capture_name] = 0\n",
      "            \n",
      "            # internal nodes\n",
      "            else:\n",
      "                start_byte_pos = byte_pos\n",
      "\n",
      "                # recurse for all our non-null children\n",
      "                for child in item.children:\n",
      "                    if child is not None:\n",
      "                        stack.append((child, byte_pos))\n",
      "                        # _record_captures(child, data, log_prob_data, byte_data, byte_pos)\n",
      "                        if isinstance(child, Terminal):\n",
      "                            byte_pos += len(child)\n",
      "                        else:\n",
      "                            byte_pos = child.start # note that \"start\" means \"end\" since this is a reversed state set\n",
      "\n",
      "                # if we are at a capture group node then we save the matched bytes range\n",
      "                # note that we record this after calling our children so that we save the outermost version of self-recursive calls\n",
      "                cname = item.node.capture_name\n",
      "                if cname is not None and cname not in used_names and not item.node.hidden:\n",
      "                    \n",
      "                    # see if we are doing a list append\n",
      "                    if cname.startswith(\"__LIST_APPEND:\"):\n",
      "                        cname = cname[14:] # trim off the list append tag\n",
      "                        if cname not in data or not isinstance(data[cname], list):\n",
      "                            data[cname] = []\n",
      "                            log_prob_data[cname] = []\n",
      "                        data[cname].append(byte_data[start_byte_pos:item.start])\n",
      "                        log_prob_data[cname].append(item.log_prob)\n",
      "                    \n",
      "                    # or just a regular assignment\n",
      "                    else:\n",
      "                        data[cname] = byte_data[start_byte_pos:item.start] # note that \"start\" means \"end\" since this is a reversed state set\n",
      "                        log_prob_data[cname] = item.log_prob\n",
      "\n",
      "                    used_names.add(cname)    \n",
      "\n",
      "    def _compute_parse_tree(self, initial_pos, initial_item, reversed_state_sets):\n",
      "        stack = [(initial_pos, initial_item)]\n",
      "        \n",
      "        while stack:\n",
      "            pos, item = stack.pop()\n",
      "\n",
      "            # compute the children for this item\n",
      "            assert self._compute_children(pos, item, reversed_state_sets)\n",
      "\n",
      "            # recurse on the children\n",
      "            for child in item.children:\n",
      "                if child is None:\n",
      "                    pass # this child was nullable and was chosen to be null (empty)\n",
      "                elif isinstance(child, Terminal):\n",
      "                    pos += len(child)\n",
      "                else:\n",
      "                    stack.append((pos, child))\n",
      "                    pos = child.start # note that \".start\" mean end because items are reversed\n",
      "\n",
      "    def _compute_children(self, state_set_pos, item, reversed_state_sets, values_pos = 0):\n",
      "\n",
      "        # ensure we have a children array\n",
      "        if item.children is None:\n",
      "            item.children = [None for _ in range(len(item.values))]\n",
      "\n",
      "        # consume as many terminal children as possible\n",
      "        while True:\n",
      "            \n",
      "            # if we are at the end of the values then there no more children and we see if we consumed all the right bytes\n",
      "            if values_pos == len(item.values):\n",
      "                return state_set_pos == item.start # note that \".start\" mean end because items are reversed\n",
      "\n",
      "            # get the child we are trying to match (meaning we are looking for completed early items for this node)\n",
      "            value = item.values[values_pos]\n",
      "\n",
      "            # if we have a terminal node we can jump forward that many bytes\n",
      "            if isinstance(value, Terminal):\n",
      "                item.children[values_pos] = value\n",
      "                values_pos += 1\n",
      "                state_set_pos += len(value)\n",
      "            else:\n",
      "                break\n",
      "            \n",
      "        # otherwise we need to try all possible next matching items in the current state set\n",
      "        # so we loop over every item in the current state set looking for a completed match\n",
      "        for inner_item in reversed_state_sets[state_set_pos]:\n",
      "            if inner_item.node == value and inner_item.pos == len(inner_item.values):\n",
      "\n",
      "                # see if we can get a complete parse following this inner item\n",
      "                if self._compute_children(inner_item.start, item, reversed_state_sets, values_pos + 1):\n",
      "                    item.children[values_pos] = inner_item\n",
      "                    return True\n",
      "                    \n",
      "        # if we didn't find a child set and this is nullable we can skip this child (since it may not exist if nulled)\n",
      "        if value.nullable:\n",
      "            if self._compute_children(state_set_pos, item, reversed_state_sets, values_pos + 1):\n",
      "                item.children[values_pos] = None # this child was skipped since it was nullable\n",
      "                return True\n",
      "        \n",
      "        return False\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/_serialization_pb2.py\n",
      "```py\n",
      "# -*- coding: utf-8 -*-\n",
      "# Generated by the protocol buffer compiler.  DO NOT EDIT!\n",
      "# source: _serialization.proto\n",
      "# Protobuf Python Version: 4.25.2\n",
      "\"\"\"Generated protocol buffer code.\"\"\"\n",
      "from google.protobuf import descriptor as _descriptor\n",
      "from google.protobuf import descriptor_pool as _descriptor_pool\n",
      "from google.protobuf import symbol_database as _symbol_database\n",
      "from google.protobuf.internal import builder as _builder\n",
      "# @@protoc_insertion_point(imports)\n",
      "\n",
      "_sym_db = _symbol_database.Default()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x14_serialization.proto\\x12\\x08guidance\\\"3\\n\\x07Grammar\\x12(\\n\\x05nodes\\x18\\x01 \\x03(\\x0b\\x32\\x19.guidance.GrammarFunction\\\"\\x83\\x03\\n\\x12\\x45ngineCallResponse\\x12\\x11\\n\\tnew_bytes\\x18\\x01 \\x01(\\x0c\\x12\\x14\\n\\x0cis_generated\\x18\\x02 \\x01(\\x08\\x12\\x16\\n\\x0enew_bytes_prob\\x18\\x03 \\x01(\\x02\\x12G\\n\\x0e\\x63\\x61pture_groups\\x18\\x04 \\x03(\\x0b\\x32/.guidance.EngineCallResponse.CaptureGroupsEntry\\x12W\\n\\x17\\x63\\x61pture_group_log_probs\\x18\\x05 \\x03(\\x0b\\x32\\x36.guidance.EngineCallResponse.CaptureGroupLogProbsEntry\\x12\\x17\\n\\x0fnew_token_count\\x18\\x06 \\x01(\\x05\\x1a\\x34\\n\\x12\\x43\\x61ptureGroupsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t:\\x02\\x38\\x01\\x1a;\\n\\x19\\x43\\x61ptureGroupLogProbsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x02:\\x02\\x38\\x01\\\"w\\n\\x04\\x42yte\\x12\\x0c\\n\\x04\\x62yte\\x18\\x01 \\x01(\\x0c\\x12\\x0e\\n\\x06hidden\\x18\\x02 \\x01(\\x08\\x12\\x14\\n\\x0c\\x63ommit_point\\x18\\x03 \\x01(\\x08\\x12\\x10\\n\\x08nullable\\x18\\x04 \\x01(\\x08\\x12\\x14\\n\\x0c\\x63\\x61pture_name\\x18\\x05 \\x01(\\t\\x12\\x13\\n\\x0btemperature\\x18\\x06 \\x01(\\x02\\\"p\\n\\tByteRange\\x12\\x12\\n\\nbyte_range\\x18\\x01 \\x01(\\x0c\\x12\\x0e\\n\\x06hidden\\x18\\x03 \\x01(\\x08\\x12\\x14\\n\\x0c\\x63ommit_point\\x18\\x04 \\x01(\\x08\\x12\\x14\\n\\x0c\\x63\\x61pture_name\\x18\\x05 \\x01(\\t\\x12\\x13\\n\\x0btemperature\\x18\\x06 \\x01(\\x02\\\"\\x06\\n\\x04Null\\\"k\\n\\rModelVariable\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0e\\n\\x06hidden\\x18\\x02 \\x01(\\x08\\x12\\x14\\n\\x0c\\x63ommit_point\\x18\\x03 \\x01(\\x08\\x12\\x14\\n\\x0c\\x63\\x61pture_name\\x18\\x04 \\x01(\\t\\x12\\x10\\n\\x08nullable\\x18\\x05 \\x01(\\x08\\\"\\x86\\x01\\n\\x04Join\\x12\\x10\\n\\x08nullable\\x18\\x01 \\x01(\\x08\\x12\\x0e\\n\\x06values\\x18\\x02 \\x03(\\x05\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\\x12\\x0e\\n\\x06hidden\\x18\\x04 \\x01(\\x08\\x12\\x14\\n\\x0c\\x63ommit_point\\x18\\x05 \\x01(\\x08\\x12\\x14\\n\\x0c\\x63\\x61pture_name\\x18\\x06 \\x01(\\t\\x12\\x12\\n\\nmax_tokens\\x18\\x07 \\x01(\\x05\\\"\\x9b\\x01\\n\\x06Select\\x12\\x10\\n\\x08nullable\\x18\\x01 \\x01(\\x08\\x12\\x0e\\n\\x06values\\x18\\x02 \\x03(\\x05\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\\x12\\x0e\\n\\x06hidden\\x18\\x04 \\x01(\\x08\\x12\\x14\\n\\x0c\\x63ommit_point\\x18\\x05 \\x01(\\x08\\x12\\x14\\n\\x0c\\x63\\x61pture_name\\x18\\x06 \\x01(\\t\\x12\\x12\\n\\nmax_tokens\\x18\\x07 \\x01(\\x05\\x12\\x11\\n\\trecursive\\x18\\x08 \\x01(\\x08\\\"\\xe4\\x01\\n\\x0fGrammarFunction\\x12\\x1e\\n\\x04join\\x18\\x01 \\x01(\\x0b\\x32\\x0e.guidance.JoinH\\x00\\x12\\\"\\n\\x06select\\x18\\x02 \\x01(\\x0b\\x32\\x10.guidance.SelectH\\x00\\x12\\x1e\\n\\x04\\x62yte\\x18\\x03 \\x01(\\x0b\\x32\\x0e.guidance.ByteH\\x00\\x12)\\n\\nbyte_range\\x18\\x04 \\x01(\\x0b\\x32\\x13.guidance.ByteRangeH\\x00\\x12\\x31\\n\\x0emodel_variable\\x18\\x05 \\x01(\\x0b\\x32\\x17.guidance.ModelVariableH\\x00\\x42\\x0f\\n\\rfunction_typeb\\x06proto3')\n",
      "\n",
      "_globals = globals()\n",
      "_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)\n",
      "_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, '_serialization_pb2', _globals)\n",
      "if _descriptor._USE_C_DESCRIPTORS == False:\n",
      "  DESCRIPTOR._options = None\n",
      "  _globals['_ENGINECALLRESPONSE_CAPTUREGROUPSENTRY']._options = None\n",
      "  _globals['_ENGINECALLRESPONSE_CAPTUREGROUPSENTRY']._serialized_options = b'8\\001'\n",
      "  _globals['_ENGINECALLRESPONSE_CAPTUREGROUPLOGPROBSENTRY']._options = None\n",
      "  _globals['_ENGINECALLRESPONSE_CAPTUREGROUPLOGPROBSENTRY']._serialized_options = b'8\\001'\n",
      "  _globals['_GRAMMAR']._serialized_start=34\n",
      "  _globals['_GRAMMAR']._serialized_end=85\n",
      "  _globals['_ENGINECALLRESPONSE']._serialized_start=88\n",
      "  _globals['_ENGINECALLRESPONSE']._serialized_end=475\n",
      "  _globals['_ENGINECALLRESPONSE_CAPTUREGROUPSENTRY']._serialized_start=362\n",
      "  _globals['_ENGINECALLRESPONSE_CAPTUREGROUPSENTRY']._serialized_end=414\n",
      "  _globals['_ENGINECALLRESPONSE_CAPTUREGROUPLOGPROBSENTRY']._serialized_start=416\n",
      "  _globals['_ENGINECALLRESPONSE_CAPTUREGROUPLOGPROBSENTRY']._serialized_end=475\n",
      "  _globals['_BYTE']._serialized_start=477\n",
      "  _globals['_BYTE']._serialized_end=596\n",
      "  _globals['_BYTERANGE']._serialized_start=598\n",
      "  _globals['_BYTERANGE']._serialized_end=710\n",
      "  _globals['_NULL']._serialized_start=712\n",
      "  _globals['_NULL']._serialized_end=718\n",
      "  _globals['_MODELVARIABLE']._serialized_start=720\n",
      "  _globals['_MODELVARIABLE']._serialized_end=827\n",
      "  _globals['_JOIN']._serialized_start=830\n",
      "  _globals['_JOIN']._serialized_end=964\n",
      "  _globals['_SELECT']._serialized_start=967\n",
      "  _globals['_SELECT']._serialized_end=1122\n",
      "  _globals['_GRAMMARFUNCTION']._serialized_start=1125\n",
      "  _globals['_GRAMMARFUNCTION']._serialized_end=1353\n",
      "# @@protoc_insertion_point(module_scope)\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/_server.py\n",
      "```py\n",
      "from fastapi import FastAPI, Request, HTTPException, Security\n",
      "from fastapi.security import APIKeyHeader\n",
      "from fastapi.responses import StreamingResponse\n",
      "import os  # For environment variables or config files\n",
      "import base64\n",
      "\n",
      "from .models._model import Model, Engine\n",
      "from ._grammar import GrammarFunction\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "\n",
      "class GuidanceRequest(BaseModel):\n",
      "    parser: str = Field(\n",
      "        title=\"parser\", description=\"The text generated so far by the guidance program\"\n",
      "    )\n",
      "    grammar: str = Field(\n",
      "        title=\"grammar\",\n",
      "        description=\"Guidance grammar to constrain the next characters generated\",\n",
      "    )\n",
      "\n",
      "\n",
      "class Server:\n",
      "    def __init__(self, engine, api_key=None, ssl_certfile=None, ssl_keyfile=None):\n",
      "        \"\"\"This exposes an Engine object over the network.\"\"\"\n",
      "\n",
      "        if isinstance(engine, Model):\n",
      "            engine = engine.engine\n",
      "        elif not isinstance(engine, Engine):\n",
      "            raise TypeError(\"engine must be an Engine object\")\n",
      "        self.engine = engine\n",
      "        self.app = FastAPI()\n",
      "        self.valid_api_keys = self._load_api_keys(api_key)\n",
      "        if ssl_certfile is None:\n",
      "            ssl_certfile = os.getenv(\"GUIDANCE_SSL_CERTFILE\")\n",
      "        if ssl_keyfile is None:\n",
      "            ssl_keyfile = os.getenv(\"GUIDANCE_SSL_KEYFILE\")\n",
      "        self.ssl_certfile = ssl_certfile\n",
      "        self.ssl_keyfile = ssl_keyfile\n",
      "\n",
      "        api_key_header = APIKeyHeader(name=\"x-api-key\", auto_error=False)\n",
      "\n",
      "        # def get_api_key(api_key_header: str = Security(api_key_header)) -> str:\n",
      "        #     if api_key_header in self.valid_api_keys:\n",
      "        #         return api_key_header\n",
      "        #     raise HTTPException(\n",
      "        #         status_code=status.HTTP_401_UNAUTHORIZED,\n",
      "        #         detail=\"Invalid or missing API Key\",\n",
      "        #     )\n",
      "\n",
      "        @self.app.post(\"/extend\")\n",
      "        async def extend_parser(\n",
      "            guidance_request: GuidanceRequest, x_api_key: str = Security(api_key_header)\n",
      "        ):\n",
      "            if x_api_key not in self.valid_api_keys:\n",
      "                raise HTTPException(status_code=401, detail=\"Invalid API key\")\n",
      "\n",
      "            # data = await request.json()\n",
      "            # parser = data.get(\"parser\")\n",
      "            grammar = GrammarFunction.deserialize(\n",
      "                base64.b64decode(guidance_request.grammar)\n",
      "            )\n",
      "\n",
      "            return StreamingResponse(\n",
      "                self.engine(guidance_request.parser, grammar),\n",
      "                media_type=\"application/json\",\n",
      "            )\n",
      "\n",
      "    def _load_api_keys(self, api_key):\n",
      "        valid_api_keys = set()\n",
      "        if api_key is None:\n",
      "            api_key = os.getenv(\"GUIDANCE_API_KEY\")\n",
      "            if api_key:\n",
      "                valid_api_keys.add(api_key)\n",
      "        else:\n",
      "            valid_api_keys.add(api_key)\n",
      "        return valid_api_keys\n",
      "\n",
      "    def run(self, host=\"localhost\", port=8000):\n",
      "        # Use uvicorn or another ASGI server to run\n",
      "        import uvicorn\n",
      "\n",
      "        uvicorn.run(\n",
      "            self.app,\n",
      "            host=host,\n",
      "            port=port,\n",
      "            ssl_certfile=self.ssl_certfile,\n",
      "            ssl_keyfile=self.ssl_keyfile,\n",
      "        )  # use host=\"0.0.0.0\" for remote access\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/_utils.py\n",
      "```py\n",
      "import os\n",
      "import requests\n",
      "import inspect\n",
      "import json\n",
      "import asyncio\n",
      "import queue\n",
      "import ast\n",
      "import types\n",
      "import textwrap\n",
      "import sys\n",
      "import numpy as np\n",
      "\n",
      "class _Rewrite(ast.NodeTransformer):\n",
      "    def visit_Constant(self, node):\n",
      "        if isinstance(node.value, str) and node.lineno < node.end_lineno:\n",
      "            self.start_counts[node.lineno-1] += 1\n",
      "            start_line = self.source_lines[node.lineno-1]\n",
      "            start_string = start_line[node.col_offset:]\n",
      "            \n",
      "            # check for literal multiline strings\n",
      "            if start_string.startswith(\"f'''\") or start_string.startswith(\"'''\") or start_string.startswith('f\"\"\"') or start_string.startswith('\"\"\"'):\n",
      "                \n",
      "                # track our indentation level\n",
      "                if self.indentation[node.lineno-1] is None:\n",
      "                    indent = start_line[:len(start_line) - len(start_line.lstrip())]\n",
      "                    for i in range(node.lineno-1, node.end_lineno):\n",
      "                        self.indentation[i] = indent\n",
      "                indent = self.indentation[node.lineno-1]\n",
      "\n",
      "                # strip indentation when it is consistent\n",
      "                lines = node.value.split(\"\\n\")\n",
      "                fail = False\n",
      "                new_lines = []\n",
      "                for i,line in enumerate(lines):\n",
      "                    if (i == 0 and (self.start_counts[node.lineno-1] > 1 or not start_line.endswith(\"\\\\\"))) or line == \"\":\n",
      "                        new_lines.append(line)\n",
      "                    elif line.startswith(indent):\n",
      "                        new_lines.append(line[len(indent):])\n",
      "                    # elif (i == 0 and line.endswith(\"\\\\\")) or line == \"\":\n",
      "                    #     new_lines.append(line)\n",
      "                    else:\n",
      "                        fail = True\n",
      "                        break\n",
      "                if not fail:\n",
      "                    node.value = \"\\n\".join(new_lines)\n",
      "\n",
      "        return node\n",
      "class normalize_notebook_stdout_stderr():\n",
      "    '''Remaps stdout and stderr back to their normal selves from what ipykernel did to them.\n",
      "    \n",
      "    Based on: https://github.com/ipython/ipykernel/issues/795\n",
      "    '''\n",
      "\n",
      "    def __enter__(self):\n",
      "        normal_stdout = sys.__stdout__.fileno()\n",
      "        self.restore_stdout = None\n",
      "        if getattr(sys.stdout, \"_original_stdstream_copy\", normal_stdout) != normal_stdout:\n",
      "            self.restore_stdout = sys.stdout._original_stdstream_copy\n",
      "            sys.stdout._original_stdstream_copy = normal_stdout\n",
      "\n",
      "        normal_stderr = sys.__stderr__.fileno()\n",
      "        self.restore_stderr = None\n",
      "        if getattr(sys.stderr, \"_original_stdstream_copy\", normal_stderr) != normal_stderr:\n",
      "            self.restore_stderr = sys.stderr._original_stdstream_copy\n",
      "            sys.stderr._original_stdstream_copy = normal_stderr\n",
      "\n",
      "    def __exit__(self, exc_type, exc_value, traceback):\n",
      "        if self.restore_stdout is not None:\n",
      "            sys.stderr._original_stdstream_copy = self.restore_stdout\n",
      "        if self.restore_stderr is not None:\n",
      "            sys.stderr._original_stdstream_copy = self.restore_stderr\n",
      "\n",
      "def strip_multiline_string_indents(f):\n",
      "\n",
      "    source = textwrap.dedent(inspect.getsource(f))\n",
      "    blanks = '\\n' * f.__code__.co_firstlineno # padd the source so the lines in the file line up for the debugger\n",
      "    source = blanks + '\\n'.join(source.splitlines()[1:]) # remove the decorator first line.\n",
      "    \n",
      "    # define the external closure variables so f.__closure__ will match our recompiled version\n",
      "    if len(f.__code__.co_freevars) > 0:\n",
      "        raise Exception(\"You currently must use @guidance(dedent=False) for closure functions (function nested within other functions that reference the outer functions variables)!\")\n",
      "        lines = source.split(\"\\n\")\n",
      "        lines[0] = \"def __outer__closure_wrap():\"\n",
      "        lines[1] = \"    \" + \",\".join(f.__code__.co_freevars) + \" = \" + \",\".join(\"None\" for _ in f.__code__.co_freevars)\n",
      "        source = \"    \\n\".join(lines) # TODO: this does not quite work because new_code_obj is now the __outer__closure_wrap() function...could be fixed with work...\n",
      "\n",
      "    old_code_obj = f.__code__\n",
      "    old_ast = ast.parse(source)\n",
      "    r = _Rewrite()\n",
      "    r.source_lines = source.split(\"\\n\")\n",
      "    r.indentation = [None for l in r.source_lines]\n",
      "    r.start_counts = [0 for l in r.source_lines]\n",
      "    # r._avoid_backslashes = True\n",
      "    new_ast = r.visit(old_ast)\n",
      "    new_code_obj = compile(new_ast, old_code_obj.co_filename, 'exec')\n",
      "\n",
      "    # find the code block\n",
      "    for i in range(len(new_code_obj.co_consts)):\n",
      "        if str(type(new_code_obj.co_consts[i])) == \"<class 'code'>\":\n",
      "            break\n",
      "\n",
      "    # create a new function based on the modified code\n",
      "    new_f = types.FunctionType(\n",
      "        new_code_obj.co_consts[i],\n",
      "        f.__globals__,\n",
      "        name=f.__name__,\n",
      "        argdefs=f.__defaults__,\n",
      "        closure=f.__closure__\n",
      "    )\n",
      "    new_f.__kwdefaults__ = f.__kwdefaults__\n",
      "    return new_f\n",
      "\n",
      "class CaptureEvents():\n",
      "    \"\"\"Creates a scope where all the events are captured in a queue.\n",
      "    \n",
      "    Note that this does not stop the events from being captured by higher level scopes.\n",
      "    \"\"\"\n",
      "    def __init__(self, lm):\n",
      "        self.lm = lm\n",
      "    \n",
      "    def __enter__(self):\n",
      "        self.lm._event_queue = queue.Queue()\n",
      "        return self.lm._event_queue\n",
      "\n",
      "    def __exit__(self, type, value, traceback):\n",
      "        self.lm._event_queue = None\n",
      "\n",
      "class JupyterComm():\n",
      "    def __init__(self, target_id, ipython_handle, callback=None, on_open=None, mode=\"register\"):\n",
      "        from ipykernel.comm import Comm\n",
      "\n",
      "        self.target_name = \"guidance_interface_target_\" + target_id\n",
      "        # print(\"TARGET NAME\", self.target_name)\n",
      "        self.callback = callback\n",
      "        self.jcomm = None\n",
      "        self.ipython_handle = ipython_handle\n",
      "        self.addd = 1\n",
      "        self.send_queue = asyncio.Queue()\n",
      "        self.open_event = asyncio.Event()\n",
      "        self.is_open = False\n",
      "        asyncio.get_event_loop().create_task(self._send_loop())\n",
      "        if mode == \"register\":\n",
      "            # log(\"REGISTERING\", self.target_name)\n",
      "            # asyncio.get_event_loop().create_task(self._register())\n",
      "            def comm_opened(comm, open_msg):\n",
      "                # log(\"OPENED\")\n",
      "                self.addd = 2\n",
      "                self.jcomm = comm\n",
      "                self.is_open = True\n",
      "                self.jcomm.on_msg(self._fire_callback)\n",
      "                self.open_event.set()\n",
      "                self._fire_callback({\"content\": {\"data\": {\"event\": \"opened\"}}})\n",
      "\n",
      "            self.ipython_handle.kernel.comm_manager.register_target(self.target_name, comm_opened)\n",
      "            # get_ipython().kernel.comm_manager.register_target(self.target_name, comm_opened) # noqa: F821\n",
      "        elif mode == \"open\":\n",
      "            # log(\"OPENING\", self.target_name)\n",
      "            self.jcomm = Comm(target_name=self.target_name)\n",
      "            self.jcomm.on_msg(self._fire_callback)\n",
      "            # self._fire_callback({\"content\": {\"data\": \"opened\"}})\n",
      "        else:\n",
      "            raise Exception(\"Passed mode must be either 'open' or 'register'!\")\n",
      "\n",
      "    def clear_send_queue(self):\n",
      "        while not self.send_queue.empty():\n",
      "            self.send_queue.get_nowait()\n",
      "            self.send_queue.task_done()\n",
      "\n",
      "    def _fire_callback(self, msg):\n",
      "        self.callback(msg[\"content\"][\"data\"])\n",
      "\n",
      "    def send(self, data):\n",
      "        self.send_queue.put_nowait(data)\n",
      "\n",
      "    async def _send_loop(self):\n",
      "        while True:\n",
      "            # log(\"SENDING_LOOP\")\n",
      "            if self.jcomm is None:\n",
      "                self.open_event.clear()\n",
      "                await self.open_event.wait()\n",
      "            data = await self.send_queue.get()\n",
      "            # log(\"SENDING_LOOP got one!\")\n",
      "            self.jcomm.send({\"data\": json.dumps(data)})\n",
      "\n",
      "    # async def _waiting_send(self, data):\n",
      "    #     #log(\"SENDING\", self.jcomm, data)\n",
      "\n",
      "    #     # await the open event if needed\n",
      "    #     if self.jcomm is None:\n",
      "    #         self.open_event.clear()\n",
      "    #         await self.open_event.wait()\n",
      "    #     #log(\"SENDING_now\", self.jcomm, data)\n",
      "    #     self.jcomm.send({\"data\": json.dumps(data)}) # we encode the JSON so iPython doesn't mess it up\n",
      "\n",
      "\n",
      "# https://stackoverflow.com/questions/15411967/how-can-i-check-if-code-is-executed-in-the-ipython-notebook\n",
      "def is_interactive():\n",
      "    import __main__ as main\n",
      "    return not hasattr(main, '__file__')\n",
      "\n",
      "\n",
      "def log_softmax(array: np.ndarray, axis: int = -1) -> np.ndarray:\n",
      "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.log_softmax.html\n",
      "    array_maxs: np.ndarray = np.amax(array, axis=axis, keepdims=True)\n",
      "    if array_maxs.ndim > 0:\n",
      "        array_maxs[~np.isfinite(array_maxs)] = 0\n",
      "    elif not np.isfinite(array_maxs):\n",
      "        array_maxs = 0\n",
      "    subtract_maxs = array - array_maxs\n",
      "    exp = np.exp(subtract_maxs)\n",
      "    # suppress warnings about log of zero\n",
      "    with np.errstate(divide='ignore'):\n",
      "        summed = np.sum(exp, axis=axis, keepdims=True)\n",
      "        out = np.log(summed)\n",
      "    return subtract_maxs - out\n",
      "\n",
      "\n",
      "def softmax(array: np.ndarray, axis: int = -1) -> np.ndarray:\n",
      "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html\n",
      "    array_maxs = np.amax(array, axis=axis, keepdims=True)\n",
      "    exp_x_shifted = np.exp(array - array_maxs)\n",
      "    return exp_x_shifted / np.sum(exp_x_shifted, axis=axis, keepdims=True)\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_tool.py\n",
      "```py\n",
      "import guidance\n",
      "from ._any_char import any_char\n",
      "from .._grammar import select, capture, string, commit_point\n",
      "from ._zero_or_more import zero_or_more\n",
      "from ._one_or_more import one_or_more\n",
      "from ._any_char_but import any_char_but\n",
      "from ._any_char import any_char\n",
      "\n",
      "class Tool:\n",
      "    def __init__(self, call_grammar=None, tool_call=None, callable=None):\n",
      "        \"\"\"\n",
      "        Create a tool that can be called within a guidance program.\n",
      "\n",
      "        The Tool class encapsulates the grammar for calling a tool and the function\n",
      "        that performs the tool's action. It can work with either a predefined grammar\n",
      "        and function pair or a callable that is converted into a grammar call.\n",
      "\n",
      "        Args:\n",
      "            call_grammar: The grammar that specifies how the tool is called.\n",
      "            tool_call: The function that is called when the tool grammar is matched.\n",
      "            callable: Guidance function or regular callable, will be converted to grammar\n",
      "        Raises:\n",
      "            Exception: If the arguments do not match the expected pattern of either\n",
      "                        providing both a grammar and a function or a callable.\n",
      "        \"\"\"\n",
      "        # call_grammar specifies how the tool can be called. Crucially, it has to capture the args in variable 'tool_args'\n",
      "        # tool_call is a guidance function  actually calls the tool, and returns an lm object with whatever outputs it wants\n",
      "        # callable: guidance function or regular callable, will be converted to grammar\n",
      "        # TODO: hidden is not working yet\n",
      "        first_option = (call_grammar is not None) and (tool_call is not None)\n",
      "        second_option = callable is not None\n",
      "        # either both are true or both false\n",
      "        if first_option == second_option:\n",
      "            raise Exception(\"Must pass either (call_grammar, tool call) or callable, but not both or neither\")\n",
      "        if second_option:\n",
      "            call_grammar, tool_call = fn_to_grammar_call(callable)\n",
      "        self.call_grammar = call_grammar\n",
      "        self.tool_call = tool_call\n",
      "\n",
      "def valid_chars():\n",
      "    return any_char_but(['=', ')'])\n",
      "def positional_arg():\n",
      "    return one_or_more(valid_chars())\n",
      "def kwarg():\n",
      "    return one_or_more(valid_chars()) + '=' + one_or_more(valid_chars())\n",
      "\n",
      "def basic_func_grammar(name):\n",
      "    obj = string(name + '(')\n",
      "    obj += capture(select([zero_or_more(positional_arg()), ''])\n",
      "        + select([zero_or_more(kwarg()), '']), name='tool_args')\n",
      "    obj += string(')')\n",
      "    return obj\n",
      "\n",
      "def fn_to_grammar_call(callable):\n",
      "    # TODO later: validate the call. Here is code to get required and optional args of 'guidance_fn':\n",
      "    # name = guidance_fn.__name__\n",
      "    # required_args = []\n",
      "    # optional_args = []\n",
      "    # sig = inspect.signature(guidance_fn)\n",
      "    # for i, x in enumerate(sig.parameters.values()):\n",
      "    #     if i == 0:\n",
      "    #         continue\n",
      "    #     if x.default is x.empty:\n",
      "    #         required_args.append(x.name)\n",
      "    #     else:\n",
      "    #         optional_args.append(x.name)\n",
      "    name = callable.__name__\n",
      "    call_grammar = basic_func_grammar(name)\n",
      "    @guidance(dedent=False)\n",
      "    def basic_tool_call(lm):\n",
      "        args = lm['tool_args']\n",
      "        args = args.split(',')\n",
      "        positional = [x.strip() for x in args if '=' not in x]\n",
      "        kwargs = dict([tuple(x.strip().split('=')) for x in args if '=' in x])\n",
      "        lm += callable(*positional, **kwargs)\n",
      "        return lm\n",
      "    return call_grammar, basic_tool_call\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_any_char.py\n",
      "```py\n",
      "import guidance\n",
      "from .._grammar import byte_range\n",
      "\n",
      "@guidance(stateless=True)\n",
      "def any_char(lm):\n",
      "    \"\"\"\n",
      "    Generates a grammar function that matches any single byte character.\n",
      "\n",
      "    This function extends the given language model (`lm`) by appending a grammar\n",
      "    function that allows for any single byte character in the range from 0x00 to 0xff,\n",
      "    which includes all ASCII characters.\n",
      "    \"\"\"\n",
      "    # TODO: extend this to support utf-8 encoded multibyte unicode characters\n",
      "    return lm + byte_range(b'\\x00', b'\\xff')\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_zero_or_more.py\n",
      "```py\n",
      "import guidance\n",
      "from .._grammar import select\n",
      "\n",
      "@guidance(stateless=True)\n",
      "def zero_or_more(model, value):\n",
      "    \"\"\"\n",
      "    Creates a grammar function that matches zero or more occurrences of a given pattern,\n",
      "    equivalent to the Kleene star operator (*) in regular expressions.\n",
      "\n",
      "    Args:\n",
      "        model: The language model state to which the zero-or-more grammar function\n",
      "            will be appended.\n",
      "        value: The pattern to match zero or more times. Can be a string or another\n",
      "            grammar function.\n",
      "\n",
      "    Returns:\n",
      "        The language model state with the appended grammar function that matches\n",
      "        zero or more occurrences of the specified pattern.\n",
      "\n",
      "    Example:\n",
      "        >>> lm += zero_or_more(lm, 'word ')\n",
      "        # lm now includes a grammar function that can match the pattern 'word ' zero or more times.\n",
      "    \"\"\"\n",
      "    return model + select([\"\", value], recurse=True)\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_char_range.py\n",
      "```py\n",
      "from .._grammar import byte_range\n",
      "    \n",
      "def char_range(low: str, high: str):\n",
      "    \"\"\"\n",
      "    Creates a grammar function that matches any single character within a specified range.\n",
      "\n",
      "    This function takes two single-character strings representing the lower and upper bounds\n",
      "    of a character range, and returns a grammar function that matches any character within\n",
      "    that range, inclusive. Only single-byte character ranges are currently supported.\n",
      "\n",
      "    Args:\n",
      "        low (str): A single-byte string representing the low end of the character range.\n",
      "        high (str): A single-byte string representing the high end of the character range.\n",
      "\n",
      "    Returns:\n",
      "        A grammar function that matches any single character within the specified range.\n",
      "\n",
      "    Raises:\n",
      "        Exception: If either `low` or `high` is a multibyte character.\n",
      "\n",
      "    Example:\n",
      "        >>> char_range('a', 'z')\n",
      "        # This will return a grammar function that matches any lowercase letter from a to z.\n",
      "    \"\"\"\n",
      "    low_bytes = bytes(low, encoding=\"utf8\")\n",
      "    high_bytes = bytes(high, encoding=\"utf8\")\n",
      "    if len(low_bytes) > 1 or len(high_bytes) > 1:\n",
      "        raise Exception(\"We don't yet support multi-byte character ranges!\")\n",
      "    return byte_range(low_bytes, high_bytes)\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_optional.py\n",
      "```py\n",
      "import guidance\n",
      "from .._grammar import select\n",
      "\n",
      "@guidance(stateless=True)\n",
      "def optional(lm, value):\n",
      "    \"\"\"\n",
      "    Creates a grammar function that matches an optional element in a pattern.\n",
      "    This is functionally equivalent to select([\"\", value]).\n",
      "\n",
      "    Args:\n",
      "        lm: The language model state to which the optional grammar function\n",
      "            will be appended.\n",
      "        value: The pattern that is optional in the generated text. Can be a string\n",
      "            or another grammar function.\n",
      "\n",
      "    Returns:\n",
      "        The language model state with the appended grammar function that matches\n",
      "        an optional occurrence of the specified pattern.\n",
      "\n",
      "    Example:\n",
      "        >>> lm += optional(\"perhaps \")\n",
      "        # lm now includes a grammar function for the optional word \"perhaps\".\n",
      "    \"\"\"\n",
      "    return lm + select([value, \"\"])\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_any_char_but.py\n",
      "```py\n",
      "import guidance\n",
      "from .._grammar import byte_range, select\n",
      "\n",
      "@guidance(stateless=True)\n",
      "def any_char_but(lm, forbidden):\n",
      "    \"\"\"Allows any char except those in forbidden\"\"\"\n",
      "    # TODO: extend this to support utf-8 encoded multibyte unicode characters\n",
      "    forb = sorted(set([ord(x) for x in forbidden]))\n",
      "    start = 0\n",
      "    ranges = []\n",
      "    for i in forb:\n",
      "        if i == 0:\n",
      "            continue\n",
      "        newrange = (start, i - 1)\n",
      "        if newrange[0] < newrange[1]:\n",
      "            ranges.append(newrange)\n",
      "        start = i + 1\n",
      "    if start < 127:\n",
      "        ranges.append((start, 127))\n",
      "    ranges = [(i.to_bytes(1, 'big'), j.to_bytes(1, 'big')) for i, j in ranges]\n",
      "    return select([byte_range(x[0], x[1]) for x in ranges])\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/__init__.py\n",
      "```py\n",
      "# import functions that can be called directly\n",
      "from ._gen import gen, call_tool, will_gen\n",
      "from ._image import image\n",
      "\n",
      "# core grammar functions\n",
      "from .._grammar import select\n",
      "from .._grammar import commit_point\n",
      "from .._grammar import with_temperature\n",
      "from .._grammar import string\n",
      "from .._grammar import token_limit\n",
      "from .._grammar import capture\n",
      "from .._grammar import byte_range\n",
      "\n",
      "# context blocks\n",
      "from ._block import block\n",
      "from ._role import role, system, assistant, user, function, instruction, indent_roles\n",
      "from ._format import monospace\n",
      "from ._silent import silent\n",
      "from ._set_var import set_var\n",
      "from ._set_attribute import set_attribute\n",
      "# from ..models._model import context_free\n",
      "\n",
      "# stateless library functions\n",
      "from ._any_char import any_char\n",
      "from ._zero_or_more import zero_or_more\n",
      "from ._one_or_more import one_or_more\n",
      "from ._char_range import char_range\n",
      "from ._char_set import char_set\n",
      "from ._prefix_tree import prefix_tree\n",
      "from ._substring import substring, substring_no_empty\n",
      "from ._regex import regex\n",
      "from ._optional import optional\n",
      "from ._tool import Tool\n",
      "from ._any_char_but import any_char_but\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_image.py\n",
      "```py\n",
      "import guidance\n",
      "import urllib\n",
      "import typing\n",
      "import http\n",
      "import re\n",
      "\n",
      "@guidance\n",
      "def image(lm, src, allow_local=True):\n",
      "    \"\"\"\n",
      "    Appends an image tag to the language model state with the source specified.\n",
      "\n",
      "    This function handles loading image data from a URL, a local file path, or directly\n",
      "    from image bytes. It then appends an image tag to the model state that references\n",
      "    the image data. The function supports loading images from local paths if `allow_local`\n",
      "    is set to True.\n",
      "\n",
      "    Args:\n",
      "        lm: The language model state to which the image tag will be appended.\n",
      "        src (str or bytes): The source of the image. Can be a URL, a local file path,\n",
      "                            or raw image bytes.\n",
      "        allow_local (bool, optional): If True, allows loading images from local file paths.\n",
      "                                    Defaults to True.\n",
      "\n",
      "    Returns:\n",
      "        The language model state with the appended image tag referencing the loaded image data.\n",
      "\n",
      "    Raises:\n",
      "        Exception: If the image data cannot be loaded from the specified source.\n",
      "\n",
      "    Example:\n",
      "        >>> lm = LanguageModelState()\n",
      "        >>> lm = image(lm, 'https://example.com/image.png')\n",
      "        # lm now includes an image tag with the source pointing to the provided URL.\n",
      "    \"\"\"\n",
      "\n",
      "    # load the image bytes\n",
      "    # ...from a url\n",
      "    if isinstance(src, str) and re.match(r'$[^:/]+://', src):\n",
      "        with urllib.request.urlopen(src) as response:\n",
      "            response = typing.cast(http.client.HTTPResponse, response)\n",
      "            bytes_data = response.read()\n",
      "    \n",
      "    # ...from a local path\n",
      "    elif allow_local and isinstance(src, str):\n",
      "        with open(src, \"rb\") as f:\n",
      "            bytes_data = f.read()\n",
      "\n",
      "    # ...from image file bytes\n",
      "    elif isinstance(src, bytes):\n",
      "        bytes_data = src\n",
      "        \n",
      "    else:\n",
      "        raise Exception(f\"Unable to load image bytes from {src}!\")\n",
      "\n",
      "    bytes_id = str(id(bytes_data))\n",
      "\n",
      "    # set the image bytes\n",
      "    lm = lm.set(bytes_id, bytes_data)\n",
      "    lm += f'<|_image:{bytes_id}|>'\n",
      "    return lm\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_regex.py\n",
      "```py\n",
      "import guidance\n",
      "from .._grammar import byte_range, select, string, Byte\n",
      "from ._zero_or_more import zero_or_more\n",
      "from ._any_char_but import any_char_but\n",
      "import re\n",
      "from types import SimpleNamespace\n",
      "from pyformlang.regular_expression import PythonRegex\n",
      "\n",
      "\n",
      "@guidance(stateless=True, dedent=False)\n",
      "def regex(lm, pattern):\n",
      "    \"\"\"\n",
      "    Creates a guidance grammar based on a regular expression pattern.\n",
      "\n",
      "    Args:\n",
      "        lm: The language model state to which the regex grammar will be appended.\n",
      "        pattern (str): A regular expression pattern defining the text to match.\n",
      "\n",
      "    Returns:\n",
      "        The language model state with the appended grammar that matches the regex pattern.\n",
      "\n",
      "    Raises:\n",
      "        Exception: If the simplification process encounters multibyte character ranges\n",
      "                    or other unsupported regex features.\n",
      "\n",
      "    Example:\n",
      "        >>> lm += regex(lm, r'^[A-Za-z0-9]+$')\n",
      "        # lm now includes a grammar function that matches alphanumeric strings.\n",
      "    \"\"\"\n",
      "    # find all of the brackets we'll need to negate later\n",
      "    nots = re.findall('\\[\\^(.*?)\\]', pattern)\n",
      "    nots = [re.compile('[' + x + ']') for x in nots]\n",
      "    # Compensating for a weird design choice in pyformlang where they don't accept \\n in .\n",
      "    pattern = re.sub(r'(?<!\\\\)\\.', '(.|\\n)', pattern)\n",
      "    regex = PythonRegex(pattern)\n",
      "    cfg = tree_to_grammar(simplify_tree(regex), nots=nots)\n",
      "    return cfg\n",
      "\n",
      "\n",
      "# This is just a helper class so I can merge nodes without having to worry about pyformlang types\n",
      "class FakeNode:\n",
      "    def __init__(self, value, sons):\n",
      "        self.head = SimpleNamespace()\n",
      "        self.head.value = value\n",
      "        self.sons = sons\n",
      "    def get_tree_str(self, depth: int = 0) -> str:\n",
      "        \"\"\" Get a string representation of the tree behind the regex\n",
      "        \"\"\"\n",
      "        temp = \" \" * depth + str(self.head) + \"\\n\"\n",
      "        for son in self.sons:\n",
      "            temp += son.get_tree_str(depth + 1)\n",
      "        return temp\n",
      "\n",
      "\n",
      "# \n",
      "def tree_to_grammar(node, nots):\n",
      "    \"\"\"Takes a pyformlang regex tree and returns a guidance gramar\n",
      "    \"\"\"\n",
      "    if not node.sons:\n",
      "        val = node.head.value\n",
      "        if val == 'Epsilon' or val == 'Empty':\n",
      "            val = ''\n",
      "        else:\n",
      "            val = string(val)\n",
      "        return val\n",
      "    if node.head.value == 'Union':\n",
      "        vals = [tree_to_grammar(x, nots) for x in node.sons]\n",
      "        # If select starts with ^ and is a select between bytes and it is listed in our negations, negate it. The way I implemented it is a major hack, should fix\n",
      "        if all([isinstance(x, Byte) for x in vals]) and vals[0].byte == b'^':\n",
      "            temp_vals = [x.byte for x in vals[1:]]\n",
      "            all_chars = b''.join(temp_vals).decode('utf8')\n",
      "            for notz in nots:\n",
      "                # The not matches all of the chars.\n",
      "                # TODO: Technically there could be a situation where this is true AND this is not actually what we had in a regex, so this code is wrong. But it's probably never going to happen, and we'll replace this code later.\n",
      "                if all([notz.match(x) is not None for x in all_chars]):\n",
      "                    return any_char_but(temp_vals)\n",
      "\n",
      "            # if b''.join(temp_vals).decode('utf8') in nots:\n",
      "            #     # print('Negating', temp_vals)\n",
      "            #     return select([byte_range(x[0], x[1]) for x in negative_byte_range(temp_vals)])\n",
      "        return select(vals)\n",
      "    if node.head.value == 'Concatenation':\n",
      "        ret = ''\n",
      "        for x in node.sons:\n",
      "            ret += tree_to_grammar(x, nots)\n",
      "        return ret\n",
      "    if node.head.value == 'Kleene Star':\n",
      "        return zero_or_more(tree_to_grammar(node.sons[0], nots))\n",
      "\n",
      "\n",
      "def simplify_tree(regex):\n",
      "    \"\"\"Merges sequence of byte-based concats or unions into single nodes, to make the grammar more compact\n",
      "    \"\"\"\n",
      "    if len(regex.sons) and regex.head.value in ['Concatenation', 'Union']:\n",
      "        regex = merge_nodes(regex, regex.head.value)\n",
      "    regex.sons = [simplify_tree(x) for x in regex.sons]\n",
      "    return regex\n",
      "\n",
      "def merge_nodes(regex, op):\n",
      "    \"\"\"Merges node sequences of concats and unions.\n",
      "    op must be 'Concatenation' or 'Union'\n",
      "    \"\"\"\n",
      "    assert op in ['Concatenation', 'Union']\n",
      "    current = regex\n",
      "    if len(current.sons) and current.head.value == op:\n",
      "        val = []\n",
      "        to_visit = [current]\n",
      "        while to_visit:\n",
      "            current = to_visit.pop()\n",
      "            leaves = [x for x in current.sons if len(x.sons) == 0]\n",
      "            ops = [x for x in current.sons if len(x.sons) and x.head.value == op]\n",
      "            if len(leaves) + len(ops) == len(current.sons):\n",
      "                val.extend([x.head.value for x in leaves])\n",
      "                to_visit += [x for x in current.sons if len(x.sons) and x.head.value == op]\n",
      "        # if all I have is leaves, I can group them\n",
      "        if all([len(x.sons) == 0 for x in current.sons]) and current.head.value == op:\n",
      "            # val.extend([x.head.value for x in current.sons])\n",
      "            if op == 'Concatenation':\n",
      "                val = ''.join(val)\n",
      "                new_node = FakeNode(val, [])\n",
      "            elif op == 'Union':\n",
      "                new_node = FakeNode('Union', [FakeNode(x, []) for x in val])\n",
      "            return new_node\n",
      "    if len(val) > 1:\n",
      "        if op == 'Concatenation':\n",
      "            # merge all chars into a string, return a concat between the string (as a leaf) and whatever right-side children exist\n",
      "            val = ''.join(val)\n",
      "            new_node = FakeNode(op, [FakeNode(val, []), merge_nodes(current, op)])\n",
      "        elif op == 'Union':\n",
      "            # Merge all select options into a left leaf of a select, whatever operation is left becomes the right child.\n",
      "            new_node = FakeNode(op, [FakeNode('Union', [FakeNode(x, []) for x in val]), merge_nodes(current, op)])\n",
      "        return new_node\n",
      "    else:\n",
      "        return regex\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_role.py\n",
      "```py\n",
      "import guidance\n",
      "from ._block import block\n",
      "from ._set_attribute import set_attribute\n",
      "\n",
      "nodisp_start = \"<||_#NODISP_||>\"\n",
      "nodisp_end = \"<||_/NODISP_||>\"\n",
      "span_start = \"<||_html:<span style='background-color: rgba(255, 180, 0, 0.3); border-radius: 3px;'>_||>\"\n",
      "span_end = \"<||_html:</span>_||>\"\n",
      "\n",
      "\n",
      "@guidance\n",
      "def role_opener(lm, role_name, **kwargs):\n",
      "    indent = getattr(lm, \"indent_roles\", True)\n",
      "    if not hasattr(lm, \"get_role_start\"):\n",
      "        raise Exception(\n",
      "            f\"You need to use a chat model in order the use role blocks like `with {role_name}():`! Perhaps you meant to use the {type(lm).__name__}Chat class?\"\n",
      "        )\n",
      "\n",
      "    # Block start container (centers elements)\n",
      "    if indent:\n",
      "        lm += f\"<||_html:<div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2);  justify-content: center; align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>{role_name.lower()}</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>_||>\"\n",
      "\n",
      "    # Start of either debug or HTML no disp block\n",
      "    if indent:\n",
      "        lm += nodisp_start\n",
      "    else:\n",
      "        lm += span_start\n",
      "\n",
      "    lm += lm.get_role_start(role_name, **kwargs)\n",
      "\n",
      "    # End of either debug or HTML no disp block\n",
      "    if indent:\n",
      "        lm += nodisp_end\n",
      "    else:\n",
      "        lm += span_end\n",
      "\n",
      "    return lm\n",
      "\n",
      "\n",
      "@guidance\n",
      "def role_closer(lm, role_name, **kwargs):\n",
      "    indent = getattr(lm, \"indent_roles\", True)\n",
      "    # Start of either debug or HTML no disp block\n",
      "    if indent:\n",
      "        lm += nodisp_start\n",
      "    else:\n",
      "        lm += span_start\n",
      "\n",
      "    lm += lm.get_role_end(role_name)\n",
      "\n",
      "    # End of either debug or HTML no disp block\n",
      "    if indent:\n",
      "        lm += nodisp_end\n",
      "    else:\n",
      "        lm += span_end\n",
      "\n",
      "    # End of top container\n",
      "    if indent:\n",
      "        lm += \"<||_html:</div></div>_||>\"\n",
      "\n",
      "    return lm\n",
      "\n",
      "\n",
      "def role(role_name, text=None, **kwargs):\n",
      "    if text is None:\n",
      "        return block(\n",
      "            opener=role_opener(role_name, **kwargs),\n",
      "            closer=role_closer(role_name, **kwargs),\n",
      "        )\n",
      "    else:\n",
      "        assert False\n",
      "        # return self.append(open_text + text + close_text)\n",
      "\n",
      "\n",
      "def system(text=None, **kwargs):\n",
      "    return role(\"system\", text, **kwargs)\n",
      "\n",
      "\n",
      "def user(text=None, **kwargs):\n",
      "    return role(\"user\", text, **kwargs)\n",
      "\n",
      "\n",
      "def assistant(text=None, **kwargs):\n",
      "    return role(\"assistant\", text, **kwargs)\n",
      "\n",
      "\n",
      "def function(text=None, **kwargs):\n",
      "    return role(\"function\", text, **kwargs)\n",
      "\n",
      "\n",
      "def instruction(text=None, **kwargs):\n",
      "    return role(\"instruction\", text, **kwargs)\n",
      "\n",
      "def indent_roles(indent=True):\n",
      "    return set_attribute(\"indent_roles\", indent)\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_format.py\n",
      "```py\n",
      "from ._block import block\n",
      "\n",
      "def monospace():\n",
      "    \"\"\"\n",
      "    Creates a context block that applies monospaced font styling to enclosed text.\n",
      "\n",
      "    This function generates a context block for use within a guidance program to apply\n",
      "    monospaced font styling, commonly used for code or preformatted text. The block\n",
      "    is defined with HTML `span` tags that include inline CSS to set the font family\n",
      "    to a monospaced font and the font size to 13 pixels.\n",
      "\n",
      "    Returns:\n",
      "        ContextBlock: An instance of the `ContextBlock` context manager configured for\n",
      "                    monospaced styling.\n",
      "\n",
      "    Example:\n",
      "        >>> with monospace():\n",
      "        ...     # Text generated within this block will be styled in monospace font.\n",
      "        ...     pass\n",
      "    \"\"\"\n",
      "    return block(opener=\"<||_html:<span style='font-family: Menlo, Monaco, monospace; font-size: 13px;'>_||>\", closer=\"<||_html:</span>_||>\")\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_char_set.py\n",
      "```py\n",
      "from .._grammar import select\n",
      "from ._char_range import char_range\n",
      "\n",
      "def char_set(def_string: str):\n",
      "    \"\"\"\n",
      "    Creates a grammar function that matches any single character from a defined set.\n",
      "\n",
      "    Args:\n",
      "        def_string (str): A string defining the set of characters to match.\n",
      "                        It can include ranges (e.g., 'a-z') and escaped characters (e.g., '\\\\n').\n",
      "\n",
      "    Returns:\n",
      "        A grammar function that matches any single character from the specified set.\n",
      "\n",
      "    Raises:\n",
      "        Exception: If the range definition in `def_string` includes multibyte characters,\n",
      "        which are not supported.\n",
      "\n",
      "    Example:\n",
      "        >>> char_set('a-zA-Z0-9')\n",
      "        # This will return a grammar function that matches any alphanumeric character.\n",
      "\n",
      "        >>> char_set('a-dx-z')\n",
      "        # This will return a grammar function that matches characters a to d and x to z.\n",
      "\n",
      "        >>> char_set('\\\\n\\\\t')\n",
      "        # This will return a grammar function that matches newline and tab characters.\n",
      "    \"\"\"\n",
      "    parts = []\n",
      "    pos = 0\n",
      "    while pos < len(def_string):\n",
      "        if pos + 2 < len(def_string) and def_string[pos + 1] == \"-\":\n",
      "            parts.append(char_range(def_string[pos], def_string[pos + 2]))\n",
      "            pos += 3\n",
      "        elif pos + 1 < len(def_string) and def_string[pos] == \"\\\\\":\n",
      "            parts.append(def_string[pos + 1])\n",
      "            pos += 2\n",
      "        else:\n",
      "            parts.append(def_string[pos])\n",
      "            pos += 1\n",
      "    return select(parts)\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_prefix_tree.py\n",
      "```py\n",
      "import guidance\n",
      "from .._grammar import select, string\n",
      "\n",
      "@guidance(stateless=True, dedent=False)\n",
      "def prefix_tree(lm, strings, partial_matches=False):\n",
      "    \"\"\"\n",
      "    Creates a grammar function that represents a prefix tree (trie) for efficient matching.\n",
      "\n",
      "    This function constructs a trie that can match any of the strings provided in\n",
      "    the `strings` list, based on their common prefixes. It's useful for efficiently matching\n",
      "    against a large list of potential options where many share common starting characters.\n",
      "\n",
      "    Args:\n",
      "        lm: The language model state to which the prefix tree grammar will be appended.\n",
      "        strings (list of str): A list of strings to include in the prefix tree.\n",
      "        partial_matches (bool, optional): If True, allows the grammar to match partial strings\n",
      "                                          that begin with any of the prefixes. Defaults to False.\n",
      "\n",
      "    Returns:\n",
      "        The language model state with the appended grammar function representing the prefix tree.\n",
      "\n",
      "    Example:\n",
      "        >>> strings = [\"apple\", \"apricot\", \"banana\", \"berry\", \"blueberry\"]\n",
      "        >>> lm += prefix_tree( strings)\n",
      "        # lm now includes a grammar function that can match any of the strings in the list.\n",
      "    \"\"\"\n",
      "\n",
      "    if len(strings) == 0:\n",
      "        return lm\n",
      "\n",
      "    # group the strings by their starting character\n",
      "    char_groups = {}\n",
      "    for s in strings:\n",
      "        if len(s) > 0:\n",
      "            if s[0] not in char_groups:\n",
      "                char_groups[s[0]] = []\n",
      "            char_groups[s[0]].append(s[1:])\n",
      "    \n",
      "    # enable any empty followup if partial matches are allowed\n",
      "    if partial_matches:\n",
      "        char_groups[\"\"] = []\n",
      "    \n",
      "    # recursively build the tree\n",
      "    suboptions = [string(k) + prefix_tree(v, partial_matches=partial_matches) for k,v in char_groups.items()]\n",
      "\n",
      "    return lm + select(suboptions, skip_checks=True) # we skip normal type checks for speed\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_block.py\n",
      "```py\n",
      "from guidance import models\n",
      "\n",
      "class ContextBlock:\n",
      "    \"\"\"\n",
      "    A context manager that defines a block of text with an opener and a closer.\n",
      "\n",
      "    This context manager is used to create structured blocks of text in a guidance\n",
      "    program, with specified opening and closing strings. It also optionally associates\n",
      "    a name with the block for reference within the program.\n",
      "\n",
      "    Attributes:\n",
      "        opener (str): The opening string for the block.\n",
      "        closer (str): The closing string for the block.\n",
      "        name (str, optional): An optional name to associate with the block.\n",
      "\n",
      "    Example:\n",
      "        >>> with ContextBlock(\"BEGIN \", \" END\", name=\"example\"):\n",
      "        ...     # Generation logic within the block\n",
      "        ...     pass\n",
      "    \"\"\"\n",
      "    def __init__(self, opener, closer, name=None):\n",
      "        self.opener = opener\n",
      "        self.closer = closer\n",
      "        self.name = name\n",
      "\n",
      "    def __enter__(self):\n",
      "        \"\"\"Registers the block as open in the global model state upon entry.\"\"\"\n",
      "        models.Model.open_blocks[self] = None\n",
      "    \n",
      "    def __exit__(self, exc_type, exc_value, traceback):\n",
      "        \"\"\"Removes the block from the global model state upon exit.\"\"\"\n",
      "        del models.Model.open_blocks[self]\n",
      "\n",
      "def block(name=None, opener=\"\", closer=\"\"):\n",
      "    \"\"\"\n",
      "    Factory function to create a `ContextBlock` with the given parameters.\n",
      "\n",
      "    Args:\n",
      "        name (str, optional): A name to associate with the block.\n",
      "        opener (str): The string to append to the model state at the start of the block.\n",
      "        closer (str): The string to append to the model state at the end of the block.\n",
      "\n",
      "    Returns:\n",
      "        ContextBlock: An instance of the `ContextBlock` context manager.\n",
      "\n",
      "    Example:\n",
      "        >>> with block(name=\"example\", opener=\"BEGIN \", closer=\" END\"):\n",
      "        ...     # Generation logic within the block\n",
      "        ...     pass\n",
      "    \"\"\"\n",
      "    return ContextBlock(opener, closer, name=name)\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_one_or_more.py\n",
      "```py\n",
      "import guidance\n",
      "from .._grammar import select\n",
      "\n",
      "@guidance(stateless=True)\n",
      "def one_or_more(model, value):\n",
      "    \"\"\"\n",
      "    Creates a grammar function that matches one or more occurrences of a given pattern.\n",
      "\n",
      "    Args:\n",
      "        model: The language model state to which the one-or-more grammar function\n",
      "            will be appended.\n",
      "        value: The pattern to match one or more times. Can be a string or another\n",
      "            grammar function.\n",
      "\n",
      "    Returns:\n",
      "        The language model state with the appended grammar function that matches\n",
      "        one or more occurrences of the specified pattern.\n",
      "\n",
      "    Example:\n",
      "        >>> pattern = 'word'\n",
      "        >>> lm += one_or_more(pattern)\n",
      "        # lm now includes a grammar function for one or more occurrences of 'word'.\n",
      "    \"\"\"\n",
      "    return model + select([value], recurse=True)\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_substring.py\n",
      "```py\n",
      "import guidance\n",
      "# from ._prefix_tree import prefix_tree\n",
      "from .._grammar import string, select\n",
      "from ._optional import optional\n",
      "\n",
      "\n",
      "# @guidance(stateless=True, dedent=False)\n",
      "# def substring(lm, s):\n",
      "    \n",
      "#     # build a prefix tree over all suffixes\n",
      "#     strings = [s[i:] for i in range(len(s))]\n",
      "#     return lm + prefix_tree(strings, partial_matches=True)\n",
      "\n",
      "@guidance(stateless=True, dedent=False)\n",
      "def substring_simple(lm, s):\n",
      "    grammars = ['' for _ in range(len(s))]\n",
      "    grammars[-1] = select(['', string(s[-1])])\n",
      "    for i in range(len(s) - 2, -1, -1):\n",
      "        si = string(s[i])\n",
      "        grammars[i] = select([si, si + grammars[i + 1]])\n",
      "    return lm + select(grammars)\n",
      "\n",
      "class State:\n",
      "    def __init__(self):\n",
      "        self.len = 0\n",
      "        self.link = -1\n",
      "        self.next = {}\n",
      "\n",
      "class SuffixAutomaton:\n",
      "    '''A suffix automoton.\n",
      "\n",
      "    For details see https://en.wikipedia.org/wiki/Suffix_automaton.\n",
      "    \n",
      "    Implementation is based on https://cp-algorithms.com/string/suffix-automaton.html\n",
      "    '''\n",
      "    def __init__(self, s):\n",
      "        \n",
      "        # init our variables\n",
      "        self.MAXLEN = 2*len(s) # we can't have more than 2n-1 states for a string of length n\n",
      "        self.states = [State() for _ in range(self.MAXLEN * 2)]\n",
      "        self.states[0].len = 0\n",
      "        self.states[0].link = -1\n",
      "        self.sz = 1 # init with a single state\n",
      "        self.last = 0\n",
      "\n",
      "        # Add characters to the suffix automaton\n",
      "        for character in s:\n",
      "            self.sa_extend(character)\n",
      "\n",
      "    @property\n",
      "    def root(self):\n",
      "        return self.states[0]\n",
      "    \n",
      "    def sa_extend(self, c):\n",
      "        cur = self.sz\n",
      "        self.sz += 1\n",
      "        self.states[cur].len = self.states[self.last].len + 1\n",
      "\n",
      "        p = self.last\n",
      "        while p != -1 and c not in self.states[p].next:\n",
      "            self.states[p].next[c] = cur\n",
      "            p = self.states[p].link\n",
      "\n",
      "        if p == -1:\n",
      "            self.states[cur].link = 0\n",
      "        else:\n",
      "            q = self.states[p].next[c]\n",
      "            if self.states[p].len + 1 == self.states[q].len:\n",
      "                self.states[cur].link = q\n",
      "            else:\n",
      "                clone = self.sz\n",
      "                self.sz += 1\n",
      "                self.states[clone].len = self.states[p].len + 1\n",
      "                self.states[clone].next = self.states[q].next.copy()\n",
      "                self.states[clone].link = self.states[q].link\n",
      "\n",
      "                while p != -1 and self.states[p].next.get(c) == q:\n",
      "                    self.states[p].next[c] = clone\n",
      "                    p = self.states[p].link\n",
      "                \n",
      "                self.states[q].link = self.states[cur].link = clone\n",
      "        \n",
      "        self.last = cur\n",
      "\n",
      "@guidance(stateless=True, dedent=False)\n",
      "def substring(lm, s):\n",
      "    \"\"\"\n",
      "    Creates a grammar function that matches substrings of the given string.\n",
      "\n",
      "    Args:\n",
      "        lm: The language model state to which the substring grammar will be appended.\n",
      "        s (str): The string for which substrings will be matched and generated.\n",
      "\n",
      "    Returns:\n",
      "        The language model state with the appended grammar function that matches substrings of `s`.\n",
      "\n",
      "    Example:\n",
      "        >>> lm += substring(\"example\")\n",
      "        # lm now includes a grammar function that can match any substring of \"example\".\n",
      "    \"\"\"\n",
      "    suffix_automaton = SuffixAutomaton(s)\n",
      "    node_cache = {}\n",
      "    state_stack = [0]  # Start with the initial state index (0) on the stack\n",
      "\n",
      "    # Loop as long as there are states on the stack\n",
      "    while state_stack:\n",
      "        state_ind = state_stack[-1]  # Check the state on the top of the stack\n",
      "\n",
      "        state = suffix_automaton.states[state_ind]\n",
      "\n",
      "        # If we have already computed the result for this state, skip it\n",
      "        if state_ind in node_cache:\n",
      "            state_stack.pop()\n",
      "            continue\n",
      "\n",
      "        # If the state is a leaf node, meaning no outgoing edges (is an end of some suffix)\n",
      "        if not state.next:\n",
      "            node_cache[state_ind] = string(\"\")  # Leaf nodes represent empty string suffixes\n",
      "            state_stack.pop()\n",
      "            continue\n",
      "\n",
      "        # If there's an unprocessed child, add it to the stack\n",
      "        unprocessed_children = [next_state for next_state in state.next.values() if next_state not in node_cache]\n",
      "        if unprocessed_children:\n",
      "            state_stack.extend(unprocessed_children)\n",
      "        else:\n",
      "            # Once all children are processed, create the node for this state\n",
      "            options = [string(c) + node_cache[state.next[c]] for c in state.next]\n",
      "            node_cache[state_ind] = optional(select(options, skip_checks=True)) if len(options) > 1 else optional(options[0])\n",
      "            state_stack.pop()\n",
      "\n",
      "    return lm + node_cache[0]\n",
      "\n",
      "\n",
      "@guidance(stateless=True, dedent=False)\n",
      "def substring_no_empty(lm, s):\n",
      "    \"\"\"\n",
      "    Creates a grammar function that matches non-empty substrings of the given string.\n",
      "\n",
      "    Args:\n",
      "        lm: The language model state to which the substring grammar will be appended.\n",
      "        s (str): The string for which substrings will be matched and generated.\n",
      "\n",
      "    Returns:\n",
      "        The language model state with the appended grammar function that matches non-empty substrings of `s`.\n",
      "\n",
      "    Example:\n",
      "        >>> lm += substring(\"example\")\n",
      "        # lm now includes a grammar function that can match any non-empty substring of \"example\".\n",
      "    \"\"\"\n",
      "    suffix_automaton = SuffixAutomaton(s)\n",
      "    node_cache = {}\n",
      "    state_stack = [0]  # Start with the initial state index (0) on the stack\n",
      "\n",
      "    # Loop as long as there are states on the stack\n",
      "    while state_stack:\n",
      "        state_ind = state_stack[-1]  # Check the state on the top of the stack\n",
      "\n",
      "        state = suffix_automaton.states[state_ind]\n",
      "\n",
      "        # If we have already computed the result for this state, skip it\n",
      "        if state_ind in node_cache:\n",
      "            state_stack.pop()\n",
      "            continue\n",
      "\n",
      "        # If the state is a leaf node, meaning no outgoing edges (is an end of some suffix)\n",
      "        if not state.next:\n",
      "            node_cache[state_ind] = None  # Leaf nodes represent empty string suffixes, which we want to exclude\n",
      "            state_stack.pop()\n",
      "            continue\n",
      "\n",
      "        # If there's an unprocessed child, add it to the stack\n",
      "        unprocessed_children = [next_state for next_state in state.next.values() if next_state not in node_cache]\n",
      "        if unprocessed_children:\n",
      "            state_stack.extend(unprocessed_children)\n",
      "        else:\n",
      "            # Once all children are processed, create the node for this state\n",
      "            options = [string(c) + node_cache[state.next[c]] for c in state.next if node_cache[state.next[c]] is not None]\n",
      "            if options:\n",
      "                node_cache[state_ind] = select(options, skip_checks=True) if len(options) > 1 else options[0]\n",
      "            else:\n",
      "                node_cache[state_ind] = None\n",
      "            state_stack.pop()\n",
      "\n",
      "    return lm + node_cache[0] if node_cache[0] is not None else lm\n",
      "\n",
      "# @guidance(stateless=True, dedent=False)\n",
      "# def substring(s):\n",
      "#     a = SuffixAutomaton(s)\n",
      "#     return _rec_substring(a, 0, {})\n",
      "\n",
      "# def _rec_substring(suffix_automaton, state_ind, node_cache):\n",
      "#     if state_ind in node_cache:\n",
      "#         return node_cache[state_ind]\n",
      "\n",
      "#     state = suffix_automaton.states[state_ind]\n",
      "\n",
      "#     if len(state.next) == 0:\n",
      "#         return string(\"\")\n",
      "\n",
      "#     options = []\n",
      "#     for c in state.next:\n",
      "#         options.append(string(c) + _rec_substring(suffix_automaton, state.next[c], node_cache))\n",
      "    \n",
      "#     if len(options) == 1:\n",
      "#         node = optional(options[0])\n",
      "#     else:\n",
      "#         node = optional(select(options, skip_checks=True))\n",
      "    \n",
      "#     node_cache[state_ind] = node\n",
      "#     return node\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_silent.py\n",
      "```py\n",
      "from ._block import block\n",
      "\n",
      "def silent():\n",
      "    open_text = f\"\"\"<||_html:<div style='display: inline-block; cursor: pointer; opacity: 0.5; width: 2px; margin-left: -2px;' onClick='this.nextSibling.style.display = \"inline\"; this.style.display = \"none\"'>&caron;</div><div style='display: none;'>_||>\"\"\"\n",
      "    close_text = \"<||_html:</div>_||>\"\n",
      "    return block(opener=open_text, closer=close_text)\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_set_var.py\n",
      "```py\n",
      "import guidance\n",
      "from ._block import block\n",
      "\n",
      "@guidance\n",
      "def set_opener(lm, name, value):\n",
      "    if name in lm:\n",
      "        lm = lm.set(\"__save\" + name, lm[name])\n",
      "    return lm.set(name, value)\n",
      "\n",
      "@guidance\n",
      "def set_closer(lm, name):\n",
      "    if \"__save\" + name in lm:\n",
      "        return lm.set(name, lm[\"__save\" + name]).remove(\"__save\" + name)\n",
      "    else:\n",
      "        return lm.remove(name)\n",
      "\n",
      "def set_var(name, value=True):\n",
      "    return block(\n",
      "        opener=set_opener(name, value),\n",
      "        closer=set_closer(name),\n",
      "    )\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_set_attribute.py\n",
      "```py\n",
      "import guidance\n",
      "from ._block import block\n",
      "\n",
      "@guidance\n",
      "def set_attr_opener(lm, name, value):\n",
      "    if hasattr(lm, name):\n",
      "        lm = lm.setattr(\"__save\" + name, getattr(lm, name))\n",
      "    return lm.setattr(name, value)\n",
      "\n",
      "@guidance\n",
      "def set_attr_closer(lm, name):\n",
      "    if hasattr(lm, \"__save\" + name):\n",
      "        return lm.setattr(name, lm[\"__save\" + name]).delattr(\"__save\" + name)\n",
      "    else:\n",
      "        return lm.delattr(name)\n",
      "\n",
      "def set_attribute(name, value=True):\n",
      "    return block(\n",
      "        opener=set_attr_opener(name, value),\n",
      "        closer=set_attr_closer(name),\n",
      "    )\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/library/_gen.py\n",
      "```py\n",
      "import regex as regex_module\n",
      "import logging\n",
      "import guidance\n",
      "from ._silent import silent\n",
      "from .._grammar import select\n",
      "from ._zero_or_more import zero_or_more\n",
      "from .._grammar import commit_point\n",
      "from ._any_char import any_char\n",
      "from .._grammar import capture\n",
      "from ._regex import regex as regex_grammar\n",
      "from .._grammar import token_limit, eos_token, active_role_end, with_temperature\n",
      "from ._tool import Tool\n",
      "from ._block import block\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "# TODO: make this stateless!\n",
      "@guidance(stateless=lambda *args, **kwargs: kwargs.get(\"tools\", None) is None) # TODO: uncomment this once we get temperature stateless\n",
      "def gen(lm, name=None, *, max_tokens=1000, list_append=False, regex=None,\n",
      "        tools=None, hide_tool_call=False, stop=None, stop_regex=None, suffix=\"\", n=1, temperature=0.0, top_p=1.0,\n",
      "        save_stop_text=False):\n",
      "    \"\"\" Generate a set of tokens until a given stop criteria has been met.\n",
      "\n",
      "    This function is a useful utility that can allow you to specify most grammars used by typical\n",
      "    LM generation programs. It also has the added ability to interleave generation with tool calls.\n",
      "\n",
      "    Parameters\n",
      "        ----------\n",
      "        name : str or None\n",
      "            If this is not None then the the results of the generation will be saved as a variable on\n",
      "            the Model object (so you can access the result as `lm[\"var_name\"]`).\n",
      "\n",
      "        max_tokens : int\n",
      "            The maximum number of generation tokens we should use. Note that this limit is not exact when\n",
      "            regular expression pattern constraints are present, but guidance does attempt to end the generation\n",
      "            as soon as possible while keeping the regex constraints satisfied.\n",
      "\n",
      "        list_append : bool\n",
      "            If this is True then the results saved to `lm[name]` will not be written directly but rather appended\n",
      "            to a list (if no list with the current name is present one will be created). This is useful for\n",
      "            building lists inside python loops.\n",
      "        \n",
      "        regex : str or None\n",
      "            This is a regular expression that will be used to constrain the generation. The model is only allowed\n",
      "            to generate tokens that match this regular expression. Note that for variable length expressions the\n",
      "            model is free to continue the expression after a complete match, but generation will terminate as soon\n",
      "            as the model generates anything that does not match the pattern (this ending behavior may change a bit we\n",
      "            update guidance to maintain the grammar parsing state between calls).\n",
      "\n",
      "        stop : str or list or None\n",
      "            The stop string (or list of strings) we should use for terminating this generation segment.\n",
      "\n",
      "        stop_regex : str or list or None\n",
      "            The stop regular expression (or list of regular expressions) we should use for terminating this generation segment.\n",
      "\n",
      "        save_stop_text : bool or str\n",
      "            If True then this saves the captured stop text or regex into a variable of the name `str(name) + \"_stop_text\"`. If\n",
      "            a string is given then the captured stop text is saved under that name.\n",
      "\n",
      "        temperature : float\n",
      "            The temperature to use during this generation call. Note that when parsing ambiguous grammars that include\n",
      "            multiple conflicting temperatures (for example from multiple possible `gen` calls inside a `select`) the highest\n",
      "            temperature of all options is used by the model (since we only want to run the model once, not once for every\n",
      "            possible parse path).\n",
      "\n",
      "        top_p : float\n",
      "            TODO! Will control the models top_p generation parameter, but has been yet been implemented beyond top_p=1.0.\n",
      "\n",
      "        n : int\n",
      "            TODO! Will control the number of parallel generation calls made during gen.\n",
      "\n",
      "        tools : Tool or list or None\n",
      "            A list of guidance.Tool or python functions (which will be converted to guidance.Tool)\n",
      "\n",
      "        hide_tool_call : bool\n",
      "            Controls if we should hide the text generated by the model to trigger a tool call. You may want to hide the tool\n",
      "            call from the model's context if you plan to change it's format after the call is made.\n",
      "    \"\"\"\n",
      "    # TODO: expand the tools doc string\n",
      "    assert n == 1, \"We still need to add support for n>1! Consider putting your gen call in a loop for now.\"\n",
      "    assert top_p == 1, \"We still need to add support for top_p != 1!\"\n",
      "    \n",
      "    logger.debug(f'start gen(name=\"{name}\")')\n",
      "\n",
      "    # set stream if we are interactive\n",
      "    # if stream_tokens is None and not lm.is_silent() and n == 1:\n",
      "    #     stream_tokens = True\n",
      "\n",
      "    # use the suffix as the stop string if not otherwise specified\n",
      "    # TODO: still need to make suffix work with grammars\n",
      "    # eos_token = lm.eos_token.decode('utf8')\n",
      "    if stop is None and stop_regex is None and suffix != \"\":\n",
      "        stop = suffix\n",
      "    # if stop is None and stop_regex is None and getattr(lm, \"suffix\", False):\n",
      "    #     if lm.suffix.startswith(\"\\n\"):\n",
      "    #         stop = \"\\n\"\n",
      "    #     elif lm.suffix.startswith('\"') and str(lm).endswith('\"'):\n",
      "    #         stop = '\"'\n",
      "    #     elif lm.suffix.startswith(\"'\") and str(lm).endswith(\"'\"):\n",
      "    #         stop = \"'\"\n",
      "\n",
      "    # fall back to stopping at the EOS token\n",
      "    if stop is not False:\n",
      "        if stop is None:\n",
      "            stop = []\n",
      "        if isinstance(stop, str):\n",
      "            stop = [stop]\n",
      "        if regex is None:\n",
      "            stop.append(select([eos_token(), active_role_end()]))\n",
      "\n",
      "        if stop_regex is None:\n",
      "            stop_regex = []\n",
      "        if isinstance(stop_regex, str):\n",
      "            stop_regex = [stop_regex]\n",
      "        stop_regex = [regex_grammar(x) for x in stop_regex]\n",
      "\n",
      "    # This needs to be here for streaming\n",
      "    # if name is not None and not list_append:\n",
      "    #     lm[name] = \"\"\n",
      "    \n",
      "    # define the generation pattern\n",
      "    if regex is not None:\n",
      "        pattern = regex_grammar(regex)\n",
      "    else:\n",
      "        pattern = zero_or_more(any_char())\n",
      "\n",
      "    tagged_name = \"__LIST_APPEND:\" + name if list_append and name is not None else name\n",
      "\n",
      "    # define any capture group for non-tool calls\n",
      "    if name is not None and tools is None:\n",
      "        pattern = capture(pattern, name=tagged_name)\n",
      "    \n",
      "    # limit the number of tokens\n",
      "    pattern = token_limit(pattern, max_tokens)\n",
      "    \n",
      "    # define the stop pattern\n",
      "    if stop is False or len(stop + stop_regex) == 0:\n",
      "        stop_pattern = ''\n",
      "    else:\n",
      "        stop_pattern = select(stop + stop_regex)\n",
      "        if save_stop_text is True:\n",
      "            save_stop_text = str(name) + \"_stop_text\"\n",
      "        if isinstance(save_stop_text, str):\n",
      "            stop_pattern = capture(stop_pattern, name=save_stop_text)\n",
      "        stop_pattern = commit_point(stop_pattern, hidden=True)\n",
      "\n",
      "    # single generation\n",
      "    start_pos = len(str(lm))\n",
      "    if tools is not None:\n",
      "        with block(tagged_name):\n",
      "            tools = [Tool(callable=x) if not isinstance(x, Tool) else x for x in tools]\n",
      "            init_token_count = lm.token_count\n",
      "            gen_grammar = pattern + select([stop_pattern] + [capture(commit_point(x.call_grammar, hidden=hide_tool_call), name=f'tool{i}') for i, x in enumerate(tools)])\n",
      "            while lm.token_count <= max_tokens + init_token_count:\n",
      "                lm = lm._run_stateless(gen_grammar, temperature=temperature) # TODO: we should not be using this internal method\n",
      "                tool_called = False\n",
      "                for i in range(len(tools)):\n",
      "                    tool_i = f'tool{i}'\n",
      "                    if tool_i in lm:\n",
      "                        tool_called = True\n",
      "                        lm += tools[i].tool_call()\n",
      "                        lm = lm.remove(tool_i)\n",
      "                if not tool_called:\n",
      "                    lm += suffix\n",
      "                    break\n",
      "    elif n == 1:\n",
      "        lm += with_temperature(pattern + stop_pattern + suffix, temperature)\n",
      "\n",
      "    logger.debug(f'finish gen')\n",
      "    return lm\n",
      "\n",
      "\n",
      "def click_loop_start(id, total_count, echo, color):\n",
      "    click_script = '''\n",
      "function cycle_IDVAL(button_el) {\n",
      "var i = 0;\n",
      "while (i < 50) {\n",
      "var el = document.getElementById(\"IDVAL_\" + i);\n",
      "if (el.style.display == \"inline\") {\n",
      "    el.style.display = \"none\";\n",
      "    var next_el = document.getElementById(\"IDVAL_\" + (i+1));\n",
      "    if (!next_el) {\n",
      "        next_el = document.getElementById(\"IDVAL_0\");\n",
      "    }\n",
      "    if (next_el) {\n",
      "        next_el.style.display = \"inline\";\n",
      "    }\n",
      "    break;\n",
      "}\n",
      "i += 1;\n",
      "}\n",
      "button_el.innerHTML = (((i+1) % TOTALCOUNT) + 1)  + \"/\" + TOTALCOUNT;\n",
      "}\n",
      "cycle_IDVAL(this);'''.replace(\"IDVAL\", id).replace(\"TOTALCOUNT\", str(total_count)).replace(\"\\n\", \"\")\n",
      "    out = f'''<div style='background: rgba(255, 255, 255, 0.0); border-radius: 4px 0px 0px 4px; border: 1px solid {color}; border-right: 0px; padding-left: 3px; padding-right: 3px; user-select: none; color: {color}; display: inline; font-weight: normal; cursor: pointer' onClick='{click_script}'>1/{total_count}</div>'''\n",
      "    out += f\"<div style='display: inline;' id='{id}_0'>\"\n",
      "    return \"<||_html:\" + out + \"_||>\"\n",
      "\n",
      "def click_loop_mid(id, index, echo):\n",
      "    alpha = 1.0 if not echo else 0.5\n",
      "    out = f\"</div><div style='display: none; opacity: {alpha}' id='{id}_{index}'>\"\n",
      "    return \"<||_html:\" + out + \"_||>\"\n",
      "\n",
      "@guidance\n",
      "def gen_line(lm, *args, **kwargs):\n",
      "    return lm.gen(*args, suffix='\\n', **kwargs)\n",
      "\n",
      "@guidance\n",
      "def gen_quote(lm, name=None, quote='\"', *args, **kwargs):\n",
      "    return lm(quote).gen(*args,name=name, suffix=quote, **kwargs)\n",
      "\n",
      "@guidance\n",
      "def will_gen(lm, stop=None, stop_regex=None, ignore_spaces=False, max_tokens=30):\n",
      "    # this is obviously not the right implementation, just here so we can explore\n",
      "    if stop and not isinstance(stop, list):\n",
      "        stop = [stop]\n",
      "    if stop_regex and not isinstance(stop_regex, list):\n",
      "        stop_regex = [stop_regex]\n",
      "    assert (stop is not None) or (stop_regex is not None)\n",
      "    if not stop:\n",
      "        stop = []\n",
      "    if not stop_regex:\n",
      "        stop_regex = []\n",
      "    regexes = [regex_module.escape(x) for x in stop + stop_regex]\n",
      "    optional_space = '\\\\s*' if ignore_spaces else ''\n",
      "    pattern = regex_module.compile(f'{optional_space}({\"|\".join(regexes)})')\n",
      "    lm2 = lm\n",
      "    with silent():\n",
      "        for _ in range(max_tokens):\n",
      "            lm2 += gen('temp_variable', list_append=True, max_tokens=1)\n",
      "            if not lm2['temp_variable'] or not pattern.match(''.join(lm2['temp_variable']), partial=True):\n",
      "                return False\n",
      "            if pattern.match(''.join(lm2['temp_variable']), partial=False):\n",
      "                return True\n",
      "    return False\n",
      "\n",
      "@guidance\n",
      "def call_tool(lm, tool):\n",
      "    return lm + tool.call_grammar + tool.tool_call()\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/_lite_llm.py\n",
      "```py\n",
      "import tiktoken\n",
      "\n",
      "from ._model import Chat, Instruct\n",
      "from ._grammarless import GrammarlessTokenizer, GrammarlessEngine, Grammarless\n",
      "\n",
      "class LiteLLMEngine(GrammarlessEngine):\n",
      "    def __init__(self, model, tokenizer, timeout, compute_log_probs, max_streaming_tokens, **kwargs):\n",
      "        try:\n",
      "            import litellm\n",
      "        except ImportError:\n",
      "            raise Exception(\"Please install the litellm package version >= 1.7 using `pip install litellm -U` in order to use guidance.models.LiteLLM!\")\n",
      "        \n",
      "        self.litellm = litellm\n",
      "\n",
      "        # self.client = openai_package.OpenAI(api_key=api_key, organization=organization, base_url=base_url)\n",
      "        self.model_name = model\n",
      "\n",
      "        # we pretend it tokenizes like gpt2 if tiktoken does not know about it... TODO: make this better\n",
      "        if tokenizer is None:\n",
      "            try:\n",
      "                tokenizer = tiktoken.encoding_for_model(model)\n",
      "            except:\n",
      "                tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
      "\n",
      "        super().__init__(\n",
      "            tokenizer,\n",
      "            max_streaming_tokens=max_streaming_tokens,\n",
      "            timeout=timeout,\n",
      "            compute_log_probs=compute_log_probs\n",
      "        )\n",
      "\n",
      "class LiteLLM(Grammarless):\n",
      "    def __init__(self, model, tokenizer=None, echo=True, timeout=0.5, max_streaming_tokens=1000, compute_log_probs=False):\n",
      "        '''Build a new LiteLLM model object that represents a model in a given state.'''\n",
      "\n",
      "        # if we are called directly (as opposed to through super()) then we convert ourselves to a more specific subclass if possible\n",
      "        if self.__class__ is LiteLLM:\n",
      "            raise Exception(\"The LightLLM class is not meant to be used directly! Please use LiteLLMChat, LiteLLMInstruct, or LiteLLMCompletion depending on the model you are using.\")\n",
      "\n",
      "        # this allows us to use a single constructor for all our subclasses\n",
      "        engine_map = {\n",
      "            LiteLLMCompletion: LiteLLMCompletionEngine,\n",
      "            LiteLLMInstruct: LiteLLMInstructEngine,\n",
      "            LiteLLMChat: LiteLLMChatEngine\n",
      "        }\n",
      "\n",
      "        for k in engine_map:\n",
      "            if issubclass(self.__class__, k):\n",
      "                super().__init__(\n",
      "                    engine_map[k](model, tokenizer, timeout, compute_log_probs, max_streaming_tokens),\n",
      "                    echo=echo\n",
      "                )\n",
      "\n",
      "class LiteLLMCompletion(LiteLLM):\n",
      "    pass\n",
      "\n",
      "class LiteLLMCompletionEngine(LiteLLMEngine):\n",
      "\n",
      "    def _generator(self, prompt, temperature):\n",
      "        \n",
      "        # update our shared data state\n",
      "        self._reset_shared_data(prompt, temperature)\n",
      "\n",
      "        try:\n",
      "            generator = self.litellm.completion(\n",
      "                model=self.model_name,\n",
      "                messages=[{\"content\": prompt.decode(\"utf8\"), \"role\": \"system\"}], # note that role=system is just ignored by litellm but used by them to match chat syntax\n",
      "                max_tokens=self.max_streaming_tokens,\n",
      "                n=1,\n",
      "                top_p=1,\n",
      "                temperature=temperature,\n",
      "                stream=True\n",
      "            )\n",
      "        except Exception as e: # TODO: add retry logic\n",
      "            raise e\n",
      "        \n",
      "        for part in generator:\n",
      "            chunk = part.choices[0].delta.content or \"\"\n",
      "            yield chunk.encode(\"utf8\")\n",
      "\n",
      "class LiteLLMInstruct(LiteLLM, Instruct):\n",
      "    def get_role_start(self, name):\n",
      "        return \"\"\n",
      "    \n",
      "    def get_role_end(self, name):\n",
      "        if name == \"instruction\":\n",
      "            return \"<|endofprompt|>\"\n",
      "        else:\n",
      "            raise Exception(f\"The LiteLLMInstruct model does not know about the {name} role type!\")\n",
      "\n",
      "class LiteLLMInstructEngine(LiteLLMEngine):\n",
      "\n",
      "    def _generator(self, prompt, temperature):\n",
      "        # start the new stream\n",
      "        prompt_end = prompt.find(b'<|endofprompt|>')\n",
      "        if prompt_end >= 0:\n",
      "            stripped_prompt = prompt[:prompt_end]\n",
      "        else:\n",
      "            raise Exception(\"This model cannot handle prompts that don't match the instruct format!\")\n",
      "        \n",
      "        # make sure you don't try and instruct the same model twice\n",
      "        if b'<|endofprompt|>' in prompt[prompt_end + len(b'<|endofprompt|>'):]:\n",
      "            raise Exception(\"This model has been given two separate instruct blocks, but this is not allowed!\")\n",
      "        \n",
      "        # update our shared data state\n",
      "        self._reset_shared_data(stripped_prompt + b'<|endofprompt|>', temperature)\n",
      "\n",
      "        try:\n",
      "            generator = self.litellm.completion(\n",
      "                model=self.model_name,\n",
      "                messages=[{\"content\": self._data.decode(\"utf8\"), \"role\": \"system\"}], # note that role=system is just ignored by litellm but used by them to match chat syntax\n",
      "                prompt=self._data.decode(\"utf8\"), \n",
      "                max_tokens=self.max_streaming_tokens, \n",
      "                n=1, \n",
      "                top_p=1, \n",
      "                temperature=temperature, \n",
      "                stream=True\n",
      "            )\n",
      "        except Exception as e: # TODO: add retry logic\n",
      "            raise e\n",
      "        \n",
      "        for part in generator:\n",
      "            chunk = part.choices[0].delta.content or \"\"\n",
      "            yield chunk.encode(\"utf8\")\n",
      "\n",
      "class LiteLLMChat(LiteLLM, Chat):\n",
      "    pass\n",
      "\n",
      "class LiteLLMChatEngine(LiteLLMEngine):\n",
      "    def _generator(self, prompt, temperature):\n",
      "        \n",
      "        # find the system text\n",
      "        pos = 0\n",
      "        role_end = b'<|im_end|>'\n",
      "\n",
      "        # find the user/assistant pairs\n",
      "        messages = []\n",
      "        found = True\n",
      "        while found:\n",
      "\n",
      "            # find the user text\n",
      "            found = False\n",
      "            for role_name,start_bytes in ((\"system\", b'<|im_start|>system\\n'), (\"user\", b'<|im_start|>user\\n'), (\"assistant\", b'<|im_start|>assistant\\n')):\n",
      "                if prompt[pos:].startswith(start_bytes):\n",
      "                    pos += len(start_bytes)\n",
      "                    end_pos = prompt[pos:].find(role_end)\n",
      "                    if end_pos < 0:\n",
      "                        assert role_name == \"assistant\", \"Bad chat format! Last role before gen needs to be assistant!\"\n",
      "                        break\n",
      "                    btext = prompt[pos:pos+end_pos]\n",
      "                    pos += end_pos + len(role_end)\n",
      "                    messages.append({\"role\": role_name, \"content\": btext.decode(\"utf8\")})\n",
      "                    found = True\n",
      "                    break\n",
      "        \n",
      "        # update our shared data state\n",
      "        self._reset_shared_data(prompt[:pos], temperature)\n",
      "\n",
      "        try:\n",
      "            generator = self.litellm.completion(\n",
      "                model=self.model_name,\n",
      "                messages=messages,\n",
      "                max_tokens=self.max_streaming_tokens, \n",
      "                n=1, \n",
      "                top_p=1, \n",
      "                temperature=temperature, \n",
      "                stream=True\n",
      "            )\n",
      "        except Exception as e: # TODO: add retry logic\n",
      "            raise e\n",
      "\n",
      "        for part in generator:\n",
      "            chunk = part.choices[0].delta.content or \"\"\n",
      "            yield chunk.encode(\"utf8\")\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/_mock.py\n",
      "```py\n",
      "import numpy as np\n",
      "\n",
      "from ._model import Tokenizer, Engine, Model, Chat\n",
      "from ._remote import RemoteEngine\n",
      "\n",
      "class MockEngine(Engine):\n",
      "    def __init__(self, tokenizer, byte_patterns, compute_log_probs, force):\n",
      "        super().__init__(tokenizer, compute_log_probs=compute_log_probs)\n",
      "\n",
      "        self._valid_mask = np.zeros(len(tokenizer.tokens))\n",
      "        for i,t in enumerate(tokenizer.tokens):\n",
      "            try:\n",
      "                t.decode(\"utf8\")\n",
      "                self._valid_mask[i] = 1.0\n",
      "            except:\n",
      "                pass\n",
      "        self.force = force\n",
      "\n",
      "        # allow a single byte pattern to be passed\n",
      "        if isinstance(byte_patterns, (bytes, str)):\n",
      "            byte_patterns = [byte_patterns]\n",
      "\n",
      "        # allow for strings to be passed\n",
      "        for i,pattern in enumerate(byte_patterns):\n",
      "            if isinstance(pattern, str):\n",
      "                byte_patterns[i] = pattern.encode(\"utf8\")\n",
      "        \n",
      "        self.byte_patterns = byte_patterns\n",
      "\n",
      "        # seed the random number generator\n",
      "        self._rand_generator = np.random.default_rng(seed=42)\n",
      "\n",
      "    def get_logits(self, token_ids, forced_bytes, current_temp):\n",
      "        '''Pretends to compute the logits for the given token state.\n",
      "        '''\n",
      "\n",
      "        # build the byte strings\n",
      "        byte_string = b\"\".join(self.tokenizer.tokens[i] for i in token_ids)\n",
      "\n",
      "        # if we are forcing the bytes patterns then don't allow other tokens\n",
      "        if self.force:\n",
      "            logits = np.ones(len(self.tokenizer.tokens)) * -np.inf\n",
      "        \n",
      "        # otherwise we randomly generate valid unicode bytes\n",
      "        else:\n",
      "            logits = self._rand_generator.standard_normal(len(self.tokenizer.tokens)) * self._valid_mask\n",
      "\n",
      "        # if we have a pattern that matches then force the next token\n",
      "        bias = 100.0\n",
      "        if self.byte_patterns is not None:\n",
      "            byte_string\n",
      "            for p in self.byte_patterns:\n",
      "                if p.startswith(byte_string) and len(p) > len(byte_string):\n",
      "                    for i in self._get_next_tokens(p[len(byte_string):]):\n",
      "                        logits[i] += bias\n",
      "                    bias /= 2 # if we have multiple matches then they apply with decreasing bias\n",
      "        \n",
      "        return logits\n",
      "    \n",
      "    def _get_next_tokens(self, byte_string):\n",
      "        for i,t in enumerate(self.tokenizer.tokens):\n",
      "            if byte_string.startswith(t):\n",
      "                yield i\n",
      "\n",
      "class Mock(Model):\n",
      "    def __init__(self, byte_patterns=[], echo=True, compute_log_probs=False, force=False, **kwargs):\n",
      "        '''Build a new Mock model object that represents a model in a given state.'''\n",
      "\n",
      "        if isinstance(byte_patterns, str) and byte_patterns.startswith(\"http\"):\n",
      "            engine = RemoteEngine(byte_patterns, **kwargs)\n",
      "        else:\n",
      "            tokenizer = Tokenizer(\n",
      "                # our tokens are all bytes and all lowercase letter pairs\n",
      "                [b\"<s>\"] + [bytes([i,j]) for i in range(ord('a'), ord('z')) for j in range(ord('a'), ord('z'))] + [bytes([i]) for i in range(256)],\n",
      "                0,\n",
      "                0\n",
      "            )\n",
      "            engine = MockEngine(tokenizer, byte_patterns, compute_log_probs, force)\n",
      "        \n",
      "        \n",
      "        super().__init__(\n",
      "            engine,\n",
      "            echo=echo\n",
      "        )\n",
      "        \n",
      "\n",
      "class MockChat(Mock, Chat):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/_cohere.py\n",
      "```py\n",
      "from ._lite_llm import LiteLLMEngine, LiteLLM, LiteLLMCompletion, LiteLLMInstruct\n",
      "\n",
      "class Cohere(LiteLLM):\n",
      "    def __init__(self, model, tokenizer=None, echo=True, timeout=0.5, compute_log_probs=False, max_streaming_tokens=1000):\n",
      "        '''Build a new Anthropic model object that represents a model in a given state.'''\n",
      "        try:\n",
      "            import tokenizers\n",
      "        except ImportError:\n",
      "            raise Exception(\"Please install the HuggingFace tokenizers package using `pip install tokenizers -U` in order to use guidance.models.Cohere!\")\n",
      "\n",
      "        # get the tokenizer\n",
      "        if tokenizer is None:\n",
      "            try:\n",
      "                tokenizer = tokenizers.Tokenizer.from_pretrained(\"Cohere/\"+model)\n",
      "            except:\n",
      "                tokenizer = tokenizers.Tokenizer.from_pretrained(\"Cohere/command-nightly\")\n",
      "\n",
      "        super().__init__(\n",
      "            model, tokenizer=tokenizer, echo=echo, timeout=timeout,\n",
      "            max_streaming_tokens=max_streaming_tokens, compute_log_probs=compute_log_probs\n",
      "        )\n",
      "\n",
      "class CohereCompletion(Cohere, LiteLLMCompletion):\n",
      "    pass\n",
      "\n",
      "class CohereInstruct(Cohere, LiteLLMInstruct):\n",
      "    pass\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/_azure_openai.py\n",
      "```py\n",
      "from urllib.parse import parse_qs, urlparse\n",
      "\n",
      "from ._grammarless import Grammarless\n",
      "from ._model import Chat, Instruct\n",
      "from ._openai import (\n",
      "    OpenAIChatEngine,\n",
      "    OpenAICompletionEngine,\n",
      "    OpenAIInstructEngine,\n",
      ")\n",
      "\n",
      "try:\n",
      "    # TODO: can we eliminate the torch requirement for llama.cpp by using numpy in the caller instead?\n",
      "    import openai as openai_package\n",
      "\n",
      "    is_openai = True\n",
      "except ImportError:\n",
      "    is_openai = False\n",
      "\n",
      "\n",
      "class AzureOpenAI(Grammarless):\n",
      "    def __init__(\n",
      "        self,\n",
      "        model: str,\n",
      "        azure_endpoint: str,\n",
      "        azure_deployment: str = None,\n",
      "        azure_ad_token_provider=None,\n",
      "        api_key: str = None,\n",
      "        tokenizer=None,\n",
      "        echo=True,\n",
      "        version: str = None,\n",
      "        max_streaming_tokens=1000,\n",
      "        timeout=0.5,\n",
      "        compute_log_probs=False,\n",
      "        **kwargs,\n",
      "    ):\n",
      "        \"\"\"Build a new AzureOpenAI model object that represents a model in a given state.\"\"\"\n",
      "        if not is_openai or not hasattr(openai_package, \"OpenAI\"):\n",
      "            raise ImportError(\n",
      "                \"Please install the openai package version >= 1 using `pip install openai -U` \"\n",
      "                \"in order to use guidance.models.OpenAI!\"\n",
      "            )\n",
      "\n",
      "        if api_key is None and azure_ad_token_provider is None:\n",
      "            raise ValueError(\"Please provide either api_key or azure_ad_token_provider\")\n",
      "\n",
      "        parsed_url = urlparse(azure_endpoint)\n",
      "\n",
      "        # if we are called directly (as opposed to through super()) then we convert ourselves to\n",
      "        # a more specific subclass if possible\n",
      "        if self.__class__ is AzureOpenAI:\n",
      "            # chat\n",
      "            if parsed_url.path.endswith(\"/chat/completions\"):\n",
      "                found_subclass = AzureOpenAIChat\n",
      "            # regular completion\n",
      "            else:\n",
      "                found_subclass = AzureOpenAICompletion\n",
      "\n",
      "            # convert to any found subclass\n",
      "            self.__class__ = found_subclass\n",
      "            found_subclass.__init__(\n",
      "                self,\n",
      "                model=model,\n",
      "                azure_endpoint=azure_endpoint,\n",
      "                api_key=api_key,\n",
      "                azure_ad_token_provider=azure_ad_token_provider,\n",
      "                azure_deployment=azure_deployment,\n",
      "                tokenizer=tokenizer,\n",
      "                echo=echo,\n",
      "                version=version,\n",
      "                **kwargs,\n",
      "            )\n",
      "            return\n",
      "\n",
      "        parsed_query = parse_qs(parsed_url.query)\n",
      "        api_version = (\n",
      "            version\n",
      "            if \"api-version\" not in parsed_query\n",
      "            else parsed_query[\"api-version\"]\n",
      "        )\n",
      "        engine_map = {\n",
      "            AzureOpenAICompletion: OpenAICompletionEngine,\n",
      "            AzureOpenAIChat: OpenAIChatEngine,\n",
      "            AzureOpenAIInstruct: OpenAIInstructEngine,\n",
      "        }\n",
      "        engine_class = engine_map[self.__class__]\n",
      "\n",
      "        engine_instance = engine_class(\n",
      "            tokenizer=tokenizer,\n",
      "            max_streaming_tokens=max_streaming_tokens,\n",
      "            timeout=timeout,\n",
      "            compute_log_probs=compute_log_probs,\n",
      "            model=model,\n",
      "            azure_endpoint=azure_endpoint,\n",
      "            api_key=api_key,\n",
      "            azure_ad_token_provider=azure_ad_token_provider,\n",
      "            api_version=api_version,\n",
      "            azure_deployment=azure_deployment,\n",
      "            client_class=openai_package.AzureOpenAI,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        super().__init__(\n",
      "            engine_instance,\n",
      "            echo=echo,\n",
      "        )\n",
      "\n",
      "\n",
      "class AzureOpenAIChat(AzureOpenAI, Chat):\n",
      "    pass\n",
      "\n",
      "\n",
      "class AzureOpenAICompletion(AzureOpenAI):\n",
      "    pass\n",
      "\n",
      "\n",
      "class AzureOpenAIInstruct(AzureOpenAI, Instruct):\n",
      "    def get_role_start(self, name):\n",
      "        return \"\"\n",
      "\n",
      "    def get_role_end(self, name):\n",
      "        if name == \"instruction\":\n",
      "            return \"<|endofprompt|>\"\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                f\"The OpenAIInstruct model does not know about the {name} role type!\"\n",
      "            )\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/__init__.py\n",
      "```py\n",
      "from ._model import Model, Instruct, Chat\n",
      "\n",
      "# local models\n",
      "from .transformers._transformers import Transformers, TransformersChat\n",
      "from .llama_cpp import LlamaCpp, LlamaCppChat, MistralInstruct, MistralChat\n",
      "from ._mock import Mock, MockChat\n",
      "\n",
      "# grammarless models (we can't do constrained decoding for them)\n",
      "from ._grammarless import Grammarless\n",
      "from .vertexai._vertexai import VertexAI, VertexAIChat, VertexAICompletion, VertexAIInstruct\n",
      "from ._azure_openai import AzureOpenAI, AzureOpenAIChat, AzureOpenAICompletion, AzureOpenAIInstruct\n",
      "from ._openai import OpenAI, OpenAIChat, OpenAIInstruct, OpenAICompletion\n",
      "from ._lite_llm import LiteLLM, LiteLLMChat, LiteLLMInstruct, LiteLLMCompletion\n",
      "from ._cohere import Cohere,CohereCompletion, CohereInstruct\n",
      "from ._anthropic import Anthropic, AnthropicChat\n",
      "from ._googleai import GoogleAI, GoogleAIChat\n",
      "from ._togetherai import TogetherAI, TogetherAIChat, TogetherAIInstruct, TogetherAICompletion\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/_openai.py\n",
      "```py\n",
      "import os\n",
      "from pathlib import Path\n",
      "import multiprocessing\n",
      "from itertools import takewhile\n",
      "import operator\n",
      "import threading\n",
      "import numpy as np\n",
      "import queue\n",
      "import time\n",
      "import tiktoken\n",
      "import re\n",
      "import diskcache as dc\n",
      "import hashlib\n",
      "import platformdirs\n",
      "\n",
      "from ._model import Chat, Instruct\n",
      "from ._grammarless import GrammarlessEngine, Grammarless\n",
      "\n",
      "try:\n",
      "    import openai as openai_package\n",
      "    is_openai = True\n",
      "except ImportError:\n",
      "    is_openai = False\n",
      "\n",
      "chat_model_pattern = r'^(ft:)?(gpt-3\\.5-turbo|gpt-4)(?:(?!-instruct$)(-\\w+)+)?(:[\\w-]+(?:[:\\w-]+)*)?(::\\w+)?$'\n",
      "\n",
      "class OpenAIEngine(GrammarlessEngine):\n",
      "    def __init__(self, tokenizer, max_streaming_tokens, timeout, compute_log_probs, model, client_class = openai_package.OpenAI, **kwargs):\n",
      "        \n",
      "        if not is_openai or not hasattr(openai_package, \"OpenAI\"):\n",
      "            raise Exception(\"Please install the openai package version >= 1 using `pip install openai -U` in order to use guidance.models.OpenAI!\")\n",
      "\n",
      "        self.client = client_class(**kwargs)\n",
      "        self.model_name = model\n",
      "\n",
      "        if tokenizer is None:\n",
      "            tokenizer = tiktoken.encoding_for_model(model)\n",
      "\n",
      "        super().__init__(\n",
      "            tokenizer, max_streaming_tokens, timeout, compute_log_probs\n",
      "        )\n",
      "\n",
      "class OpenAI(Grammarless):\n",
      "    def __init__(self, model, tokenizer=None, echo=True, api_key=None, max_streaming_tokens=1000, timeout=0.5, compute_log_probs=False, engine_class=None, **kwargs):\n",
      "        '''Build a new OpenAI model object that represents a model in a given state.\n",
      "\n",
      "        This class automatically subclasses itself into the appropriate OpenAIChat, OpenAIInstruct,\n",
      "        or OpenAICompletion subclass based on the model name.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        model : str\n",
      "            The name of the OpenAI model to use (e.g. gpt-3.5-turbo).\n",
      "        tokenizer : None or tiktoken.Encoding\n",
      "            The tokenizer to use for the given model. If set to None we use `tiktoken.encoding_for_model(model)`.\n",
      "        echo : bool\n",
      "            If true the final result of creating this model state will be displayed (as HTML in a notebook).\n",
      "        api_key : None or str\n",
      "            The OpenAI API key to use for remote requests, passed directly to the `openai.OpenAI` constructor.\n",
      "        max_streaming_tokens : int\n",
      "            The maximum number of tokens we allow this model to generate in a single stream. Normally this is set very\n",
      "            high and we rely either on early stopping on the remote side, or on the grammar terminating causing the\n",
      "            stream loop to break on the local side. This number needs to be longer than the longest stream you want\n",
      "            to generate.\n",
      "        **kwargs : \n",
      "            All extra keyword arguments are passed directly to the `openai.OpenAI` constructor. Commonly used argument\n",
      "            names include `base_url` and `organization`\n",
      "        '''\n",
      "\n",
      "        if not is_openai or not hasattr(openai_package, \"OpenAI\"):\n",
      "            raise Exception(\"Please install the openai package version >= 1 using `pip install openai -U` in order to use guidance.models.OpenAI!\")\n",
      "        \n",
      "        # if we are called directly (as opposed to through super()) then we convert ourselves to a more specific subclass if possible\n",
      "        if self.__class__ is OpenAI:\n",
      "            found_subclass = None\n",
      "\n",
      "            # chat\n",
      "            if re.match(chat_model_pattern, model):\n",
      "                found_subclass = OpenAIChat\n",
      "\n",
      "            # instruct\n",
      "            # elif \"instruct\" in model: # All current OpenAI instruct models behave as Completion models. \n",
      "            #     found_subclass = OpenAIInstruct\n",
      "\n",
      "            # regular completion\n",
      "            else:\n",
      "                found_subclass = OpenAICompletion\n",
      "            \n",
      "            # convert to any found subclass\n",
      "            self.__class__ = found_subclass\n",
      "            found_subclass.__init__(self, model, tokenizer=tokenizer, echo=echo, api_key=api_key, max_streaming_tokens=max_streaming_tokens, **kwargs)\n",
      "            return # we return since we just ran init above and don't need to run again\n",
      "\n",
      "        # this allows us to use a single constructor for all our subclasses\n",
      "        if engine_class is None:\n",
      "            engine_map = {\n",
      "                OpenAICompletion: OpenAICompletionEngine,\n",
      "                OpenAIInstruct: OpenAIInstructEngine,\n",
      "                OpenAIChat: OpenAIChatEngine\n",
      "            }\n",
      "            for k in engine_map:\n",
      "                if issubclass(self.__class__, k):\n",
      "                    engine_class = engine_map[k]\n",
      "                    break\n",
      "\n",
      "        super().__init__(\n",
      "            engine_class(\n",
      "                tokenizer=tokenizer, api_key=api_key, max_streaming_tokens=max_streaming_tokens,\n",
      "                timeout=timeout, compute_log_probs=compute_log_probs, model=model, **kwargs\n",
      "            ),\n",
      "            echo=echo\n",
      "        )\n",
      "\n",
      "class OpenAICompletion(OpenAI):\n",
      "    pass\n",
      "\n",
      "class OpenAICompletionEngine(OpenAIEngine):\n",
      "    def _generator(self, prompt, temperature):\n",
      "        \n",
      "        self._reset_shared_data(prompt, temperature) # update our shared data state\n",
      "\n",
      "        try:\n",
      "            generator = self.client.completions.create(\n",
      "                model=self.model_name,\n",
      "                prompt=prompt.decode(\"utf8\"),\n",
      "                max_tokens=self.max_streaming_tokens,\n",
      "                n=1,\n",
      "                top_p=1.0, # TODO: this should be controllable like temp (from the grammar)\n",
      "                temperature=temperature, \n",
      "                stream=True\n",
      "            )\n",
      "        except Exception as e: # TODO: add retry logic\n",
      "            raise e\n",
      "\n",
      "        for part in generator:\n",
      "            if len(part.choices) > 0:\n",
      "                chunk = part.choices[0].text or \"\"\n",
      "            else:\n",
      "                chunk = \"\"\n",
      "            yield chunk.encode(\"utf8\")\n",
      "\n",
      "class OpenAIInstruct(OpenAI, Instruct):\n",
      "    def get_role_start(self, name):\n",
      "        return \"\"\n",
      "    \n",
      "    def get_role_end(self, name):\n",
      "        if name == \"instruction\":\n",
      "            return \"<|endofprompt|>\"\n",
      "        else:\n",
      "            raise Exception(f\"The OpenAIInstruct model does not know about the {name} role type!\")\n",
      "\n",
      "class OpenAIInstructEngine(OpenAIEngine):\n",
      "    def _generator(self, prompt, temperature):\n",
      "        # start the new stream\n",
      "        eop_count = prompt.count(b'<|endofprompt|>')\n",
      "        if eop_count > 1:\n",
      "            raise Exception(\"This model has been given multiple instruct blocks or <|endofprompt|> tokens, but this is not allowed!\")\n",
      "        updated_prompt = prompt + b'<|endofprompt|>' if eop_count == 0 else prompt\n",
      "\n",
      "        self._reset_shared_data(updated_prompt, temperature)\n",
      "\n",
      "        try:\n",
      "            generator = self.client.completions.create(\n",
      "                model=self.model_name,\n",
      "                prompt=self._shared_state[\"data\"].decode(\"utf8\"), \n",
      "                max_tokens=self.max_streaming_tokens, \n",
      "                n=1, \n",
      "                top_p=1.0, # TODO: this should be controllable like temp (from the grammar)\n",
      "                temperature=temperature, \n",
      "                stream=True\n",
      "            )\n",
      "        except Exception as e: # TODO: add retry logic\n",
      "            raise e\n",
      "\n",
      "        for part in generator:\n",
      "            if len(part.choices) > 0:\n",
      "                chunk = part.choices[0].text or \"\"\n",
      "            else:\n",
      "                chunk = \"\"\n",
      "            yield chunk.encode(\"utf8\")\n",
      "\n",
      "class OpenAIChat(OpenAI, Chat):\n",
      "    pass\n",
      "\n",
      "class OpenAIChatEngine(OpenAIEngine):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "        path = os.path.join(platformdirs.user_cache_dir(\"guidance\"), \"openai.tokens\")\n",
      "        self.cache = dc.Cache(path)\n",
      "        \n",
      "    def _hash_prompt(self, prompt):\n",
      "        return hashlib.sha256(f\"{prompt}\".encode()).hexdigest()\n",
      "\n",
      "    def _generator(self, prompt, temperature):\n",
      "        \n",
      "        # find the role tags\n",
      "        pos = 0\n",
      "        role_end = b'<|im_end|>'\n",
      "        messages = []\n",
      "        found = True\n",
      "        while found:\n",
      "\n",
      "            # find the role text blocks\n",
      "            found = False\n",
      "            for role_name,start_bytes in ((\"system\", b'<|im_start|>system\\n'), (\"user\", b'<|im_start|>user\\n'), (\"assistant\", b'<|im_start|>assistant\\n')):\n",
      "                if prompt[pos:].startswith(start_bytes):\n",
      "                    pos += len(start_bytes)\n",
      "                    end_pos = prompt[pos:].find(role_end)\n",
      "                    if end_pos < 0:\n",
      "                        assert role_name == \"assistant\", \"Bad chat format! Last role before gen needs to be assistant!\"\n",
      "                        break\n",
      "                    btext = prompt[pos:pos+end_pos]\n",
      "                    pos += end_pos + len(role_end)\n",
      "                    messages.append({\"role\": role_name, \"content\": btext.decode(\"utf8\")})\n",
      "                    found = True\n",
      "                    break\n",
      "        \n",
      "        \n",
      "        \n",
      "        # Add nice exception if no role tags were used in the prompt.\n",
      "        # TODO: Move this somewhere more general for all chat models?\n",
      "        if messages == []:\n",
      "            raise ValueError(f\"The OpenAI model {self.model_name} is a Chat-based model and requires role tags in the prompt! \\\n",
      "            Make sure you are using guidance context managers like `with system():`, `with user():` and `with assistant():` \\\n",
      "            to appropriately format your guidance program for this type of model.\")\n",
      "  \n",
      "\n",
      "        # Update shared data state\n",
      "        self._reset_shared_data(prompt[:pos], temperature)\n",
      "\n",
      "        # Use cache only when temperature is 0\n",
      "        if temperature == 0:\n",
      "            cache_key = self._hash_prompt(prompt)\n",
      "\n",
      "            # Check if the result is already in the cache\n",
      "            if cache_key in self.cache:\n",
      "                for chunk in self.cache[cache_key]:\n",
      "                    yield chunk\n",
      "                return\n",
      "\n",
      "        # API call and response handling\n",
      "        try:\n",
      "            generator = self.client.chat.completions.create(\n",
      "                model=self.model_name,\n",
      "                messages=messages,\n",
      "                max_tokens=self.max_streaming_tokens,\n",
      "                n=1,\n",
      "                top_p=1.0,# TODO: this should be controllable like temp (from the grammar)\n",
      "                temperature=temperature,\n",
      "                stream=True\n",
      "            )\n",
      "\n",
      "            if temperature == 0:\n",
      "                cached_results = []\n",
      "\n",
      "            for part in generator:\n",
      "                if len(part.choices) > 0:\n",
      "                    chunk = part.choices[0].delta.content or \"\"\n",
      "                else:\n",
      "                    chunk = \"\"\n",
      "                encoded_chunk = chunk.encode(\"utf8\")\n",
      "                yield encoded_chunk\n",
      "\n",
      "                if temperature == 0:\n",
      "                    cached_results.append(encoded_chunk)\n",
      "\n",
      "            # Cache the results after the generator is exhausted\n",
      "            if temperature == 0:\n",
      "                self.cache[cache_key] = cached_results\n",
      "\n",
      "        except Exception as e: # TODO: add retry logic\n",
      "            raise e\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/_googleai.py\n",
      "```py\n",
      "import re\n",
      "from ._model import Chat, Instruct\n",
      "from ._grammarless import Grammarless, GrammarlessEngine\n",
      "import tiktoken\n",
      "import os\n",
      "_image_token_pattern = re.compile(r'<\\|_image:(.*)\\|>')\n",
      "\n",
      "\n",
      "class GoogleAIEngine(GrammarlessEngine):\n",
      "    def __init__(self, model, tokenizer, api_key, max_streaming_tokens, timeout, compute_log_probs, **kwargs):\n",
      "        try:\n",
      "            import google.generativeai as genai\n",
      "        except ImportError:\n",
      "            raise Exception(\"Please install the Google AI Studio(makersuite.google.com) package using `pip install google-generativeai google-ai-generativelanguage` in order to use guidance.models.GoogleAI!\")\n",
      "\n",
      "        assert not compute_log_probs, \"We don't support compute_log_probs=True yet for GoogleAIEngine!\"\n",
      "\n",
      "        if api_key is None:\n",
      "            api_key = os.environ.get(\"GOOGLEAI_API_KEY\")\n",
      "        \n",
      "        genai.configure(api_key=api_key)\n",
      "        \n",
      "        # Gemini does not have a public tokenizer, so we pretend it tokenizes like gpt2...\n",
      "        if tokenizer is None:\n",
      "            tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
      "        self.model_name = model\n",
      "\n",
      "        self.model_obj = genai.GenerativeModel(self.model_name, **kwargs)\n",
      "\n",
      "        super().__init__(tokenizer, max_streaming_tokens, timeout, compute_log_probs)\n",
      "\n",
      "class GoogleAI(Grammarless):\n",
      "    def __init__(self, model, tokenizer=None, echo=True, api_key=None, max_streaming_tokens=None, timeout=0.5, compute_log_probs=False, **kwargs):\n",
      "        '''Build a new GoogleAI model object that represents a model in a given state.'''\n",
      "\n",
      "        # if we are called directly (as opposed to through super()) then we convert ourselves to a more specific subclass if possible\n",
      "        if self.__class__ is GoogleAI:\n",
      "            found_subclass = None\n",
      "\n",
      "            # chat\n",
      "            found_subclass = GoogleAIChat # we assume all models are chat right now\n",
      "\n",
      "            # instruct\n",
      "            # elif \"instruct\" in model:\n",
      "            #     found_subclass = GoogleAIInstruct\n",
      "\n",
      "            # # regular completion\n",
      "            # else:\n",
      "            #     found_subclass = GoogleAICompletion\n",
      "            \n",
      "            # convert to any found subclass\n",
      "            self.__class__ = found_subclass\n",
      "            found_subclass.__init__(self, model, tokenizer=None, echo=True, api_key=api_key, max_streaming_tokens=max_streaming_tokens, timeout=timeout, compute_log_probs=False, **kwargs)\n",
      "            return # we return since we just ran init above and don't need to run again\n",
      "\n",
      "        # this allows us to use a single constructor for all our subclasses\n",
      "        engine_map = {\n",
      "            GoogleAIChat: GoogleAIChatEngine\n",
      "        }\n",
      "\n",
      "        super().__init__(\n",
      "            engine=engine_map[self.__class__](\n",
      "                model=model,\n",
      "                tokenizer=tokenizer,\n",
      "                api_key=api_key,\n",
      "                max_streaming_tokens=max_streaming_tokens,\n",
      "                timeout=timeout,\n",
      "                compute_log_probs=compute_log_probs,\n",
      "                **kwargs\n",
      "            ),\n",
      "            echo=echo\n",
      "        )\n",
      "\n",
      "\n",
      "class GoogleAIChatEngine(GoogleAIEngine):\n",
      "    def _generator(self, prompt, temperature):\n",
      "        \n",
      "        # find the system text\n",
      "        pos = 0\n",
      "        system_start = b'<|im_start|>system\\n'\n",
      "        user_start = b'<|im_start|>user\\n'\n",
      "        assistant_start = b'<|im_start|>assistant\\n'\n",
      "        role_end = b'<|im_end|>'\n",
      "        # system_start_pos = prompt.startswith(system_start)\n",
      "        \n",
      "        # find the system text\n",
      "        system_text = b''\n",
      "        if prompt.startswith(system_start):\n",
      "            pos += len(system_start)\n",
      "            system_end_pos = prompt.find(role_end)\n",
      "            system_text = prompt[pos:system_end_pos]\n",
      "            pos = system_end_pos + len(role_end)\n",
      "\n",
      "        # find the user/assistant pairs\n",
      "        messages = []\n",
      "        valid_end = False\n",
      "        while True:\n",
      "\n",
      "            # find the user text\n",
      "            if prompt[pos:].startswith(user_start):\n",
      "                pos += len(user_start)\n",
      "                end_pos = prompt[pos:].find(role_end)\n",
      "                if end_pos < 0:\n",
      "                    break\n",
      "                messages.append(dict(\n",
      "                    role=\"user\",\n",
      "                    content=prompt[pos:pos+end_pos].decode(\"utf8\"),\n",
      "                ))\n",
      "                pos += end_pos + len(role_end)\n",
      "            elif prompt[pos:].startswith(assistant_start):\n",
      "                pos += len(assistant_start)\n",
      "                end_pos = prompt[pos:].find(role_end)\n",
      "                if end_pos < 0:\n",
      "                    valid_end = True\n",
      "                    break\n",
      "                messages.append(dict(\n",
      "                    role=\"model\",\n",
      "                    content=prompt[pos:pos+end_pos].decode(\"utf8\"),\n",
      "                ))\n",
      "                pos += end_pos + len(role_end)\n",
      "            else:\n",
      "                raise Exception(\"It looks like your prompt is not a well formed chat prompt! Please enclose all model state appends inside chat role blocks like `user()` or `assistant()`.\")\n",
      "            \n",
      "        self._data = prompt[:pos]\n",
      "\n",
      "        assert len(messages) > 0, \"Bad chat format! No chat blocks were defined.\"\n",
      "        assert messages[-1][\"role\"] == \"user\", \"Bad chat format! There must be a user() role before the last assistant() role.\"\n",
      "        assert valid_end, \"Bad chat format! You must generate inside assistant() roles.\"\n",
      "\n",
      "        # TODO: don't make a new session on every call\n",
      "        # last_user_text = messages.pop().content\n",
      "        \n",
      "        return self._start_generator(system_text.decode(\"utf8\"), messages, temperature)\n",
      "\n",
      "        # kwargs = {}\n",
      "        # if self.max_streaming_tokens is not None:\n",
      "        #     kwargs[\"max_output_tokens\"] = self.max_streaming_tokens\n",
      "        # generator = chat_session.send_message_streaming(last_user_text, temperature=temperature, **kwargs)\n",
      "\n",
      "        # for chunk in generator:\n",
      "        #     yield chunk.text.encode(\"utf8\")\n",
      "    def _start_chat(self, system_text, messages):\n",
      "        assert system_text == \"\", \"We don't support passing system text to Gemini models (yet?)!\"\n",
      "        out = self.model_obj.start_chat(\n",
      "            history=messages\n",
      "        )\n",
      "        return out\n",
      "    \n",
      "    def _start_generator(self, system_text, messages, temperature):\n",
      "        from google.ai.generativelanguage import Content, Part, Blob\n",
      "        # last_user_text = messages[-1][\"content\"]\n",
      "        formated_messages = []\n",
      "        for m in messages:\n",
      "            raw_parts = _image_token_pattern.split(m[\"content\"])\n",
      "            parts = []\n",
      "            for i in range(0, len(raw_parts), 2):\n",
      "                \n",
      "                # append the text portion\n",
      "                if len(raw_parts[i]) > 0:\n",
      "                    parts.append(Part(text=raw_parts[i]))\n",
      "                \n",
      "                # append any image\n",
      "                if i + 1 < len(raw_parts):\n",
      "                    # parts.append(Part.from_image(Image.from_bytes(self[raw_parts[i+1]])))\n",
      "                    parts.append(\n",
      "                        Part(\n",
      "                inline_data=Blob(\n",
      "                    mime_type='image/jpeg',\n",
      "                    data=self[raw_parts[i+1]]\n",
      "                )\n",
      "                    ))\n",
      "            formated_messages.append(Content(role=m[\"role\"], parts=parts))\n",
      "        last_user_parts = formated_messages.pop() # remove the last user stuff that goes in send_message (and not history)\n",
      "\n",
      "        chat_session = self.model_obj.start_chat(\n",
      "            history=formated_messages,\n",
      "        )\n",
      "\n",
      "        generation_config = {\n",
      "            \"temperature\": temperature\n",
      "        }\n",
      "        if self.max_streaming_tokens is not None:\n",
      "            generation_config[\"max_output_tokens\"] = self.max_streaming_tokens\n",
      "        generator = chat_session.send_message(last_user_parts, generation_config=generation_config, stream=True)\n",
      "\n",
      "        for chunk in generator:\n",
      "            yield chunk.candidates[0].content.parts[0].text.encode(\"utf8\")\n",
      "\n",
      "class GoogleAIChat(GoogleAI, Chat):\n",
      "    pass\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/_remote.py\n",
      "```py\n",
      "import requests\n",
      "import os\n",
      "import base64\n",
      "\n",
      "from ._model import Engine, EngineCallResponse\n",
      "\n",
      "class RemoteEngine(Engine):\n",
      "    '''This connects to a remote guidance server and runs all computation using the remote engine.'''\n",
      "    def __init__(self, server_url, api_key, verify=None):\n",
      "        self.server_url = server_url\n",
      "        self.api_key = api_key\n",
      "        if verify is None:\n",
      "            verify = os.getenv(\"GUIDANCE_SSL_CERTFILE\", None)\n",
      "        self.verify_crt = verify\n",
      "\n",
      "    def __call__(self, parser, grammar, ensure_bos_token=True):\n",
      "        # Prepare the request data\n",
      "        data = {\n",
      "            \"parser\": parser,\n",
      "            \"grammar\": base64.b64encode(grammar.serialize()).decode('utf-8')\n",
      "        }\n",
      "\n",
      "        headers = {\n",
      "            \"x-api-key\": self.api_key,\n",
      "            \"Content-Type\": \"application/json\"\n",
      "        }\n",
      "\n",
      "        # Send the request to the server\n",
      "        response = requests.post(self.server_url + \"/extend\", json=data, headers=headers, stream=True, verify=self.verify_crt)\n",
      "\n",
      "        # Check for valid response\n",
      "        if response.status_code != 200:\n",
      "            response.raise_for_status()\n",
      "\n",
      "        # Process and yield the response data\n",
      "        for chunk in response.iter_content(chunk_size=None):  # chunk_size=None means it'll stream the content\n",
      "            response_data = EngineCallResponse.deserialize(chunk)\n",
      "            yield response_data\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/_grammarless.py\n",
      "```py\n",
      "import threading\n",
      "import numpy as np\n",
      "import queue\n",
      "import time\n",
      "import tiktoken\n",
      "import re\n",
      "import logging\n",
      "from ._model import Tokenizer, Engine, Model, format_pattern, ConstraintException\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class GrammarlessTokenizer(Tokenizer):\n",
      "    def __init__(self, tokenizer):\n",
      "\n",
      "        # Grammarless models don't always have public tokenizations, so when not provided we pretend they tokenize like gpt2...\n",
      "        if tokenizer is None:\n",
      "            tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
      "\n",
      "        # tiktoken tokenizer was given\n",
      "        if hasattr(tokenizer, \"decode_single_token_bytes\"):\n",
      "            special_map = {v: k for k,v in tokenizer._special_tokens.items()}\n",
      "            byte_tokens = [] # b'<|invalid_special_token|>'\n",
      "            for i in range(tokenizer.n_vocab):\n",
      "                try:\n",
      "                    bval = tokenizer.decode_single_token_bytes(i)\n",
      "                except KeyError:\n",
      "                    bval = special_map.get(i, b'<|invalid_special_token|>')\n",
      "                byte_tokens.append(bval)\n",
      "\n",
      "            bos_token_id = None\n",
      "            eos_token_id = tokenizer._special_tokens[\"<|endoftext|>\"]\n",
      "        \n",
      "        # a transformer tokenizer was given that has a byte_decoder\n",
      "        elif hasattr(tokenizer, \"byte_decoder\"):\n",
      "            byte_tokens = []\n",
      "            for i in range(tokenizer.vocab_size):\n",
      "                byte_coded = bytes([tokenizer.byte_decoder[c] for c in tokenizer.convert_ids_to_tokens(i)])\n",
      "                byte_tokens.append(byte_coded)\n",
      "            bos_token_id = tokenizer.bos_token_id\n",
      "            eos_token_id = tokenizer.eos_token_id\n",
      "        \n",
      "        # a transformer tokenizer was given with byte_decoder\n",
      "        elif hasattr(tokenizer, \"convert_ids_to_tokens\"):\n",
      "            byte_tokens = [bytes(tokenizer.convert_tokens_to_string(['a', tokenizer.convert_ids_to_tokens(i)])[1:], encoding=\"utf8\") for i in range(tokenizer.vocab_size)]\n",
      "            bos_token_id = tokenizer.bos_token_id\n",
      "            eos_token_id = tokenizer.eos_token_id\n",
      "\n",
      "        # a HuggingFace tokenizers tokenizer was given with id_to_token\n",
      "        elif hasattr(tokenizer, \"id_to_token\"):\n",
      "            a_token_ids = tokenizer.encode(\"a\").ids\n",
      "            if len(a_token_ids) == 3:\n",
      "                bos_token_id = a_token_ids[0]\n",
      "                a_id = a_token_ids[1]\n",
      "                eos_token_id = a_token_ids[2]\n",
      "            else:\n",
      "                raise Exception(\"This tokenizer does not seem to have a BOS and EOS, support for this need to be implemented still.\")\n",
      "\n",
      "            byte_tokens = [bytes(tokenizer.decode([a_id, i])[1:], encoding=\"utf8\") for i in range(tokenizer.get_vocab_size())]\n",
      "            for i,b in enumerate(byte_tokens):\n",
      "                if b == b'':\n",
      "                    byte_tokens[i] = bytes(tokenizer.id_to_token(i), encoding=\"utf8\")\n",
      "\n",
      "        else:\n",
      "            raise Exception(\"The tokenizer given was not of a recognized type!\")\n",
      "        \n",
      "        super().__init__(\n",
      "            byte_tokens,\n",
      "            bos_token_id,\n",
      "            eos_token_id\n",
      "        )\n",
      "\n",
      "\n",
      "class GrammarlessEngine(Engine):\n",
      "    def __init__(self, tokenizer, max_streaming_tokens, timeout, compute_log_probs):\n",
      "        self.max_streaming_tokens = max_streaming_tokens\n",
      "        self.timeout = timeout\n",
      "\n",
      "        self._data_queue = queue.Queue() # this is where the streaming thread puts results\n",
      "        self._data = b'' # these are the bytes we are ready to use in the main thread\n",
      "        self._not_running_stream = threading.Event() # this is phrased negatively so we can wait for the stop event\n",
      "        self._last_call = 0\n",
      "        self._num_calls_made = 0\n",
      "        self._current_temp = 0\n",
      "        self._last_stream_start = None\n",
      "\n",
      "        self._not_running_stream.set()\n",
      "\n",
      "        self.max_repeated_calls = 10\n",
      "        self.timeout = timeout\n",
      "\n",
      "        # If tokenizer is not already an instance of Tokenizer, then instantiate it as a GrammarlessTokenizer\n",
      "        if not isinstance(tokenizer, Tokenizer):\n",
      "            tokenizer = GrammarlessTokenizer(tokenizer)\n",
      "\n",
      "        # build the \n",
      "        super().__init__(\n",
      "            tokenizer=tokenizer,\n",
      "            compute_log_probs=compute_log_probs\n",
      "        )\n",
      "\n",
      "    def __call__(self, *args, **kwargs):\n",
      "        self._num_calls_made = 0 # reset the number of calls count so we only limit the number of calls within a single grammar execution\n",
      "        return super().__call__(*args, **kwargs)\n",
      "    \n",
      "    def _running_stream(self):\n",
      "        return not self._not_running_stream.is_set() # wrap double negation (which)\n",
      "\n",
      "    def _start_generator_stream(self, generator):\n",
      "        logger.debug(f\"start Grammarless._start_generator_stream\")\n",
      "        dqueue = self._data_queue\n",
      "        first_iteration = True\n",
      "        try: \n",
      "            for chunk in generator:\n",
      "                logger.debug(f\"Got chunk: \" + str(chunk))\n",
      "                if len(chunk) > 0:\n",
      "                    dqueue.put(chunk)\n",
      "                if self._not_running_stream.is_set() or not first_iteration and time.time() - self._last_call > self.timeout:\n",
      "                    break\n",
      "                first_iteration = False\n",
      "        \n",
      "        # we pass any exceptions back to the main thread\n",
      "        except Exception as e:\n",
      "            self._not_running_stream.set()\n",
      "            while not dqueue.empty(): \n",
      "                dqueue.get()\n",
      "            dqueue.put(e)\n",
      "\n",
      "        if self._running_stream():\n",
      "            dqueue.put(self.tokenizer.eos_token)\n",
      "        self._not_running_stream.set()\n",
      "        dqueue.put(b'') # so we never get stuck waiting for a running stream to return something\n",
      "\n",
      "    def _start_new_stream(self, prompt, temperature):\n",
      "\n",
      "        # make sure the display is up to date (since we are about to delay for a while)\n",
      "        # TODO: how can we handle this better since the engine is now separate from the client?\n",
      "        #       we could use a timeout for the GUI update throttling, those were just kind of slow... (but would be best)\n",
      "        # self._update_display(throttle=False)\n",
      "\n",
      "        if self._num_calls_made > self.max_repeated_calls:\n",
      "            raise Exception(f\"We have exceeded the maximum number of repeat calls ({self.max_repeated_calls}) per grammar execution!\")\n",
      "\n",
      "        # stop any running stream\n",
      "        if self._running_stream():\n",
      "            self._not_running_stream.set() # stop the generator\n",
      "            self._remote_thread.join() # wait for the thread to finish\n",
      "\n",
      "        # clear the data queue\n",
      "        while not self._data_queue.empty(): \n",
      "            self._data_queue.get()\n",
      "\n",
      "        # start the new stream\n",
      "        self._used_bytes_len = 0\n",
      "        self._current_temp = temperature\n",
      "        self._last_call = time.time()\n",
      "        generator = self._generator(prompt, temperature)\n",
      "        self._not_running_stream.clear() # so we know we are running\n",
      "        self._num_calls_made += 1\n",
      "        self._remote_thread = threading.Thread(target=self._start_generator_stream, args=(generator,))\n",
      "        self._remote_thread.start()\n",
      "\n",
      "    def _reset_shared_data(self, new_data, temperature):\n",
      "        \"\"\"Should be called by _generator calls to reset the shared data state.\"\"\"\n",
      "        if temperature == 0 and self._last_stream_start == new_data:\n",
      "            raise self._report_failed_match(new_data)\n",
      "        self._data = new_data\n",
      "        self._last_stream_start = self._data\n",
      "    \n",
      "    def get_logits(self, token_ids, forced_bytes, current_temp):\n",
      "        '''Computes the logits for the given token state.\n",
      "        \n",
      "        This overrides a method from the Local class that is used to get\n",
      "        inference results from the model.\n",
      "        '''\n",
      "\n",
      "        logger.debug(f\"start Grammarless._get_logits(token_ids={token_ids})\")\n",
      "\n",
      "        if len(token_ids) == 0:\n",
      "            raise ValueError(\"token_ids must contain some tokens.\")\n",
      "        \n",
      "        # compute the prompt bytes\n",
      "        whole_token_prompt = b''.join([self.tokenizer.tokens[i] for i in token_ids])\n",
      "        prompt = whole_token_prompt + forced_bytes\n",
      "\n",
      "        self._last_call = time.time()\n",
      "\n",
      "        # keep looping until we have at least one more byte past our prompt\n",
      "        token_id = None\n",
      "        restarted = False # track if we have restarted the data stream during this call\n",
      "        while True:\n",
      "\n",
      "            # if the generation temperature changes we have to restart\n",
      "            if self._current_temp != current_temp:\n",
      "                self._start_new_stream(prompt, current_temp)\n",
      "                continue\n",
      "\n",
      "            # try and get the next token id\n",
      "            elif self._data.startswith(prompt):\n",
      "                token_id = self._get_next_token(len(prompt)-len(forced_bytes))\n",
      "                if token_id is not None:\n",
      "                    \n",
      "                    # if we have a non-zero sampling temperature we can't reuse bytes\n",
      "                    new_used_len = len(whole_token_prompt) + len(self.tokenizer.tokens[token_id])\n",
      "                    if current_temp > 0 and self._used_bytes_len >= new_used_len:\n",
      "                        token_id = None\n",
      "                        self._start_new_stream(prompt, current_temp)\n",
      "                        continue\n",
      "                    \n",
      "                    # ...otherwise we have found the token id we want to emit\n",
      "                    else:\n",
      "                        self._used_bytes_len = len(whole_token_prompt) + len(self.tokenizer.tokens[token_id])\n",
      "                        break\n",
      "\n",
      "            # restart if extending our data will never lead to matching our prompt\n",
      "            elif not self._data.startswith(prompt) and len(self._data) >= len(prompt): #not prompt.startswith(self._data): # len(self._data) >= len(prompt) or \n",
      "\n",
      "                # check if we have already restarted once and so retrying by default is not likely to be helpful\n",
      "                if restarted:\n",
      "                    raise self._report_failed_match(prompt)\n",
      "\n",
      "                # check the length of the prefix match\n",
      "                match_len = 0\n",
      "                found_mismatch = False\n",
      "                data = self._data\n",
      "                for match_len,v in enumerate(prompt):\n",
      "                    if v != data[match_len]:\n",
      "                        found_mismatch = True\n",
      "                        break\n",
      "                if not found_mismatch:\n",
      "                    match_len = len(prompt)\n",
      "                leftover = prompt[match_len:]\n",
      "\n",
      "                # record any active non-empty role ends. Ignore role ends that are spaces\n",
      "                parts = [b\"<|im_end|>\", self.tokenizer.eos_token] # note we assume we are role tags that end with <|im_end|>\n",
      "\n",
      "                # for _,role_end_str in self.opened_blocks.values():\n",
      "                #     role_end_str = format_pattern.sub(\"\", role_end_str)\n",
      "                #     if len(role_end_str) > 0 and not re.fullmatch(r'\\s+', role_end_str):\n",
      "                #         parts.append(role_end_str.encode(\"utf8\"))\n",
      "\n",
      "                # # record the eos token\n",
      "                # parts.append(self.eos_token)\n",
      "\n",
      "                # see if adding an end token would work here (if so we avoid recalling the server and just produce an end token)\n",
      "                found_match = False\n",
      "                for p in parts:\n",
      "                    if p.startswith(leftover):\n",
      "                        self._data = self._data[:match_len] + p\n",
      "                        logger.debug(f'automatically adding an end token since it fits the forcing of the grammar')\n",
      "                        found_match = True\n",
      "                        break\n",
      "                if found_match:\n",
      "                    continue # start our loop over again\n",
      "\n",
      "                logger.debug(f'restarting a stream because the data we have does not match the ids. We have {str(self._data)} but the prompt is {str(prompt)}')\n",
      "                restarted = True\n",
      "                self._start_new_stream(prompt, current_temp)\n",
      "\n",
      "            # extend our data with a chunk from the model stream\n",
      "            if not self._data_queue.empty():\n",
      "                new_bytes = self._data_queue.get_nowait()\n",
      "                if isinstance(new_bytes, Exception):\n",
      "                    raise new_bytes\n",
      "                \n",
      "                # if we are at the end of the generation then we try again allowing for early token stopping\n",
      "                if len(new_bytes) == 0:\n",
      "                    token_id = self._get_next_token(len(prompt), allow_early_stop=True)\n",
      "                    if token_id is not None:\n",
      "                        break\n",
      "                self._data += new_bytes\n",
      "            \n",
      "            # but if there is nothing and we are not running then we start a stream\n",
      "            elif self._not_running_stream.is_set():\n",
      "                logger.debug(\"starting a new stream because there is no data to read and no stream running...\")\n",
      "                restarted = True\n",
      "                self._start_new_stream(prompt, current_temp)\n",
      "\n",
      "            # we wait for the running stream to put something in the queue\n",
      "            else:\n",
      "                self._last_call = 10e9 # set to essentialy infinity so we don't stop the data stream while we are waiting for it\n",
      "                new_bytes = self._data_queue.get()\n",
      "                if isinstance(new_bytes, Exception):\n",
      "                    raise new_bytes\n",
      "                self._data += new_bytes\n",
      "                self._last_call = time.time() # reset out call time to allow the data stream to time out if we happen to be done with it\n",
      "        \n",
      "        # # if we don't have the next byte of data yet then we wait for it (from the streaming thread)\n",
      "        # if len(self._data) == len(prompt):\n",
      "        #     self._data += self._data_queue.get() \n",
      "\n",
      "        # token_id = self._get_next_token(len(prompt))\n",
      "\n",
      "        # set the logits to the next byte the model picked\n",
      "        logits = np.ones(len(self.tokenizer.tokens)) * -np.inf\n",
      "        logits[token_id] = 100\n",
      "        if token_id != self.tokenizer.eos_token:\n",
      "            logits[self.tokenizer.eos_token_id] = 0 # we always allow the model to use EOS if that is the only way forward\n",
      "        \n",
      "        return logits\n",
      "    \n",
      "    def _report_failed_match(self, prompt):\n",
      "\n",
      "        # check the length of the prefix match\n",
      "        match_len = 0\n",
      "        found_mismatch = False\n",
      "        data = self._data\n",
      "        for match_len,v in enumerate(prompt):\n",
      "            if v != data[match_len]:\n",
      "                found_mismatch = True\n",
      "                break\n",
      "        if not found_mismatch:\n",
      "            match_len = len(prompt)\n",
      "        leftover = prompt[match_len:]\n",
      "\n",
      "        # compute the mismatch parts\n",
      "        data_after_prompt = self._data[match_len:]\n",
      "        if len(data_after_prompt) > 40:\n",
      "            data_after_prompt = data_after_prompt[:40] + b\"...\"\n",
      "        prompt_tail = prompt[:match_len]\n",
      "        if len(prompt_tail) > 40:\n",
      "            prompt_tail = b\"...\" + prompt_tail[-40:]\n",
      "\n",
      "        # show in the model output where and how we diverged from the grammar\n",
      "        try:\n",
      "            # just for display when echo is on\n",
      "            already_shown = len(self._current_prompt().encode())\n",
      "            self += self._data[already_shown:match_len].decode() + f\"<||_html:<span style='color: rgba(165,0,0,1);' title='{leftover}'><span style='text-decoration: underline;'>{data_after_prompt.decode()}</span></span>_||>\"\n",
      "        except:\n",
      "            pass # could not decode the data the model generated into a string...\n",
      "        \n",
      "        # create an exception for users to deal with (that our caller can throw)\n",
      "        return ConstraintException(\n",
      "            f\"The model attempted to generate {str(data_after_prompt)} after the prompt `{prompt_tail}`, but that does\\n\" +\n",
      "            \"not match the given grammar constraints! Since your model is a remote API that does not support full guidance\\n\" +\n",
      "            \"integration we cannot force the model to follow the grammar, only flag an error when it fails to match.\\n\" +\n",
      "            \"You can try to address this by improving the prompt, making your grammar more flexible, rerunning with\\n\" +\n",
      "            \"a non-zero temperature, or using a model that supports full guidance grammar constraints.\"\n",
      "        )\n",
      "    \n",
      "    def _get_next_token(self, pos, allow_early_stop=False):\n",
      "        data = self._data\n",
      "        trie = self._token_trie\n",
      "        token_id = None\n",
      "        while True:\n",
      "            \n",
      "            # see if we have run out of data\n",
      "            if pos >= len(data):\n",
      "                if allow_early_stop:\n",
      "                    return token_id\n",
      "                else:\n",
      "                    return None\n",
      "            \n",
      "            # try and walk down the trie\n",
      "            next_byte = data[pos:pos+1]\n",
      "            if trie.has_child(next_byte):\n",
      "                trie = trie.child(next_byte)\n",
      "                pos += 1\n",
      "                if trie.value >= 0:\n",
      "                    token_id = trie.value\n",
      "            else:\n",
      "                return token_id # this is the longest greedy token match we can make\n",
      "\n",
      "\n",
      "class Grammarless(Model):\n",
      "    '''The base class for all remote models (hosted behind a remote API).'''\n",
      "    pass\n",
      "    \n",
      "    # def __init__(self, model, tokenizer=None, echo=True, compute_log_probs=False, max_streaming_tokens=None, timeout=0.5):\n",
      "    #     '''Build a new remote model object that represents a model in a given state.\n",
      "\n",
      "    #     This is an abstract class. To instantiate it use a specific subclass like guidance.models.OpenAI.\n",
      "    #     '''\n",
      "    #     super().__init__(\n",
      "    #         engine=GrammarlessEngine(model, tokenizer, max_streaming_tokens, timeout, compute_log_probs),\n",
      "    #         echo=echo\n",
      "    #     )\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/_anthropic.py\n",
      "```py\n",
      "import os\n",
      "import tiktoken\n",
      "\n",
      "from ._model import Chat, Instruct\n",
      "from ._grammarless import GrammarlessEngine, Grammarless\n",
      "\n",
      "class AnthropicEngine(GrammarlessEngine):\n",
      "    def __init__(self, model, tokenizer, api_key, timeout, max_streaming_tokens, compute_log_probs, **kwargs):        \n",
      "        try:\n",
      "            from anthropic import Anthropic\n",
      "        except ImportError:\n",
      "            raise Exception(\"Please install the anthropic package version >= 0.7 using `pip install anthropic -U` in order to use guidance.models.Anthropic!\")\n",
      "        \n",
      "        # if we are called directly (as opposed to through super()) then we convert ourselves to a more specific subclass if possible\n",
      "        if self.__class__ is Anthropic:\n",
      "            raise Exception(\"The Anthropic class is not meant to be used directly! Please use AnthropicChat assuming the model you are using is chat-based.\")\n",
      "\n",
      "        if api_key is None:\n",
      "            api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
      "\n",
      "        if api_key is None:\n",
      "            raise Exception(\"Expected an api_key argument or the ANTHROPIC_API_KEY environment variable to be set!\")\n",
      "\n",
      "        self.anthropic = Anthropic(api_key=api_key, **kwargs)\n",
      "\n",
      "        self.model_name = model\n",
      "\n",
      "        # we pretend it tokenizes like gpt2 if tiktoken does not know about it... TODO: make this better\n",
      "        if tokenizer is None:\n",
      "            try:\n",
      "                tokenizer = tiktoken.encoding_for_model(model)\n",
      "            except:\n",
      "                tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
      "\n",
      "        super().__init__(tokenizer, max_streaming_tokens, timeout, compute_log_probs)\n",
      "\n",
      "    def _generator(self, prompt, temperature):\n",
      "\n",
      "        # update our shared data state\n",
      "        self._reset_shared_data(prompt, temperature)\n",
      "\n",
      "        try:\n",
      "            generator = self.anthropic.completions.create(\n",
      "                model=self.model_name,\n",
      "                prompt=prompt.decode(\"utf8\"),\n",
      "                max_tokens_to_sample=self.max_streaming_tokens,\n",
      "                stream=True,\n",
      "                temperature=temperature\n",
      "            )\n",
      "        except Exception as e: # TODO: add retry logic\n",
      "            raise e\n",
      "        \n",
      "        for part in generator:\n",
      "            chunk = part.completion or \"\"\n",
      "            # print(chunk)\n",
      "            yield chunk.encode(\"utf8\")\n",
      "\n",
      "class Anthropic(Grammarless):\n",
      "    '''Represents an Anthropic model as exposed through their remote API.\n",
      "    \n",
      "    Note that because this uses a remote API endpoint without built-in guidance support\n",
      "    there are some things we cannot do, like force the model to follow a pattern inside\n",
      "    a chat role block.\n",
      "    '''\n",
      "    def __init__(self, model, tokenizer=None, echo=True, api_key=None, timeout=0.5, max_streaming_tokens=1000, compute_log_probs=False, **kwargs):\n",
      "        '''Build a new Anthropic model object that represents a model in a given state.'''\n",
      "        \n",
      "        # if we are called directly (as opposed to through super()) then we convert ourselves to a more specific subclass if possible\n",
      "        if self.__class__ is Anthropic:\n",
      "            found_subclass = None\n",
      "\n",
      "            # chat\n",
      "            found_subclass = AnthropicChat # we assume all models are chat right now\n",
      "            \n",
      "            # convert to any found subclass\n",
      "            self.__class__ = found_subclass\n",
      "            found_subclass.__init__(self, model, tokenizer=tokenizer, echo=echo, api_key=api_key, max_streaming_tokens=max_streaming_tokens, timeout=timeout, compute_log_probs=compute_log_probs, **kwargs)\n",
      "            return # we return since we just ran init above and don't need to run again\n",
      "\n",
      "        super().__init__(\n",
      "            engine=AnthropicEngine(\n",
      "                model=model,\n",
      "                tokenizer=tokenizer,\n",
      "                api_key=api_key,\n",
      "                max_streaming_tokens=max_streaming_tokens,\n",
      "                timeout=timeout,\n",
      "                compute_log_probs=compute_log_probs,\n",
      "                **kwargs\n",
      "            ),\n",
      "            echo=echo\n",
      "        )\n",
      "\n",
      "class AnthropicChat(Anthropic, Chat):\n",
      "    def get_role_start(self, role_name, **kwargs):\n",
      "        if role_name == \"user\":\n",
      "            return \"\\n\\nHuman:\"\n",
      "        if role_name == \"assistant\":\n",
      "            return \"\\n\\nAssistant:\"\n",
      "        if role_name == \"system\":\n",
      "            return \"\"\n",
      "    \n",
      "    def get_role_end(self, role_name=None):\n",
      "        return \"\"\n",
      "    \n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/_model.py\n",
      "```py\n",
      "try:\n",
      "    from IPython.display import clear_output, display, HTML\n",
      "except ImportError:\n",
      "    clear_output = lambda wait=True: None\n",
      "    display = lambda arg: None\n",
      "    HTML = lambda arg: None\n",
      "try:\n",
      "    import torch\n",
      "except ImportError:\n",
      "    torch = None\n",
      "import html\n",
      "import re\n",
      "import copy\n",
      "import time\n",
      "import numpy as np\n",
      "import logging\n",
      "import base64\n",
      "import queue\n",
      "import threading\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "try:\n",
      "    from .. import cpp\n",
      "except ImportError:\n",
      "    logger.warn(\"Failed to load guidance.cpp, falling back to Python mirror implementations...\")\n",
      "    from .. import _cpp as cpp\n",
      "from .._utils import softmax, CaptureEvents\n",
      "from .._parser import EarleyCommitParser, Parser\n",
      "from .._grammar import GrammarFunction, string, _call_pool, _tag_pattern, Null, replace_model_variables, unreplace_model_variables, select\n",
      "from .. import _serialization_pb2\n",
      "\n",
      "# define some constants we will reuse many times\n",
      "_null_grammar = string('')\n",
      "format_pattern = re.compile(r\"<\\|\\|_.*?_\\|\\|>\", flags=re.DOTALL)\n",
      "nodisp_pattern = re.compile(r\"&lt;\\|\\|_#NODISP_\\|\\|&gt;.*?&lt;\\|\\|_/NODISP_\\|\\|&gt;\", flags=re.DOTALL)\n",
      "html_pattern = re.compile(r\"&lt;\\|\\|_html:(.*?)_\\|\\|&gt;\", flags=re.DOTALL)\n",
      "image_pattern = re.compile(r\"&lt;\\|_image:(.*?)\\|&gt;\")\n",
      "\n",
      "class Tokenizer:\n",
      "    '''This is the standardized tokenizer interface used by guidance models.\n",
      "    \n",
      "    This class should be subclassed by specific implementations and then used as the\n",
      "    tokenizer in the corresponding Engine subclass.\n",
      "    '''\n",
      "\n",
      "    def __init__(self, tokens, bos_token_id=None, eos_token_id=None):\n",
      "        \n",
      "        # a numpy array of token byte strings indexed by their token id\n",
      "        if isinstance(tokens, list):\n",
      "            self.tokens = np.array(tokens, dtype='object') # note that we need np.bytes_ to zero bytes are not treated as null terminations\n",
      "        \n",
      "        # a numpy array of token byte strings indexed by their token id\n",
      "        elif isinstance(tokens, np.array):\n",
      "            self.tokens = tokens\n",
      "\n",
      "        else:\n",
      "            raise Exception(\"Unknown tokenizer was passed!\")\n",
      "\n",
      "        assert isinstance(self.tokens[0], bytes), \"The tokens need to be provided as bytes!\"\n",
      "\n",
      "        self.bos_token_id = bos_token_id\n",
      "        self.bos_token = None if self.bos_token_id is None else self.tokens[self.bos_token_id]\n",
      "        self.eos_token_id = eos_token_id if eos_token_id is not None else bos_token_id\n",
      "        self.eos_token = None if self.eos_token_id is None else self.tokens[self.eos_token_id]\n",
      "        \n",
      "        # track which tokens are duplicates\n",
      "        self.duplicate_tokens = []\n",
      "        found = {}\n",
      "        for i,t in enumerate(self.tokens):\n",
      "            if t in found:\n",
      "                self.duplicate_tokens.append((i, found[t]))\n",
      "            else:\n",
      "                found[t] = i\n",
      "\n",
      "    def __call__(self, byte_string):\n",
      "        '''Returns a list of tokens that represent the given byte string.'''\n",
      "        raise NotImplementedError(\"You need to use a Tokenize subclass that overrides the __call__ method\")\n",
      "\n",
      "    def clean_duplicate_tokens(self, probs):\n",
      "        '''This moves all the probability mass from duplicate positons on to their primary index.'''\n",
      "        for i,j in self.duplicate_tokens:\n",
      "            probs[j] += probs[i]\n",
      "            probs[i] = 0\n",
      "\n",
      "class EngineCallResponse():\n",
      "    new_bytes: bytes\n",
      "    is_generated: bool\n",
      "    new_bytes_prob: float\n",
      "    capture_groups: dict\n",
      "    capture_group_log_probs: dict\n",
      "    new_token_count: int\n",
      "\n",
      "    def __init__(self, new_bytes, is_generated, new_bytes_prob, capture_groups, capture_group_log_probs, new_token_count):\n",
      "        self.new_bytes = new_bytes\n",
      "        self.is_generated = is_generated\n",
      "        self.new_bytes_prob = new_bytes_prob\n",
      "        self.capture_groups = capture_groups\n",
      "        self.capture_group_log_probs = capture_group_log_probs\n",
      "        self.new_token_count = new_token_count\n",
      "\n",
      "    def _to_proto(self):\n",
      "        \"\"\"Converts an EngineCallResponse object to its Protobuf representation.\n",
      "\n",
      "        Returns:\n",
      "            engine_response_pb2.EngineCallResponse: The Protobuf equivalent of this object.\n",
      "        \"\"\"\n",
      "\n",
      "        return _serialization_pb2.EngineCallResponse(\n",
      "            new_bytes=self.new_bytes,\n",
      "            is_generated=self.is_generated,\n",
      "            new_bytes_prob=self.new_bytes_prob,\n",
      "            capture_groups=self.capture_groups,\n",
      "            capture_group_log_probs=self.capture_group_log_probs,\n",
      "            new_token_count=self.new_token_count\n",
      "        )\n",
      "    \n",
      "    def encode(self, charset):\n",
      "        '''Used to support FastAPI encoding of EngineCallResponse objects.'''\n",
      "        return self.serialize()\n",
      "    \n",
      "    def serialize(self):\n",
      "        proto = self._to_proto()\n",
      "        return proto.SerializeToString()\n",
      "    \n",
      "    @staticmethod\n",
      "    def deserialize(byte_data):\n",
      "        proto = _serialization_pb2.EngineCallResponse()\n",
      "        proto.ParseFromString(byte_data)\n",
      "        return EngineCallResponse(\n",
      "            new_bytes=proto.new_bytes,\n",
      "            is_generated=proto.is_generated,\n",
      "            new_bytes_prob=proto.new_bytes_prob,\n",
      "            capture_groups=proto.capture_groups,\n",
      "            capture_group_log_probs=proto.capture_group_log_probs,\n",
      "            new_token_count=proto.new_token_count\n",
      "        )\n",
      "\n",
      "\n",
      "class Engine:\n",
      "    '''The engine owns the inference computation and is used/created by the Model class.\n",
      "    \n",
      "    Engine objects represent the expensive parts of inference. While Model objects are cheap and do not\n",
      "    need to know about the tokenizer or the model parameters, Engine objects know about both. Many\n",
      "    Model objects can reference a single Engine object. Engine objects can also be hidden behind a\n",
      "    Server so a single server can serve many clients' model objects through a single Engine object.\n",
      "    '''\n",
      "\n",
      "    def __init__(self, tokenizer, compute_log_probs=False):\n",
      "        self.tokenizer = tokenizer\n",
      "        self.compute_log_probs = compute_log_probs\n",
      "\n",
      "        # build a prefix tree of the tokens\n",
      "        self._token_trie = cpp.ByteTrie(self.tokenizer.tokens, np.arange(len(self.tokenizer.tokens)))\n",
      "        self._token_trie.match = True\n",
      "        self._token_trie.match_version = 0\n",
      "\n",
      "    def __call__(self, parser, grammar, ensure_bos_token=True):\n",
      "        '''Returns a new updated parser state executed through the grammar.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        parser : str or Parser\n",
      "            This is represents the current state of a guidance parser that will be extended\n",
      "            using the passed grammar. If a string is given then we assume the previous parser\n",
      "            state is just a fixed string prompt, if a full Parser is given then we extend that\n",
      "            parser by appending the new grammar to the parser's current grammar and then\n",
      "            inferencing the model. (TODO: implement full parser extension support)\n",
      "        grammar: Grammar\n",
      "            This is the grammar we are extending the parser with.\n",
      "        '''\n",
      "        # def __call__(self, grammar, max_tokens=1000000, n=1, top_p=1, temperature=0.0, ensure_bos_token=True):\n",
      "        # assert n == 1, \"Still need to add support for n > 1!\"\n",
      "\n",
      "        # note we only support a fixed set of engine variables for the sake of security\n",
      "        replacements = replace_model_variables(grammar, self, allowed_vars=[\"eos_token\", \"bos_token\"])\n",
      "\n",
      "        # right now we only support a text/bytes prompt parser state, so we extract that\n",
      "        if isinstance(parser, bytes):\n",
      "            prompt = parser\n",
      "        elif isinstance(parser, str):\n",
      "            prompt = bytes(parser, encoding=\"utf8\")\n",
      "        elif isinstance(parser, Parser):\n",
      "            raise NotImplementedError(\"Still need to implement support for extending a full Parser state.\")\n",
      "        else:\n",
      "            raise Exception(\"The passed parser is of an unknown type!\")\n",
      "\n",
      "        # add the beginning of sequence token if needed\n",
      "        if ensure_bos_token and self.tokenizer.bos_token is not None and not prompt.startswith(self.tokenizer.bos_token):\n",
      "            prompt = self.tokenizer.bos_token + prompt\n",
      "        \n",
      "        # run a simple tokenizer (that does not use a grammar) on the prefix for better performance\n",
      "        token_ids,token_byte_positions = self._tokenize_prefix(prompt)\n",
      "        token_ids,token_byte_positions = self._cleanup_tokens(token_ids, token_byte_positions)\n",
      "        if len(token_byte_positions) > 0:\n",
      "            pre_parser_bytes = token_byte_positions[-1]\n",
      "            trimmed_prompt_prefix = prompt[:token_byte_positions[-1]]\n",
      "            prompt = prompt[token_byte_positions[-1]:]\n",
      "        else:\n",
      "            trimmed_prompt_prefix = b''\n",
      "            pre_parser_bytes = 0\n",
      "        \n",
      "        # create a parser with a grammar that includes both our context and the passed grammar\n",
      "        parser = EarleyCommitParser(prompt + grammar)\n",
      "\n",
      "        # loop until we have generated a complete pattern\n",
      "        hidden_count = len(prompt) # we don't emit the prompt\n",
      "        generated_pos = 0 \n",
      "        sampled_token_ind = None\n",
      "        token_count = 0\n",
      "        last_token_count = 0\n",
      "        was_forced = False\n",
      "        captured_data = {}\n",
      "        captured_log_prob_data = {}\n",
      "        while True: # each iteration generates one more token (and some of the associated bytes)\n",
      "\n",
      "            if token_count >= 10:\n",
      "                pass\n",
      "\n",
      "            # note where we are starting for this token\n",
      "            start_pos = parser.pos\n",
      "\n",
      "            # let the parser know that we have advanced another token (used ofr tracking max token limits)\n",
      "            parser.mark_new_token()\n",
      "\n",
      "            # walk down the trie as far as possible before computing the logits\n",
      "            is_generated = False\n",
      "            retry_token_gen = False\n",
      "            trie = self._token_trie\n",
      "            trie.match_version += 1 # this invalidates all the match caches from the previous token\n",
      "            # trie.prob = 0.0 # need to reset when we reset the match_version\n",
      "            while True:\n",
      "                next_byte_mask = parser.next_byte_mask()\n",
      "                next_byte_mask_sum = next_byte_mask.sum()\n",
      "                \n",
      "                # see if we reached a dead end of the grammar\n",
      "                if next_byte_mask_sum == 0:\n",
      "                    break\n",
      "                \n",
      "                # if there is more than one option we cannot advance without computing the logits \n",
      "                elif next_byte_mask_sum != 1:\n",
      "                    break\n",
      "\n",
      "                # we are not forced if we are at the end of the grammar\n",
      "                elif parser.matched():\n",
      "                    break\n",
      "\n",
      "                # if there is only one possible next byte we can keep forcing\n",
      "                elif next_byte_mask_sum == 1:\n",
      "\n",
      "                    # look for valid children\n",
      "                    next_byte = None\n",
      "                    for byte in trie.keys():\n",
      "                        \n",
      "                        # mark this trie node with an up-to-date match flag (may save work later)\n",
      "                        node = trie.child(byte)\n",
      "                        node.match_version = self._token_trie.match_version\n",
      "                        # node.prob = 0.0 # reset when we reset the match_version\n",
      "                        node.match = next_byte_mask[byte[0]]\n",
      "                        \n",
      "                        # see if we found a match\n",
      "                        if node.match:\n",
      "                            next_byte = byte\n",
      "                            break\n",
      "\n",
      "                    # if we can't extend then this token is forced\n",
      "                    if next_byte is None:\n",
      "                        break\n",
      "                    \n",
      "                    # otherwise since there is only one possible next byte we keep going\n",
      "                    else:\n",
      "                        commit_point = parser.consume_byte(next_byte, log_prob=0.0)\n",
      "                        \n",
      "                        # if we are at a hidden commit point then we need to hide the bytes that match that node\n",
      "                        if commit_point is not None and commit_point.node.hidden:\n",
      "\n",
      "                            # This takes the item and commits to it as part of the parse and then shrinks it to zero width\n",
      "                            # in other words this hides the item\n",
      "                            parser.commit_and_collapse_item(commit_point)\n",
      "                            \n",
      "                            # keep the bytes we still need to emit\n",
      "                            if start_pos < commit_point.start:\n",
      "                                parser.shadow_rewind(start_pos)\n",
      "                            \n",
      "                            else:\n",
      "                                # pop off any tokens that overlap the hidden bytes\n",
      "                                i = len(token_byte_positions) - 1\n",
      "                                while i >= 0 and token_byte_positions[i] - pre_parser_bytes > commit_point.start:\n",
      "                                    token_ids.pop()\n",
      "                                    token_byte_positions.pop()\n",
      "                                    token_count -= 1\n",
      "                                    i -= 1\n",
      "                                # re-add any bytes we cut too far on\n",
      "                                parser.shadow_rewind(token_byte_positions[-1] - pre_parser_bytes)\n",
      "                            retry_token_gen = True # this restarts us at the top of the outer token gen loop\n",
      "                            break\n",
      "                        \n",
      "                        trie = trie.child(next_byte)\n",
      "                \n",
      "            forced_pos = parser.pos # record how far the bytes are forced\n",
      "\n",
      "            if retry_token_gen:\n",
      "                continue\n",
      "\n",
      "            # back up if we got forced up to a point that is not a valid token\n",
      "            if next_byte_mask_sum <= 1:\n",
      "                while trie.value < 0 and trie.parent() is not None:\n",
      "                    trie = trie.parent()\n",
      "                    forced_pos -= 1\n",
      "                parser.pos = forced_pos\n",
      "            \n",
      "            # if we walked all the way to a forced token then we advance without computing the logits\n",
      "            # we are forced if there are no more options and we are either in the middle of the grammar or at a trie leaf\n",
      "            is_forced = next_byte_mask_sum <= 1 and (len(trie) == 0 if parser.matched() else trie != self._token_trie)\n",
      "            token_pos = 0\n",
      "            if is_forced:\n",
      "                sampled_token_ind = trie.value\n",
      "                sampled_token = self.tokenizer.tokens[sampled_token_ind]\n",
      "                new_bytes_prob = 1.0\n",
      "                was_forced = True\n",
      "\n",
      "            # we are at the end of the grammar\n",
      "            elif next_byte_mask_sum == 0:\n",
      "\n",
      "                # mark the token we \"sampled\" if we have comsumed some bytes\n",
      "                if trie != self._token_trie:\n",
      "                    sampled_token_ind = trie.value\n",
      "                    sampled_token = self.tokenizer.tokens[sampled_token_ind]\n",
      "                    new_bytes_prob = 1.0\n",
      "                    \n",
      "            # otherwise we need to compute the logits and sample a valid token\n",
      "            else:\n",
      "\n",
      "                # if we were forced we might need to clean up the greedy tokenization to match the global tokenization behavior as seen in training\n",
      "                if was_forced:\n",
      "                    token_ids,token_byte_positions = self._cleanup_tokens(token_ids, token_byte_positions)\n",
      "                    was_forced = False\n",
      "                grammar_temp = parser.next_byte_temperature()\n",
      "                current_temp = grammar_temp if grammar_temp >= 0 else 0\n",
      "                logits = self.get_logits(token_ids, parser.bytes[start_pos:forced_pos], current_temp)\n",
      "                is_generated = True\n",
      "\n",
      "                # if requested we compute the log probabilities so we can track the probabilities of each node\n",
      "                if self.compute_log_probs:\n",
      "                    if torch:\n",
      "                        probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).cpu().numpy() # note we don't adjust for temp since we consider that a sampling step, not part of the probs\n",
      "                    else:\n",
      "                        probs = softmax(logits, axis=-1) # this numpy code is slower, so we don't use it if we have torch...\n",
      "                    self.tokenizer.clean_duplicate_tokens(probs)\n",
      "                    trie.compute_probs(probs) # C++ impl\n",
      "                else:\n",
      "                    probs = None\n",
      "\n",
      "                # get the sampling order\n",
      "                if current_temp == 0:\n",
      "                    sampling_order = np.argsort(-logits) # we need numpy so the enumerate below does not get really slow...\n",
      "                else:\n",
      "                    # assert top_p == 1, \"Still need to add support for top_p!\"\n",
      "                    if torch:\n",
      "                        logits = torch.tensor(logits)\n",
      "                        torch.div(logits, current_temp, out=logits)\n",
      "                        probs_torch = torch.nn.functional.softmax(logits, dim=-1)\n",
      "                        sampling_order = torch.multinomial(probs_torch, len(probs_torch)).cpu().numpy()\n",
      "                    else:\n",
      "                        # this numpy version allows us to drop our dependence on pytorch...but it is way slower\n",
      "                        if probs is None:\n",
      "                            probs = softmax(logits / current_temp, axis=-1)\n",
      "                        probs += 1e-10 # ensure we have no zero probs that mess up numpy\n",
      "                        probs /= np.sum(probs)\n",
      "                        sampling_order = np.random.choice(len(probs), size=len(probs), p=probs, replace=False) # the 1e-10 is ensure we have no zero probs, which numpy does not like\n",
      "\n",
      "                # loop over the tokens looking for a valid one\n",
      "                for i,sampled_token_ind in enumerate(sampling_order):\n",
      "                    sampled_token = self.tokenizer.tokens[sampled_token_ind]\n",
      "\n",
      "                    # break out if we have reach impossible tokens\n",
      "                    if logits[sampled_token_ind] <= -np.inf:\n",
      "                        break\n",
      "\n",
      "                    # make sure it matches any forced prefix\n",
      "                    used_forced_pos = min(forced_pos, start_pos+len(sampled_token))\n",
      "                    if start_pos < forced_pos and not sampled_token.startswith(parser.bytes[start_pos:used_forced_pos]):\n",
      "                        continue\n",
      "                    offset = used_forced_pos - start_pos\n",
      "\n",
      "                    # make sure the parse is backed up to the position we want to start checking from TODO: make this account for shared prefixes with the last token\n",
      "                    parser.pos = used_forced_pos\n",
      "                    new_bytes_prob = 1.0\n",
      "\n",
      "                    # if we have gotten to the end of the valid tokens then we stop\n",
      "                    # if logits[sampled_token_ind] == -np.inf:\n",
      "                    #     raise self._report_failed_match(trimmed_prompt_prefix + parser.bytes)\n",
      "\n",
      "                    # check to see if the sampled token is allowed\n",
      "                    token_pos = offset\n",
      "                    node = trie # this is the Trie node we were left at when we could force the next byte above\n",
      "                    while token_pos < len(sampled_token):\n",
      "                        next_byte = sampled_token[token_pos:token_pos+1]\n",
      "                        next_node = node.child(next_byte)\n",
      "\n",
      "                        # if we don't have a cached match flag compute it using the grammar\n",
      "                        if next_node.match_version < self._token_trie.match_version:\n",
      "                            next_byte_mask = parser.next_byte_mask()\n",
      "                            for byte in node.keys(): # we update all the children since the parser knows the full mask\n",
      "                                child = node.child(byte)\n",
      "                                child.match_version = self._token_trie.match_version\n",
      "                                child.match = next_byte_mask[byte[0]]\n",
      "                        \n",
      "                        # advance or fail according to the (now up-to-date) match cache\n",
      "                        if next_node.match:\n",
      "\n",
      "                            # get the parser to consume the next byte\n",
      "                            if next_node.prob < 1e-8:\n",
      "                                if node.prob < 1e-8:\n",
      "                                    log_prob_delta = 0\n",
      "                                else:\n",
      "                                    log_prob_delta = -20\n",
      "                            else:\n",
      "                                log_prob_delta = np.log(next_node.prob) - np.log(node.prob)\n",
      "                            # log_prob_delta = np.log(next_node.prob) - np.log(node.prob)\n",
      "                            new_bytes_prob = next_node.prob\n",
      "                            commit_point = parser.consume_byte(next_byte, log_prob=log_prob_delta)\n",
      "\n",
      "                            # mark that we accepted this byte\n",
      "                            node = next_node\n",
      "                            token_pos += 1\n",
      "                        \n",
      "                            # if we are at a hidden commit point then we need to hide the bytes that match that node\n",
      "                            if commit_point is not None and commit_point.node.hidden:\n",
      "\n",
      "                                # if we are capturing the data from this node we need to do that now since we are about to remove it\n",
      "                                # TODO: build a whole parse tree under this commit_point node so we can record child node captures\n",
      "                                if commit_point.node.capture_name:\n",
      "                                    captured_data[commit_point.node.capture_name] = parser.bytes[commit_point.start:]\n",
      "                                    captured_log_prob_data[commit_point.node.capture_name] = commit_point.log_prob\n",
      "\n",
      "                                # This takes the item and commits to it as part of the parse and then shrinks it to zero width\n",
      "                                # in other words this hides the item\n",
      "                                parser.commit_and_collapse_item(commit_point)\n",
      "                                \n",
      "                                # keep the bytes we still need to emit\n",
      "                                if forced_pos < commit_point.start:\n",
      "                                    parser.shadow_rewind(forced_pos)\n",
      "                                \n",
      "                                else:\n",
      "                                    # pop off any tokens that overlap the hidden bytes\n",
      "                                    i = len(token_byte_positions) - 1\n",
      "                                    while i >= 0 and token_byte_positions[i] - pre_parser_bytes > commit_point.start:\n",
      "                                        token_ids.pop()\n",
      "                                        token_byte_positions.pop()\n",
      "                                        token_count -= 1\n",
      "                                        i -= 1\n",
      "                                    # re-add any bytes we cut too far on\n",
      "                                    parser.shadow_rewind(token_byte_positions[-1] - pre_parser_bytes)\n",
      "                                retry_token_gen = True # this restarts us at the top of the outer token gen loop\n",
      "                                break\n",
      "\n",
      "                            elif token_pos == len(sampled_token):\n",
      "                                break # this token is valid\n",
      "                        else:\n",
      "                            # partially valid tokens are okay if we are running off the end of a grammar, but not otherwise\n",
      "                            if not parser.matched():\n",
      "                                token_pos = -1\n",
      "\n",
      "                            break # this token is no longer valid\n",
      "\n",
      "                    # see if we are breaking out of the whole loop\n",
      "                    if retry_token_gen:\n",
      "                        break\n",
      "\n",
      "                    # check if this token is dominated by other longer valid tokens (and hence would never be consistent with greedy tokenization)\n",
      "                    # TODO: disabled for now because of sentencepeice non-local issues\n",
      "                    # if token_pos == len(sampled_token) and not parser.matched(): # not we don't check if we have matched, because then we can generate anything afterwards\n",
      "                    #     if _check_dominated(node, parser, self._token_trie.match_version, parser.next_byte_mask()):\n",
      "                    #         token_pos = -1\n",
      "\n",
      "                    if token_pos > 0:\n",
      "                        break # we found a valid token\n",
      "\n",
      "                    if parser.matched():\n",
      "                        break # if we already have a full match we don't try more tokens we just give up as soon as the model deviates from the grammar\n",
      "            \n",
      "            # if we just collapased a hidden commit point then we start over looking for a new token\n",
      "            if retry_token_gen:\n",
      "                continue\n",
      "\n",
      "            # emit whatever we know will not be hidden\n",
      "            new_bytes = parser.bytes[generated_pos:parser.earliest_hidden_start()]\n",
      "\n",
      "            # if we cannot consume any more tokens then we are done\n",
      "            if not is_forced and token_pos < len(sampled_token) and trie == self._token_trie:\n",
      "\n",
      "                # which if can't consume any more tokens, but we are not yet done\n",
      "                if not parser.matched():\n",
      "                    parser.matched()\n",
      "                    raise self._report_failed_match(trimmed_prompt_prefix + parser.bytes)\n",
      "                \n",
      "                # TODO: if we exactly match the end of the pattern then we can commit to this last token \n",
      "                # if m.span()[1] == len(generated_text):\n",
      "                #     self._cache_state[\"new_token_ids\"].append(sampled_token_ind)\n",
      "\n",
      "                # capture the named groups from the parse tree\n",
      "                parser.get_captures(captured_data, captured_log_prob_data)\n",
      "\n",
      "                # we have no valid log prob data if we didn't compute it\n",
      "                # yield new_bytes[hidden_count:], is_generated, new_bytes_prob, captured_data, captured_log_prob_data, token_count - last_token_count\n",
      "                yield EngineCallResponse(\n",
      "                    new_bytes=new_bytes[hidden_count:],\n",
      "                    is_generated=is_generated,\n",
      "                    new_bytes_prob=new_bytes_prob if self.compute_log_probs else 1.0,\n",
      "                    capture_groups=captured_data,\n",
      "                    capture_group_log_probs=captured_log_prob_data,\n",
      "                    new_token_count=token_count - last_token_count\n",
      "                )\n",
      "                last_token_count = token_count\n",
      "                break # we are done!\n",
      "            else:\n",
      "                generated_pos += len(new_bytes)\n",
      "\n",
      "                # yeild the snippet of text created by the next token\n",
      "                out = new_bytes[hidden_count:]\n",
      "                if len(out) > 0:\n",
      "                    # capture the named groups from the (partial) parse tree, # TODO: disabled for now until we handle list_append correctly\n",
      "                    # new_captured_data, new_captured_log_prob_data = parser.get_captures()\n",
      "                    # captured_data.update(new_captured_data)\n",
      "                    # captured_log_prob_data.update(new_captured_log_prob_data)\n",
      "                    #yield out, is_generated, new_bytes_prob, captured_data, captured_log_prob_data, token_count - last_token_count # note that we don't capture groups until a complete parse right now...\n",
      "                    yield EngineCallResponse(\n",
      "                        new_bytes=out,\n",
      "                        is_generated=is_generated,\n",
      "                        new_bytes_prob=new_bytes_prob if self.compute_log_probs else 1.0,\n",
      "                        capture_groups=captured_data,\n",
      "                        capture_group_log_probs=captured_log_prob_data,\n",
      "                        new_token_count=token_count - last_token_count\n",
      "                    )\n",
      "\n",
      "                    last_token_count = token_count\n",
      "                    hidden_count = 0\n",
      "                    token_count += 1 # note we only update this for tokens that emit non-hidden content\n",
      "                else:\n",
      "                    hidden_count -= len(new_bytes)\n",
      "\n",
      "                token_ids.append(sampled_token_ind)\n",
      "\n",
      "                # track the byte position of each token\n",
      "                if len(token_byte_positions) == 0:\n",
      "                    token_byte_positions.append(len(sampled_token))\n",
      "                else:\n",
      "                    token_byte_positions.append(token_byte_positions[-1] + len(sampled_token))\n",
      "\n",
      "        # TODO: we only need to do this when we might re-use the grammar object...we might want to account for that\n",
      "        unreplace_model_variables(replacements)\n",
      "    \n",
      "    def _tokenize_prefix(self, byte_string):\n",
      "        '''This is used to speed up the tokenization of long prompts without using the parser.'''\n",
      "        token_ids = []\n",
      "        token_byte_positions = []\n",
      "        \n",
      "        # loop trying to decode a new token at each iteration\n",
      "        pos = 0\n",
      "        while True:\n",
      "\n",
      "            # walk down the token trie looking for a unique token match\n",
      "            trie = self._token_trie\n",
      "            valid_pos = -1\n",
      "            valid_value = -1\n",
      "            while True:\n",
      "                if pos >= len(byte_string):\n",
      "                    if len(trie) > 0:\n",
      "                        valid_pos = -1\n",
      "                    break\n",
      "\n",
      "                # check if we can keep going or are at a dead end\n",
      "                if trie.has_child(byte_string[pos:pos+1]):\n",
      "                    trie = trie.child(byte_string[pos:pos+1])\n",
      "                    pos += 1\n",
      "\n",
      "                    # record the last valid token down this path as we go\n",
      "                    if trie.value >= 0:\n",
      "                        valid_pos = pos\n",
      "                        valid_value = trie.value\n",
      "                else:\n",
      "                    break # we can't go any farther\n",
      "            \n",
      "            if valid_pos == -1:\n",
      "                break\n",
      "            else:\n",
      "                token_ids.append(valid_value)\n",
      "                token_byte_positions.append(valid_pos)\n",
      "                pos = valid_pos\n",
      "\n",
      "        return token_ids,token_byte_positions\n",
      "    \n",
      "    def _cleanup_tokens(self, token_ids, token_byte_positions):\n",
      "\n",
      "        # compute a joint tokenization\n",
      "        joint_token_ids = self._joint_tokenize(token_ids)\n",
      "        \n",
      "        # see if we need to redo the tokenization\n",
      "        redo = False\n",
      "        if len(joint_token_ids) != len(token_ids):\n",
      "            redo = True\n",
      "        else:\n",
      "            for i,id in enumerate(joint_token_ids):\n",
      "                if token_ids[i] != id:\n",
      "                    redo = True\n",
      "                    break\n",
      "        \n",
      "        if redo:\n",
      "            token_ids = joint_token_ids\n",
      "            last_pos = token_byte_positions[-1]\n",
      "            token_byte_positions = []\n",
      "            pos = 0\n",
      "            for i,id in enumerate(joint_token_ids):\n",
      "                pos += len(self.tokenizer.tokens[id])\n",
      "                token_byte_positions.append(pos)\n",
      "            \n",
      "            # ugly hack to deal with sentence peice craziness of space hiding after special tokens TODO: figure out how to make this more robust\n",
      "            if token_byte_positions[-1] == last_pos + 1 and self.tokenizer.tokens[token_ids[0]] == b'<s>' and self.tokenizer.tokens[token_ids[1]][0:1] == b' ':\n",
      "                for i in range(1, len(token_byte_positions)):\n",
      "                    token_byte_positions[i] -= 1\n",
      "            assert token_byte_positions[-1] == last_pos\n",
      "        \n",
      "        return token_ids, token_byte_positions\n",
      "    \n",
      "    def get_logits(self, token_ids, forced_bytes, current_temp):\n",
      "        '''A fake method designed to be overriden by subclasses.'''\n",
      "\n",
      "        # pretend to extend the KV cache and update the log probs\n",
      "        return np.randn(len(self.tokenizer.tokens))\n",
      "\n",
      "    def _report_failed_match(self, prompt):\n",
      "        \"\"\"Note that this can be overridden by subclasses that have more likely reasons than a bug in the token set (like remote models).\"\"\"\n",
      "        return Exception(\"We can't consume any more tokens, but we are not yet done! Perhaps your model's token set is incomplete? This happened after the prompt:\" + str(prompt[-40:]))\n",
      "\n",
      "    def _joint_tokenize(self, token_ids):\n",
      "        '''What a full joint tokenizer would give for a given byte string'''\n",
      "        return token_ids\n",
      "\n",
      "class Model:\n",
      "    '''The base guidance model object, which represents a model in a given state.\n",
      "    \n",
      "    Model objects are immutable representations of model state, so whenever you change\n",
      "    them you get a new Model object. However, these copies share the \"expensive\"\n",
      "    parts of the underlying model like the the parameters and KV-cache, through a shared\n",
      "    Engine, so making copies of Model objects is cheap.\n",
      "\n",
      "    .. automethod:: __add__\n",
      "    '''\n",
      "\n",
      "    open_blocks = {} # track what context blocks are open\n",
      "    _grammar_only = 0 # a flag that tracks when we are forced to be executing only compiled grammars (like when we are inside a select)\n",
      "    _throttle_refresh = 0 # a flag that tracks when we can throttle our display since we know future display calls are going to happen\n",
      "\n",
      "    def __init__(self, engine, echo=True, **kwargs):\n",
      "        '''Build a new model object that represents a model in a given state.\n",
      "\n",
      "        Note that this constructor is not meant to be used directly, since there\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        engine : Engine\n",
      "            The inference engine to use for this model.\n",
      "        echo : bool\n",
      "            If true the final result of creating this model state will be displayed (as HTML in a notebook).\n",
      "        '''\n",
      "        if isinstance(engine, str) and engine.startswith(\"http\"):\n",
      "            from ._remote import RemoteEngine\n",
      "\n",
      "            engine = RemoteEngine(engine, **kwargs)\n",
      "\n",
      "        # # auto-wrap the tokenizer in the standard guidance interface\n",
      "        # if not isinstance(tokenizer, Tokenizer):\n",
      "        #     tokenizer = Tokenizer(tokenizer)\n",
      "        \n",
      "        self.engine = engine\n",
      "        self.echo = echo\n",
      "        self.token_count = 0 # tracks how many tokens our byte state represents\n",
      "        self.max_display_rate = 0.2 # this controls how frequently we are allowed to redraw the display (in seconds)\n",
      "        self.opened_blocks = {} # what context blocks have been opened but not closed\n",
      "        # self.compute_log_probs = compute_log_probs\n",
      "\n",
      "        # private attributes\n",
      "        self._variables = {} # these are the state variables stored with the model\n",
      "        self._variables_log_probs = {} # these are the state variables stored with the model\n",
      "        self._cache_state = {} # mutable caching state used to save computation        \n",
      "        self._state = \"\" # the current bytes that represent the state of the model\n",
      "        self._event_queue = None # TODO: these are for streaming results in code, but that needs implemented\n",
      "        self._event_parent = None\n",
      "        self._last_display = 0 # used to track the last display call to enable throttling\n",
      "        self._last_event_stream = 0 # used to track the last event streaming call to enable throttling\n",
      "\n",
      "    @property\n",
      "    def active_role_end(self):\n",
      "        '''The default end patterns we should use for `gen` calls.\n",
      "        TODO: move this logic into the gen call...we can do with if we allow model_variables to run functions.\n",
      "        \n",
      "        These patterns are computed dynamically by the model object because they can depend on\n",
      "        what the current open roles are, which is something \n",
      "        '''\n",
      "\n",
      "        # add any active non-empty role ends. Ignore role ends that are spaces\n",
      "        parts = []\n",
      "        for _, role_end_str in self.opened_blocks.values():\n",
      "            role_end_str = format_pattern.sub(\"\", role_end_str)\n",
      "            if len(role_end_str) > 0 and not re.fullmatch(r'\\s+', role_end_str):\n",
      "                parts.append(role_end_str)\n",
      "\n",
      "        return select(parts)\n",
      "\n",
      "    def _html(self):\n",
      "        '''Generate HTML that displays the model object.'''\n",
      "        display_out = self._state\n",
      "        for context in reversed(self.opened_blocks):\n",
      "            display_out += self.opened_blocks[context][1]\n",
      "        display_out = html.escape(display_out)\n",
      "        display_out = nodisp_pattern.sub(\"\", display_out)\n",
      "        display_out = html_pattern.sub(lambda x: html.unescape(x.group(1)), display_out)\n",
      "        display_out = image_pattern.sub(lambda x: '<img src=\"data:image/png;base64,' + base64.b64encode(self[x.groups(1)[0]]).decode() + '\" style=\"max-width: 400px; vertical-align: middle; margin: 4px;\">', display_out)\n",
      "        display_out = \"<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>\"+display_out+\"</pre>\"\n",
      "        return display_out\n",
      "    \n",
      "    def _send_to_event_queue(self, value):\n",
      "        '''For streaming in code.\n",
      "        \n",
      "        TODO: Is this still needed?'''\n",
      "        if self._event_queue is not None:\n",
      "            self._event_queue.put(value)\n",
      "        if self._event_parent is not None:\n",
      "            self._event_parent._send_to_event_queue(value)\n",
      "\n",
      "    def stream(self):\n",
      "        return ModelStream(self)\n",
      "    \n",
      "    def copy(self):\n",
      "        '''Create a shallow copy of the model object.'''\n",
      "        \n",
      "        # start with a shallow copy\n",
      "        new_lm = copy.copy(self)\n",
      "\n",
      "        # then copy a few things we need deeper copies of\n",
      "        new_lm._variables = self._variables.copy()\n",
      "        new_lm._variables_log_probs = self._variables_log_probs.copy()\n",
      "        new_lm.opened_blocks = self.opened_blocks.copy()\n",
      "        \n",
      "        # create a new clean event queue\n",
      "        new_lm._event_queue = None # we start with no event queue because nobody is listening to us yet\n",
      "        if self._event_queue is not None:\n",
      "            new_lm._event_parent = self # the current lm has an event que we make it our parent\n",
      "        elif self._event_parent is not None:\n",
      "            new_lm._event_parent = self._event_parent # otherwise if the current event que has an event parent then that is also our parent\n",
      "        \n",
      "        return new_lm\n",
      "    \n",
      "    def _inplace_append(self, value, force_silent=False):\n",
      "        '''This is the base way to add content to the current LM object that is being constructed.\n",
      "        \n",
      "        All updates to the model state should eventually use this function.\n",
      "        Note this should only be used after making a copy, otherwise immutability would be violated.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        value : bytes\n",
      "            The bytes we should append to our current state.\n",
      "        '''\n",
      "\n",
      "        # update the byte state\n",
      "        self._state += str(value) # TODO: make _state to be bytes not a string\n",
      "\n",
      "        # see if we should update the display\n",
      "        if not force_silent:\n",
      "            self._update_display()\n",
      "        \n",
      "        # this is for programmatic streaming among other things\n",
      "        if Model._throttle_refresh > 0:\n",
      "            curr_time = time.time()\n",
      "            if curr_time - self._last_event_stream >= self.max_display_rate:\n",
      "                self._last_event_stream = curr_time\n",
      "                self._send_to_event_queue(self)\n",
      "        else:\n",
      "            self._send_to_event_queue(self)\n",
      "                \n",
      "\n",
      "    def _update_display(self, throttle=True):\n",
      "        if self.echo:\n",
      "            if Model._throttle_refresh > 0:\n",
      "                curr_time = time.time()\n",
      "                if throttle and curr_time - self._last_display < self.max_display_rate:\n",
      "                    return # we are throttling the update\n",
      "                else:\n",
      "                    self._last_display = curr_time\n",
      "        \n",
      "            clear_output(wait=True)\n",
      "            display(HTML(self._html()))\n",
      "    \n",
      "    def reset(self, clear_variables=True):\n",
      "        '''This resets the state of the model object.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        clear_variables : bool\n",
      "            If we should clear all the model object's variables in addition to reseting the byte state.\n",
      "        '''\n",
      "        self._state = self._state[:0]\n",
      "        if clear_variables:\n",
      "            self._variables = {}\n",
      "            self._variables_log_probs = {}\n",
      "        return self\n",
      "\n",
      "    def _repr_html_(self):\n",
      "        clear_output(wait=True)\n",
      "        return self._html()\n",
      "    \n",
      "    def _current_prompt(self):\n",
      "        '''The current prompt in bytes (which is the state without the context close tags).'''\n",
      "        return format_pattern.sub(\"\", self._state)\n",
      "    \n",
      "    def __str__(self):\n",
      "        '''A string representation of the current model object (that includes context closers).'''\n",
      "        out = self._current_prompt()\n",
      "        for context in reversed(self.opened_blocks):\n",
      "            out += format_pattern.sub(\"\", self.opened_blocks[context][1])\n",
      "        return out\n",
      "    \n",
      "    def __add__(self, value):\n",
      "        '''Adding is the primary mechanism for extending model state.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        value : guidance grammar\n",
      "            The grammar used to extend the current model.\n",
      "        '''\n",
      "\n",
      "        # create the new lm object we will return\n",
      "        # (we need to do this since Model objects are immutable)\n",
      "        lm = self.copy()\n",
      "\n",
      "        # inside this context we are free to drop display calls that come too close together\n",
      "        with throttle_refresh():\n",
      "\n",
      "            # find what new blocks need to be applied\n",
      "            new_blocks = []\n",
      "            for context in Model.open_blocks:\n",
      "                if context not in lm.opened_blocks:\n",
      "                    new_blocks.append(context)\n",
      "\n",
      "                    # mark this so we don't re-add when computing the opener or closer (even though we don't know the close text yet)\n",
      "                    lm.opened_blocks[context] = (0, \"\")\n",
      "\n",
      "            # find what old blocks need to be removed\n",
      "            old_blocks = []\n",
      "            for context in list(reversed(lm.opened_blocks)):\n",
      "                if context not in Model.open_blocks and context in lm.opened_blocks:\n",
      "                    old_blocks.append((lm.opened_blocks[context], context))\n",
      "\n",
      "                    # delete this so we don't re-close when computing the opener or closer\n",
      "                    del lm.opened_blocks[context]\n",
      "\n",
      "            # close any newly closed contexts\n",
      "            for (pos, close_text), context in old_blocks:\n",
      "                if context.name is not None:\n",
      "                    lm._variables[context.name] = format_pattern.sub(\"\", lm._state[pos:])\n",
      "                lm += context.closer\n",
      "\n",
      "            # apply any newly opened contexts (new from this object's perspective)\n",
      "            for context in new_blocks:\n",
      "                lm += context.opener\n",
      "                with grammar_only():\n",
      "                    tmp = lm + context.closer\n",
      "                close_text = tmp._state[len(lm._state):] # get the new state added by calling the closer\n",
      "                lm.opened_blocks[context] = (len(lm._state), close_text)\n",
      "                \n",
      "                # clear out names that we override\n",
      "                if context.name is not None:\n",
      "                    if context.name in lm._variables:\n",
      "                        del lm._variables[context.name]\n",
      "                        if context.name in lm._variables_log_probs:\n",
      "                            del lm._variables_log_probs[context.name]\n",
      "            \n",
      "            # wrap raw string values\n",
      "            if isinstance(value, str):\n",
      "                is_id = False\n",
      "                parts = re.split(_tag_pattern, value)\n",
      "                \n",
      "                # we have no embedded objects\n",
      "                if len(parts) == 1:\n",
      "                    lm._inplace_append(value)\n",
      "                    out = lm\n",
      "                \n",
      "                # if we have embedded objects we have to convert the string to a grammar tree\n",
      "                else:\n",
      "                    partial_grammar = _null_grammar\n",
      "                    lm.suffix = \"\"\n",
      "                    for i,part in enumerate(parts):\n",
      "                        if i < len(parts) - 1:\n",
      "                            lm.suffix = parts[i+1]\n",
      "                        if is_id:\n",
      "                            call = _call_pool[part]\n",
      "                            if isinstance(call, GrammarFunction):\n",
      "                                partial_grammar += _call_pool[part]\n",
      "                            else:\n",
      "                                lm += partial_grammar\n",
      "                                lm = _call_pool[part](lm)\n",
      "                                partial_grammar = _null_grammar\n",
      "                        elif part != \"\":\n",
      "                            partial_grammar += string(part)\n",
      "                        is_id = not is_id\n",
      "                    out = lm + partial_grammar\n",
      "            \n",
      "            # if we find a null value we do nothing\n",
      "            elif isinstance(value, Null):\n",
      "                out = lm\n",
      "            \n",
      "            # run stateless functions (grammar nodes)\n",
      "            elif isinstance(value, GrammarFunction):\n",
      "                out = lm._run_stateless(value)\n",
      "            \n",
      "            # run stateful functions\n",
      "            else:\n",
      "                out = value(lm)\n",
      "                if out is None:\n",
      "                    raise Exception(f\"A guidance function returned `None`, not a model object! Did you forget to return the new lm at the end of your function?\")\n",
      "                if not isinstance(out, Model):\n",
      "                    raise Exception(f\"A guidance function did not return a model object! Did you try to add a function to a model without calling the function? For example `model + guidance_function()` is correct, while `model + guidance_function` will cause this error.\")\n",
      "        \n",
      "        # this flushes the display\n",
      "        out._inplace_append(\"\")\n",
      "\n",
      "        return out\n",
      "    \n",
      "    # def endswith(self, s):\n",
      "    #     '''Checks if the current model state ends with the given value.'''\n",
      "    #     return self._current_prompt().endswith(s)\n",
      "    \n",
      "    def __len__(self):\n",
      "        '''The string length of the current state.\n",
      "        \n",
      "        TODO: This should change to the byte length...\n",
      "        '''\n",
      "        return len(str(self))\n",
      "    \n",
      "    def __setitem__(self, key, value):\n",
      "        raise Exception(\"Model objects are immutable so you can't use __setitem__! Consider using the .set(key, value) method instead to create a new updated model object.\")\n",
      "\n",
      "    def __getitem__(self, key):\n",
      "        if key in self._variables:\n",
      "            return self._variables[key]\n",
      "        \n",
      "        # look for named blocks that are still open with the given key as their name\n",
      "        else:\n",
      "            for context in list(reversed(self.opened_blocks)):\n",
      "                if context.name == key:\n",
      "                    return format_pattern.sub(\"\", self._state[self.opened_blocks[context][0]:])\n",
      "                \n",
      "        raise KeyError(f\"Model does not contain the variable '{key}'\")\n",
      "    \n",
      "    def __contains__(self, item):\n",
      "        return item in self._variables\n",
      "    \n",
      "    def get(self, key, default=None):\n",
      "        '''Return the value of a variable, or a default value if the variable is not present.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        key : str\n",
      "            The name of the variable.\n",
      "        default : any\n",
      "            The value to return if the variable is not current set.\n",
      "        '''\n",
      "        return self._variables.get(key, default)\n",
      "    \n",
      "    def setattr(self, key, value):\n",
      "        '''Return a new model with the given model attribute set.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        key : str\n",
      "            The name of the attribute to be set.\n",
      "        value : any\n",
      "            The value to set the attribute to.\n",
      "        '''\n",
      "        copy = self.copy()\n",
      "        setattr(copy, key, value)\n",
      "        return copy\n",
      "    \n",
      "    def delattr(self, key):\n",
      "        '''Return a new model with the given attribute deleted.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        key : str\n",
      "            The attribute name to remove.\n",
      "        '''\n",
      "        copy = self.copy()\n",
      "        delattr(copy, key)\n",
      "        return copy\n",
      "\n",
      "    def set(self, key, value):\n",
      "        '''Return a new model with the given variable value set.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        key : str\n",
      "            The name of the variable to be set.\n",
      "        value : any\n",
      "            The value to set the variable to.\n",
      "        '''\n",
      "        copy = self.copy()\n",
      "        copy._variables[key] = value\n",
      "        return copy\n",
      "    \n",
      "    def remove(self, key):\n",
      "        '''Return a new model with the given variable deleted.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        key : str\n",
      "            The variable name to remove.\n",
      "        '''\n",
      "        if key in self._variables:\n",
      "            copy = self.copy()\n",
      "            del copy._variables[key]\n",
      "            if key in copy._variables_log_probs:\n",
      "                del copy._variables_log_probs[key]\n",
      "        else:\n",
      "            copy = self\n",
      "        return copy\n",
      "    \n",
      "    def log_prob(self, key, default=None):\n",
      "        '''Return the log prob of a variable, or a default value if the variable is not present.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        key : str\n",
      "            The name of the variable.\n",
      "        default : any\n",
      "            The value to return if the variable is not current set.\n",
      "        '''\n",
      "        # TODO: support calling without a key to get the log prob of the whole model\n",
      "        return self._variables_log_probs.get(key, default)\n",
      "    \n",
      "    # def get_cache(self):\n",
      "    #     return self.engine.cache\n",
      "    \n",
      "#     def tool_def(self, functions):\n",
      "\n",
      "#         self += \"\"\"\n",
      "# # Tools\n",
      "\n",
      "# \"\"\"\n",
      "#         if len(functions) > 0:\n",
      "#             self += '''## functions\n",
      "\n",
      "# namespace functions {\n",
      "\n",
      "# '''\n",
      "#         for function in functions:\n",
      "#             self += f\"\"\"// {function['description']}\n",
      "# type {function['name']} = (_: {{\"\"\"\n",
      "#             for prop_name,prop_data in function[\"parameters\"][\"properties\"].items():\n",
      "#                 if \"description\" in prop_data:\n",
      "#                     self += f\"\\n// {prop_data['description']}\\n\"\n",
      "#                 self += prop_name\n",
      "#                 if prop_name not in function[\"parameters\"][\"required\"]:\n",
      "#                     self += \"?\"\n",
      "#                 self += \": \"\n",
      "#                 if \"enum\" in prop_data:\n",
      "#                     for enum in prop_data[\"enum\"]:\n",
      "#                         self += f'\"{enum}\"'\n",
      "#                         if enum != prop_data[\"enum\"][-1]:\n",
      "#                             self += \" | \"\n",
      "#                 else:\n",
      "#                     self += prop_data[\"type\"]\n",
      "                \n",
      "#                 if prop_name != list(function[\"parameters\"][\"properties\"].keys())[-1]:\n",
      "#                     self += \",\\n\"\n",
      "#             self += \"\"\"\n",
      "# }) => any;\n",
      "\n",
      "# \"\"\"\n",
      "#             self[function['name']] = function\n",
      "#         self += \"} // namespace functions\\n\"\n",
      "        \n",
      "#         return self\n",
      "\n",
      "    def _run_stateless(self, stateless_function, temperature=0.0, top_p=1.0, n=1):\n",
      "        assert Model._grammar_only == 0, \"We can't run grammar parsing while in context free mode! (for example inside a block closer)\"\n",
      "        \n",
      "        logger.debug(\"start Model._run_stateless\")\n",
      "\n",
      "        # This needs to be here for streaming\n",
      "        # if name is not None:\n",
      "        #     self[name] = \"\"\n",
      "\n",
      "\n",
      "        # replace ModelVariables with their actual values (note we save what we replaced so we can restore it later)\n",
      "        replacements = replace_model_variables(stateless_function, self)\n",
      "\n",
      "        # start the generation stream\n",
      "        gen_obj = self.engine(self._current_prompt(), stateless_function)\n",
      "\n",
      "        # we will return a new extended version of ourselves, which we track as `lm`\n",
      "        lm = self\n",
      "\n",
      "        # single generation\n",
      "        if n == 1:\n",
      "            generated_value = \"\"\n",
      "            # logprobs_out = []\n",
      "\n",
      "            delayed_bytes = b\"\"\n",
      "            # last_is_generated = False\n",
      "            for chunk in gen_obj:\n",
      "\n",
      "                # we make everything full probability if we are not computing uncertainty\n",
      "                # if not self.engine.compute_log_probs:\n",
      "                #     chunk.new_bytes_prob = 1.0\n",
      "                \n",
      "                # convert the bytes to a string (delaying if we don't yet have a valid unicode string)\n",
      "                lm.token_count += chunk.new_token_count\n",
      "                chunk.new_bytes = delayed_bytes + chunk.new_bytes\n",
      "                try:\n",
      "                    new_text = chunk.new_bytes.decode(\"utf8\")\n",
      "                except UnicodeDecodeError:\n",
      "                    delayed_bytes = chunk.new_bytes\n",
      "                    continue\n",
      "                delayed_bytes = b\"\"\n",
      "\n",
      "                if len(chunk.new_bytes) > 0:\n",
      "                    generated_value += new_text\n",
      "                    if chunk.is_generated:\n",
      "                        lm += f\"<||_html:<span style='background-color: rgba({165*(1-chunk.new_bytes_prob) + 0}, {165*chunk.new_bytes_prob + 0}, 0, {0.15}); border-radius: 3px;' title='{chunk.new_bytes_prob}'>_||>\"\n",
      "                    lm += new_text\n",
      "                    if chunk.is_generated:\n",
      "                        lm += \"<||_html:</span>_||>\"\n",
      "                \n",
      "                # last_is_generated = chunk.is_generated\n",
      "\n",
      "                if len(chunk.capture_groups) > 0:\n",
      "                    for k in chunk.capture_groups:\n",
      "                        v = chunk.capture_groups[k]\n",
      "                            \n",
      "                        # see if we are in a list_append mode\n",
      "                        if isinstance(v, list):\n",
      "                            for i,inner_v in enumerate(v):\n",
      "                                # convert to a string if possible\n",
      "                                # TODO: will need to not just always do this once we support images etc.\n",
      "                                try:\n",
      "                                    inner_v = inner_v.decode(\"utf8\") if isinstance(inner_v, bytes) else inner_v\n",
      "                                except UnicodeDecodeError:\n",
      "                                    pass\n",
      "\n",
      "                                if k not in lm or not isinstance(lm._variables[k], list):\n",
      "                                    lm._variables[k] = []\n",
      "                                    lm._variables_log_probs[k] = []\n",
      "                                lm._variables[k].append(inner_v)\n",
      "                                lm._variables_log_probs[k].append(chunk.capture_group_log_probs[k][i])\n",
      "\n",
      "                        # ...or standard assignment mode\n",
      "                        else:\n",
      "                            # convert to a string if possible\n",
      "                            # TODO: will need to not just always do this once we support images etc.\n",
      "                            try:\n",
      "                                v = v.decode(\"utf8\") if isinstance(v, bytes) else v\n",
      "                            except UnicodeDecodeError:\n",
      "                                pass\n",
      "                            lm._variables[k] = v\n",
      "                            lm._variables_log_probs[k] = chunk.capture_group_log_probs[k]\n",
      "\n",
      "            # if len(chunk.capture_groups) > 0:\n",
      "            #     for k in chunk.capture_groups:\n",
      "            #         v = chunk.capture_groups[k]\n",
      "            #         lm[k] = v.decode(\"utf8\") if isinstance(v, bytes) else v\n",
      "        \n",
      "        unreplace_model_variables(replacements)\n",
      "\n",
      "        logger.debug(\"finish Model._run_stateless\")\n",
      "\n",
      "        return lm\n",
      "\n",
      "class ModelStream:\n",
      "    def __init__(self, model, grammar=None, timeout=5):\n",
      "        '''Create a model stream object that delays execution until it is iterated over.'''\n",
      "        if model.echo:\n",
      "            model = model.copy()\n",
      "            model.echo = False # turn off display echoing\n",
      "        self.model = model\n",
      "        self.grammar = grammar\n",
      "        self.timeout = timeout\n",
      "\n",
      "    def __add__(self, grammar):\n",
      "        '''Extend this delayed chain of execution with another grammar append.'''\n",
      "        if self.grammar is None:\n",
      "            return ModelStream(self.model, grammar)\n",
      "        else:\n",
      "            return ModelStream(self.model, self.grammar + grammar)\n",
      "\n",
      "    def _inner_run(self, model):\n",
      "        '''This runs the model stream without iterating, and is only using internally by __iter__.'''\n",
      "        if isinstance(self.grammar, ModelStream):\n",
      "            model = self.grammar._inner_run(model)\n",
      "        elif self.grammar is None:\n",
      "            model = self.model + \"\"\n",
      "        else:\n",
      "            model = self.model + self.grammar\n",
      "    \n",
      "    def __iter__(self):\n",
      "        '''Starts a thread to execute the model and grammar, yielding events as they occur.'''\n",
      "        \n",
      "        # Create a thread-safe queue to hold events\n",
      "        with CaptureEvents(self.model) as events:\n",
      "\n",
      "            # Define the target function for the thread\n",
      "            def target():\n",
      "                try:\n",
      "                    self._inner_run(self.model)\n",
      "                    events.put(None) # mark that we are done\n",
      "                except BaseException as ex:\n",
      "                    events.put(ex)\n",
      "\n",
      "            # Start the thread\n",
      "            thread = threading.Thread(target=target)\n",
      "            thread.start()\n",
      "\n",
      "            # Yield events from the queue as they become available\n",
      "            while True:\n",
      "                try:\n",
      "                    # Wait for an event with a timeout to allow for thread termination\n",
      "                    event = events.get(timeout=self.timeout)\n",
      "                    if event is None:\n",
      "                        break\n",
      "                    elif isinstance(event, BaseException):\n",
      "                        raise event\n",
      "                    yield event\n",
      "                except queue.Empty:\n",
      "                    # Check if the thread is still alive\n",
      "                    if not thread.is_alive():\n",
      "                        break\n",
      "\n",
      "            # Ensure the thread has completed\n",
      "            thread.join()\n",
      "\n",
      "class Chat(Model):\n",
      "    '''The base class for all chat-tuned models.'''\n",
      "    \n",
      "    def get_role_start(self, role_name, **kwargs):\n",
      "        '''The starting grammar for a role.\n",
      "        \n",
      "        By default we follow the GPT role tag start conventions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        role_name : str\n",
      "            The name of the role, like \"user\", or \"assistant\"\n",
      "        kwargs : dict\n",
      "            This kwargs are added to the role start as arguments.\n",
      "        '''\n",
      "        return \"<|im_start|>\"+role_name+\"\".join([f' {k}=\"{v}\"' for k,v in kwargs.items()])+\"\\n\"\n",
      "    \n",
      "    def get_role_end(self, role_name=None):\n",
      "        '''The ending bytes for a role.\n",
      "        \n",
      "        Note that we cannot use a grammar in closers because they need to remain constant\n",
      "        so we can append them whenever we need a representation before the final closing of the context.\n",
      "        By default we follow the GPT role tag end conventions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        role_name : str\n",
      "            The name of the role, like \"user\", or \"assistant\"\n",
      "        '''\n",
      "        return \"<|im_end|>\"\n",
      "    \n",
      "class Instruct(Model):\n",
      "    '''The base class for all instruction-tuned models.'''\n",
      "\n",
      "    def get_role_start(self, role_name, **kwargs):\n",
      "        raise Exception(\"Subclasses need to define what the role start should be!\")\n",
      "    \n",
      "    def get_role_end(self, role_name=None):\n",
      "        raise Exception(\"Subclasses need to define what the role end should be!\")\n",
      "    \n",
      "class GrammarOnly:\n",
      "    def __enter__(self):\n",
      "        Model._grammar_only += 1\n",
      "    \n",
      "    def __exit__(self, exc_type, exc_value, traceback):\n",
      "        Model._grammar_only -= 1\n",
      "\n",
      "def grammar_only():\n",
      "    '''Returns a context manager that ensures only grammars are executed (not full python functions).'''\n",
      "    return GrammarOnly()\n",
      "\n",
      "class ThrottleRefresh:\n",
      "    def __enter__(self):\n",
      "        Model._throttle_refresh += 1\n",
      "    \n",
      "    def __exit__(self, exc_type, exc_value, traceback):\n",
      "        Model._throttle_refresh -= 1\n",
      "\n",
      "def throttle_refresh():\n",
      "    '''Returns a context manager that allows the print statement to drop display calls above the throttle rate.'''\n",
      "    return ThrottleRefresh()\n",
      "\n",
      "class ConstraintException(Exception):\n",
      "    pass\n",
      "\n",
      "# def _compute_probs(trie, probs, found):\n",
      "#     '''Computes the log probabilities for each internal trie node.'''\n",
      "#     if trie.value is not None:\n",
      "#         found[trie.value] = 1\n",
      "#         trie.prob += probs[trie.value]\n",
      "    \n",
      "#     if len(trie) > 0:\n",
      "#         # child_probs = []\n",
      "#         for b in trie.keys():\n",
      "#             child = trie.child(b)\n",
      "#             _compute_probs(child, probs, found)\n",
      "#             trie.prob += child.prob\n",
      "#         # trie.log_prob = np.logaddexp.reduce(child_log_probs)\n",
      "\n",
      "def _check_dominated(node, parser, match_version, next_byte_mask):\n",
      "    curr_pos = parser.pos\n",
      "    for byte_num in next_byte_mask.nonzero()[0]:\n",
      "        next_byte = bytes((byte_num,))\n",
      "        if not node.has_child(next_byte):\n",
      "            return False # no possible exension this direction, so we are not dominated\n",
      "        child = node.child(next_byte)\n",
      "        if child.match_version < match_version:\n",
      "            child.match_version = match_version\n",
      "            child.match = next_byte_mask[next_byte[0]]\n",
      "        \n",
      "        if not child.match:\n",
      "            return False # this child does not dominate the node, so the node is not dominated\n",
      "        elif child.value is None: # this child might not dominate the node\n",
      "            parser.consume_byte(next_byte, log_prob=0.0)\n",
      "            child_dominate = _check_dominated(child, parser, match_version, parser.next_byte_mask())\n",
      "            parser.pos = curr_pos\n",
      "            if not child_dominate:\n",
      "                return False\n",
      "    return True\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/_togetherai.py\n",
      "```py\n",
      "import os\n",
      "from ._model import Chat, Instruct\n",
      "from ._openai import OpenAIChatEngine, OpenAI, OpenAIInstructEngine, OpenAICompletionEngine, OpenAIEngine\n",
      "from .transformers._transformers import TransformersTokenizer\n",
      "\n",
      "\n",
      "class TogetherAI(OpenAI):\n",
      "    def __init__(self, model, tokenizer=None, echo=True, api_key=None, max_streaming_tokens=1000, timeout=0.5, compute_log_probs=False, engine_class=None, **kwargs):\n",
      "        '''\n",
      "        Build a new TogetherAI model object that represents a model in a given state.\n",
      "        '''\n",
      "\n",
      "        tokenizer = TransformersTokenizer(model=model, tokenizer=tokenizer, ignore_bos_token=True)\n",
      "\n",
      "        # Default base_url is the together.ai endpoint\n",
      "        if not \"base_url\" in kwargs:\n",
      "            kwargs[\"base_url\"] = 'https://api.together.xyz'\n",
      "        # TogetherAI uses TOGETHERAI_API_KEY env value instead of OPENAI_API_KEY\n",
      "        # We pass explicitly to avoid OpenAI class complaining about a missing key\n",
      "        if api_key is None:\n",
      "            api_key = os.environ.get(\"TOGETHERAI_API_KEY\", None)\n",
      "        if api_key is None:\n",
      "            raise Exception(\n",
      "                \"The api_key client option must be set either by passing api_key to the client or by setting the TOGETHERAI_API_KEY environment variable\"\n",
      "            )\n",
      "\n",
      "        if engine_class is None:\n",
      "            engine_map = {\n",
      "                TogetherAICompletion: OpenAICompletionEngine,\n",
      "                TogetherAIInstruct: OpenAIChatEngine,\n",
      "                TogetherAIChat: OpenAIChatEngine,\n",
      "                TogetherAI: OpenAICompletionEngine,\n",
      "            }\n",
      "            for k in engine_map:\n",
      "                if issubclass(self.__class__, k):\n",
      "                    engine_class = engine_map[k]\n",
      "                    break\n",
      "\n",
      "        super().__init__(\n",
      "            model, tokenizer, echo, api_key, max_streaming_tokens, timeout, compute_log_probs, engine_class, **kwargs\n",
      "        )\n",
      "\n",
      "class TogetherAICompletion(TogetherAI):\n",
      "    pass\n",
      "\n",
      "\n",
      "class TogetherAIInstruct(TogetherAI, Instruct):\n",
      "    \"\"\"\n",
      "    Utilizes chat endpoints to simulate a single instruction query\n",
      "    together.ai will format in correct prompt template for model on their end\n",
      "    \"\"\"\n",
      "    def get_role_start(self, name):\n",
      "        if name == \"instruction\":\n",
      "            return \"<|im_start|>user\\n\"\n",
      "        else:\n",
      "            raise Exception(f\"The TogetherAIInstruct model does not know about the {name} role type!\")\n",
      "    \n",
      "    def get_role_end(self, name):\n",
      "        if name == \"instruction\":\n",
      "            return \"<|im_end|>\"\n",
      "        else:\n",
      "            raise Exception(f\"The TogetherAIInstruct model does not know about the {name} role type!\")\n",
      "\n",
      "\n",
      "class TogetherAIChat(TogetherAI, Chat):\n",
      "    pass\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/vertexai/_PaLM2.py\n",
      "```py\n",
      "import os\n",
      "from pathlib import Path\n",
      "import multiprocessing\n",
      "from itertools import takewhile\n",
      "import operator\n",
      "import threading\n",
      "import numpy as np\n",
      "import queue\n",
      "import time\n",
      "import tiktoken\n",
      "\n",
      "from ._vertexai import VertexAICompletion, VertexAIInstruct, VertexAIChat\n",
      "\n",
      "try:\n",
      "    from vertexai.language_models import TextGenerationModel, ChatModel, InputOutputTextPair\n",
      "    is_vertexai = True\n",
      "except ImportError:\n",
      "    is_vertexai = False\n",
      "\n",
      "class PaLM2Completion(VertexAICompletion):\n",
      "    def __init__(self, model, tokenizer=None, echo=True, max_streaming_tokens=None, **kwargs):\n",
      "    \n",
      "        if isinstance(model, str):\n",
      "            model = TextGenerationModel.from_pretrained(self.model_name)\n",
      "        \n",
      "        # PaLM2 does not have a public tokenizer, so we pretend it tokenizes like gpt2...\n",
      "        if tokenizer is None:\n",
      "            tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
      "        \n",
      "        # the superclass does all the work\n",
      "        super().__init__(\n",
      "            model,\n",
      "            tokenizer=tokenizer,\n",
      "            echo=echo,\n",
      "            max_streaming_tokens=max_streaming_tokens,\n",
      "            **kwargs\n",
      "        )\n",
      "\n",
      "class PaLM2Instruct(VertexAIInstruct):\n",
      "    def __init__(self, model, tokenizer=None, echo=True, max_streaming_tokens=None, **kwargs):\n",
      "        if isinstance(model, str):\n",
      "            model = TextGenerationModel.from_pretrained(model)\n",
      "        \n",
      "        # PaLM2 does not have a public tokenizer, so we pretend it tokenizes like gpt2...\n",
      "        if tokenizer is None:\n",
      "            tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
      "        \n",
      "        # the superclass does all the work\n",
      "        super().__init__(\n",
      "            model,\n",
      "            tokenizer=tokenizer,\n",
      "            echo=echo,\n",
      "            max_streaming_tokens=max_streaming_tokens,\n",
      "            **kwargs\n",
      "        )\n",
      "\n",
      "class PaLM2Chat(VertexAIChat):\n",
      "    def __init__(self, model, tokenizer=None, echo=True, max_streaming_tokens=None, **kwargs):\n",
      "        if isinstance(model, str):\n",
      "            model = ChatModel.from_pretrained(model)\n",
      "        \n",
      "        # PaLM2 does not have a public tokenizer, so we pretend it tokenizes like gpt2...\n",
      "        if tokenizer is None:\n",
      "            tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
      "        \n",
      "        # the superclass does all the work\n",
      "        super().__init__(\n",
      "            model,\n",
      "            tokenizer=tokenizer,\n",
      "            echo=echo,\n",
      "            max_streaming_tokens=max_streaming_tokens,\n",
      "            **kwargs\n",
      "        )\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/vertexai/_vertexai.py\n",
      "```py\n",
      "import re\n",
      "from .._model import Chat, Instruct\n",
      "from .._grammarless import GrammarlessEngine, Grammarless\n",
      "\n",
      "try:\n",
      "    import vertexai\n",
      "    is_vertexai = True\n",
      "except ImportError:\n",
      "    is_vertexai = False\n",
      "\n",
      "class VertexAIEngine(GrammarlessEngine):\n",
      "    def __init__(self, tokenizer, max_streaming_tokens, timeout, compute_log_probs, model_obj):\n",
      "        super().__init__(tokenizer, max_streaming_tokens, timeout, compute_log_probs)\n",
      "        self.model_obj = model_obj\n",
      "\n",
      "class VertexAI(Grammarless):\n",
      "    def __init__(self, model, tokenizer=None, echo=True, max_streaming_tokens=None, timeout=0.5, compute_log_probs=False, engine_class=None, **kwargs):\n",
      "        '''Build a new VertexAI model object that represents a model in a given state.'''\n",
      "        if not is_vertexai:\n",
      "            raise Exception(\"Please install the vertexai package using `pip install google-cloud-aiplatform` in order to use guidance.models.VertexAI!\")\n",
      "        \n",
      "        # if we are called directly (as opposed to through super()) then we convert ourselves to a more specific subclass if possible\n",
      "        if self.__class__ is VertexAI:\n",
      "            found_subclass = None\n",
      "            from .. import vertexai as vertexai_subclasses\n",
      "\n",
      "            if isinstance(model, str):\n",
      "                model_name = model\n",
      "            else:\n",
      "                model_name = model._model_id\n",
      "\n",
      "            # CodeyCompletion\n",
      "            if re.match(\"code-gecko(@[0-9]+)?\", model_name):\n",
      "                found_subclass = vertexai_subclasses.CodeyCompletion\n",
      "\n",
      "            # CodeyInstruct\n",
      "            elif re.match(\"code-bison(@[0-9]+)?\", model_name):\n",
      "                found_subclass = vertexai_subclasses.CodeyInstruct\n",
      "\n",
      "            # CodeyChat\n",
      "            elif re.match(\"codechat-bison(@[0-9]+)?\", model_name):\n",
      "                found_subclass = vertexai_subclasses.CodeyChat\n",
      "\n",
      "            # PaLM2Instruct\n",
      "            elif re.match(\"text-(bison|unicorn)(@[0-9]+)?\", model_name):\n",
      "                found_subclass = vertexai_subclasses.PaLM2Instruct\n",
      "\n",
      "            # PaLM2Chat\n",
      "            elif re.match(\"chat-bison(@[0-9]+)?\", model_name):\n",
      "                found_subclass = vertexai_subclasses.PaLM2Chat\n",
      "\n",
      "            # Gemini2Chat\n",
      "            elif re.match(\"gemini-pro(@[0-9]+)?\", model_name):\n",
      "                found_subclass = vertexai_subclasses.GeminiChat\n",
      "            \n",
      "            # convert to any found subclass\n",
      "            if found_subclass is not None:\n",
      "                self.__class__ = found_subclass\n",
      "                found_subclass.__init__(self, model, tokenizer=tokenizer, echo=echo, max_streaming_tokens=max_streaming_tokens, **kwargs)\n",
      "                return # we return since we just ran init above and don't need to run again\n",
      "        \n",
      "            # make sure we have a valid model object\n",
      "            if isinstance(model, str):\n",
      "                raise Exception(\"The model ID you passed, `{model}`, does not match any known subclasses!\")\n",
      "\n",
      "        # this allows us to use a single constructor for all our subclasses\n",
      "        if engine_class is None:\n",
      "            engine_map = {\n",
      "                VertexAICompletion: VertexAICompletionEngine,\n",
      "                VertexAIInstruct: VertexAIInstructEngine,\n",
      "                VertexAIChat: VertexAIChatEngine\n",
      "            }\n",
      "            for k in engine_map:\n",
      "                if issubclass(self.__class__, k):\n",
      "                    engine_class = engine_map[k]\n",
      "                    break\n",
      "\n",
      "        super().__init__(\n",
      "            engine_class(tokenizer=tokenizer, timeout=timeout, compute_log_probs=compute_log_probs, max_streaming_tokens=max_streaming_tokens, model_obj=model),\n",
      "            echo=echo\n",
      "        )\n",
      "\n",
      "class VertexAICompletion(VertexAI):\n",
      "    pass\n",
      "\n",
      "class VertexAICompletionEngine(VertexAIEngine):\n",
      "\n",
      "    def _generator(self, prompt, temperature):\n",
      "        self._not_running_stream.clear() # so we know we are running\n",
      "        self._data = prompt # we start with this data\n",
      "\n",
      "        try:\n",
      "            kwargs = {}\n",
      "            if self.max_streaming_tokens is not None:\n",
      "                kwargs[\"max_output_tokens\"] = self.max_streaming_tokens\n",
      "            generator = self.model_obj.predict_streaming(\n",
      "                prompt.decode(\"utf8\"),\n",
      "                #top_p=self.top_p,\n",
      "                temperature=temperature,\n",
      "                **kwargs\n",
      "            )\n",
      "        except Exception as e: # TODO: add retry logic\n",
      "            raise e\n",
      "        \n",
      "        for chunk in generator:\n",
      "            yield chunk.text.encode(\"utf8\")\n",
      "\n",
      "class VertexAIInstruct(VertexAI, Instruct):\n",
      "\n",
      "    def get_role_start(self, name):\n",
      "        return \"\"\n",
      "    \n",
      "    def get_role_end(self, name):\n",
      "        if name == \"instruction\":\n",
      "            return \"<|endofprompt|>\"\n",
      "        else:\n",
      "            raise Exception(f\"The VertexAIInstruct model does not know about the {name} role type!\")\n",
      "\n",
      "class VertexAIInstructEngine(VertexAIEngine):\n",
      "    def _generator(self, prompt, temperature):\n",
      "        # start the new stream\n",
      "        prompt_end = prompt.find(b'<|endofprompt|>')\n",
      "        if prompt_end >= 0:\n",
      "            stripped_prompt = prompt[:prompt_end]\n",
      "        else:\n",
      "            raise Exception(\"This model cannot handle prompts that don't match the instruct format! Follow for example:\\nwith instruction():\\n    lm += prompt\\nlm += gen(max_tokens=10)\")\n",
      "        self._not_running_stream.clear() # so we know we are running\n",
      "        self._data = stripped_prompt + b'<|endofprompt|>'# we start with this data\n",
      "        kwargs = {}\n",
      "        if self.max_streaming_tokens is not None:\n",
      "            kwargs[\"max_output_tokens\"] = self.max_streaming_tokens\n",
      "        for chunk in self.model_obj.predict_streaming(self._data.decode(\"utf8\"), temperature=temperature, **kwargs):\n",
      "            yield chunk.text.encode(\"utf8\")\n",
      "\n",
      "class VertexAIChat(VertexAI, Chat):\n",
      "    pass\n",
      "class VertexAIChatEngine(VertexAIEngine):\n",
      "\n",
      "    def _generator(self, prompt, temperature):\n",
      "        \n",
      "        # find the system text\n",
      "        pos = 0\n",
      "        system_start = b'<|im_start|>system\\n'\n",
      "        user_start = b'<|im_start|>user\\n'\n",
      "        assistant_start = b'<|im_start|>assistant\\n'\n",
      "        role_end = b'<|im_end|>'\n",
      "        # system_start_pos = prompt.startswith(system_start)\n",
      "        \n",
      "        # find the system text\n",
      "        system_text = b''\n",
      "        if prompt.startswith(system_start):\n",
      "            pos += len(system_start)\n",
      "            system_end_pos = prompt.find(role_end)\n",
      "            system_text = prompt[pos:system_end_pos]\n",
      "            pos = system_end_pos + len(role_end)\n",
      "\n",
      "        # find the user/assistant pairs\n",
      "        messages = []\n",
      "        valid_end = False\n",
      "        while True:\n",
      "\n",
      "            # find the user text\n",
      "            if prompt[pos:].startswith(user_start):\n",
      "                pos += len(user_start)\n",
      "                end_pos = prompt[pos:].find(role_end)\n",
      "                if end_pos < 0:\n",
      "                    break\n",
      "                messages.append(dict(\n",
      "                    role=\"user\",\n",
      "                    content=prompt[pos:pos+end_pos].decode(\"utf8\"),\n",
      "                ))\n",
      "                pos += end_pos + len(role_end)\n",
      "            elif prompt[pos:].startswith(assistant_start):\n",
      "                pos += len(assistant_start)\n",
      "                end_pos = prompt[pos:].find(role_end)\n",
      "                if end_pos < 0:\n",
      "                    valid_end = True\n",
      "                    break\n",
      "                messages.append(dict(\n",
      "                    role=\"assistant\",\n",
      "                    content=prompt[pos:pos+end_pos].decode(\"utf8\"),\n",
      "                ))\n",
      "                pos += end_pos + len(role_end)\n",
      "            else:\n",
      "                raise Exception(\"It looks like your prompt is not a well formed chat prompt! Please enclose all model state appends inside chat role blocks like `user()` or `assistant()`.\")\n",
      "            \n",
      "        self._data = prompt[:pos]\n",
      "\n",
      "        assert len(messages) > 0, \"Bad chat format! No chat blocks were defined.\"\n",
      "        assert messages[-1][\"role\"] == \"user\", \"Bad chat format! There must be a user() role before the last assistant() role.\"\n",
      "        assert valid_end, \"Bad chat format! You must generate inside assistant() roles.\"\n",
      "\n",
      "        # TODO: don't make a new session on every call\n",
      "        # last_user_text = messages.pop().content\n",
      "        \n",
      "        return self._start_generator(system_text.decode(\"utf8\"), messages, temperature)\n",
      "\n",
      "        # kwargs = {}\n",
      "        # if self.max_streaming_tokens is not None:\n",
      "        #     kwargs[\"max_output_tokens\"] = self.max_streaming_tokens\n",
      "        # generator = chat_session.send_message_streaming(last_user_text, temperature=temperature, **kwargs)\n",
      "\n",
      "        # for chunk in generator:\n",
      "        #     yield chunk.text.encode(\"utf8\")\n",
      "\n",
      "    def _start_generator(self, system_text, messages, temperature):\n",
      "        messages = [vertexai.language_models.ChatMessage(author=m[\"role\"], content=m[\"content\"]) for m in messages]\n",
      "        last_user_text = messages.pop().content\n",
      "\n",
      "        chat_session = self.model_obj.start_chat(\n",
      "            context=system_text,\n",
      "            message_history=messages,\n",
      "        )\n",
      "\n",
      "        kwargs = {}\n",
      "        if self.max_streaming_tokens is not None:\n",
      "            kwargs[\"max_output_tokens\"] = self.max_streaming_tokens\n",
      "        generator = chat_session.send_message_streaming(last_user_text, temperature=temperature, **kwargs)\n",
      "\n",
      "        for chunk in generator:\n",
      "            yield chunk.text.encode(\"utf8\")\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/vertexai/_Codey.py\n",
      "```py\n",
      "import tiktoken\n",
      "\n",
      "from ._vertexai import VertexAICompletion, VertexAIInstruct, VertexAIChat\n",
      "\n",
      "try:\n",
      "    from vertexai.language_models import CodeGenerationModel, CodeChatModel\n",
      "    is_vertexai = True\n",
      "except ImportError:\n",
      "    is_vertexai = False\n",
      "\n",
      "class CodeyCompletion(VertexAICompletion):\n",
      "    def __init__(self, model, tokenizer=None, echo=True, max_streaming_tokens=None, **kwargs):\n",
      "    \n",
      "        if isinstance(model, str):\n",
      "            model = CodeGenerationModel.from_pretrained(model)\n",
      "        \n",
      "        # Codey does not have a public tokenizer, so we pretend it tokenizes like gpt2...\n",
      "        if tokenizer is None:\n",
      "            tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
      "        \n",
      "        # the superclass does all the work\n",
      "        super().__init__(\n",
      "            model,\n",
      "            tokenizer=tokenizer,\n",
      "            echo=echo,\n",
      "            max_streaming_tokens=max_streaming_tokens,\n",
      "            **kwargs\n",
      "        )\n",
      "\n",
      "class CodeyInstruct(VertexAIInstruct):\n",
      "    def __init__(self, model, tokenizer=None, echo=True, max_streaming_tokens=None, **kwargs):\n",
      "    \n",
      "        if isinstance(model, str):\n",
      "            model = CodeGenerationModel.from_pretrained(model)\n",
      "        \n",
      "        # Codey does not have a public tokenizer, so we pretend it tokenizes like gpt2...\n",
      "        if tokenizer is None:\n",
      "            tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
      "        \n",
      "        # the superclass does all the work\n",
      "        super().__init__(\n",
      "            model,\n",
      "            tokenizer=tokenizer,\n",
      "            echo=echo,\n",
      "            max_streaming_tokens=max_streaming_tokens,\n",
      "            **kwargs\n",
      "        )\n",
      "\n",
      "class CodeyChat(VertexAIChat):\n",
      "    def __init__(self, model, tokenizer=None, echo=True, max_streaming_tokens=None, **kwargs):\n",
      "    \n",
      "        if isinstance(model, str):\n",
      "            model = CodeChatModel.from_pretrained(model)\n",
      "        \n",
      "        # PaLM2 does not have a public tokenizer, so we pretend it tokenizes like gpt2...\n",
      "        if tokenizer is None:\n",
      "            tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
      "        \n",
      "        # the superclass does all the work\n",
      "        super().__init__(\n",
      "            model,\n",
      "            tokenizer=tokenizer,\n",
      "            echo=echo,\n",
      "            max_streaming_tokens=max_streaming_tokens,\n",
      "            **kwargs\n",
      "        )\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/vertexai/__init__.py\n",
      "```py\n",
      "from ._PaLM2 import PaLM2Completion, PaLM2Chat, PaLM2Instruct\n",
      "from ._Codey import CodeyCompletion, CodeyInstruct, CodeyChat\n",
      "from ._Gemini import GeminiChat\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/vertexai/_Gemini.py\n",
      "```py\n",
      "import os\n",
      "from pathlib import Path\n",
      "import multiprocessing\n",
      "from itertools import takewhile\n",
      "import operator\n",
      "import threading\n",
      "import numpy as np\n",
      "import queue\n",
      "import time\n",
      "import tiktoken\n",
      "import re\n",
      "\n",
      "from ._vertexai import VertexAICompletion, VertexAIInstruct, VertexAIChat, VertexAIChatEngine\n",
      "_image_token_pattern = re.compile(r'<\\|_image:(.*)\\|>')\n",
      "\n",
      "try:\n",
      "    from vertexai.language_models import TextGenerationModel, ChatModel, InputOutputTextPair\n",
      "    from vertexai.preview.generative_models import GenerativeModel, Content, Part, Image\n",
      "    import vertexai\n",
      "\n",
      "    # def get_chat_response(message):\n",
      "    #     vertexai.init(project=\"PROJECT_ID\", location=\"us-central1\")\n",
      "    #     model = GenerativeModel(\"gemini-pro\")\n",
      "    #     chat = model.start_chat()\n",
      "    #     response = chat.send_message(message)\n",
      "    #     return response.text\n",
      "\n",
      "    # print(get_chat_response(\"Hello\"))\n",
      "    # print(get_chat_response(\"What are all the colors in a rainbow?\"))\n",
      "    # print(get_chat_response(\"Why does it appear when it rains?\"))\n",
      "    is_vertexai = True\n",
      "except ImportError:\n",
      "    is_vertexai = False\n",
      "\n",
      "# class GeminiCompletion(VertexAICompletion):\n",
      "#     def __init__(self, model, tokenizer=None, echo=True, caching=True, temperature=0.0, max_streaming_tokens=None, **kwargs):\n",
      "    \n",
      "#         if isinstance(model, str):\n",
      "#             self.model_name = model\n",
      "#             self.model_obj = TextGenerationModel.from_pretrained(self.model_name)\n",
      "        \n",
      "#         # Gemini does not have a public tokenizer, so we pretend it tokenizes like gpt2...\n",
      "#         if tokenizer is None:\n",
      "#             tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
      "        \n",
      "#         # the superclass does all the work\n",
      "#         super().__init__(\n",
      "#             model,\n",
      "#             tokenizer=tokenizer,\n",
      "#             echo=echo,\n",
      "#             caching=caching,\n",
      "#             temperature=temperature,\n",
      "#             max_streaming_tokens=max_streaming_tokens,\n",
      "#             **kwargs\n",
      "#         )\n",
      "\n",
      "# class GeminiInstruct(VertexAIInstruct):\n",
      "#     def __init__(self, model, tokenizer=None, echo=True, caching=True, temperature=0.0, max_streaming_tokens=None, **kwargs):\n",
      "    \n",
      "#         if isinstance(model, str):\n",
      "#             self.model_name = model\n",
      "#             self.model_obj = TextGenerationModel.from_pretrained(self.model_name)\n",
      "        \n",
      "#         # Gemini does not have a public tokenizer, so we pretend it tokenizes like gpt2...\n",
      "#         if tokenizer is None:\n",
      "#             tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
      "        \n",
      "#         # the superclass does all the work\n",
      "#         super().__init__(\n",
      "#             model,\n",
      "#             tokenizer=tokenizer,\n",
      "#             echo=echo,\n",
      "#             caching=caching,\n",
      "#             temperature=temperature,\n",
      "#             max_streaming_tokens=max_streaming_tokens,\n",
      "#             **kwargs\n",
      "#         )\n",
      "\n",
      "class GeminiChat(VertexAIChat):\n",
      "    def __init__(self, model, tokenizer=None, echo=True, max_streaming_tokens=None, **kwargs):\n",
      "        if isinstance(model, str):\n",
      "            model = GenerativeModel(model)\n",
      "        \n",
      "        # Gemini does not have a public tokenizer, so we pretend it tokenizes like gpt2...\n",
      "        if tokenizer is None:\n",
      "            tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
      "        \n",
      "        # the superclass does all the work\n",
      "        super().__init__(\n",
      "            model,\n",
      "            tokenizer=tokenizer,\n",
      "            echo=echo,\n",
      "            max_streaming_tokens=max_streaming_tokens,\n",
      "            engine_class=GeminiChatEngine,\n",
      "            **kwargs\n",
      "        )\n",
      "\n",
      "class GeminiChatEngine(VertexAIChatEngine):\n",
      "    def _start_chat(self, system_text, messages):\n",
      "        assert system_text == \"\", \"We don't support passing system text to Gemini models (yet?)!\"\n",
      "        out = self.model_obj.start_chat(\n",
      "            history=messages\n",
      "        )\n",
      "        return out\n",
      "    \n",
      "    def _start_generator(self, system_text, messages, temperature):\n",
      "        # last_user_text = messages[-1][\"content\"]\n",
      "        formated_messages = []\n",
      "        for m in messages:\n",
      "            raw_parts = _image_token_pattern.split(m[\"content\"])\n",
      "            parts = []\n",
      "            for i in range(0, len(raw_parts), 2):\n",
      "                \n",
      "                # append the text portion\n",
      "                if len(raw_parts[i]) > 0:\n",
      "                    parts.append(Part.from_text(raw_parts[i]))\n",
      "                \n",
      "                # append any image\n",
      "                if i + 1 < len(raw_parts):\n",
      "                    parts.append(Part.from_image(Image.from_bytes(self[raw_parts[i+1]])))\n",
      "            formated_messages.append(Content(role=m[\"role\"], parts=parts))\n",
      "        last_user_parts = formated_messages.pop() # remove the last user stuff that goes in send_message (and not history)\n",
      "\n",
      "        chat_session = self.model_obj.start_chat(\n",
      "            history=formated_messages,\n",
      "        )\n",
      "\n",
      "        generation_config = {\n",
      "            \"temperature\": temperature\n",
      "        }\n",
      "        if self.max_streaming_tokens is not None:\n",
      "            generation_config[\"max_output_tokens\"] = self.max_streaming_tokens\n",
      "        generator = chat_session.send_message(last_user_parts, generation_config=generation_config, stream=True)\n",
      "\n",
      "        for chunk in generator:\n",
      "            yield chunk.candidates[0].content.parts[0].text.encode(\"utf8\")\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/transformers/__init__.py\n",
      "```py\n",
      "from ._llama import Llama, LlamaChat\n",
      "from ._transformers import Transformers, TransformersChat\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/transformers/_transformers.py\n",
      "```py\n",
      "import os\n",
      "\n",
      "try:\n",
      "    import torch\n",
      "except ImportError:\n",
      "    pass\n",
      "\n",
      "from .._model import Tokenizer, Engine, Model, Chat\n",
      "\n",
      "class TransformersTokenizer(Tokenizer):\n",
      "    def __init__(self, model, tokenizer, ignore_bos_token=False):\n",
      "        if tokenizer is None:\n",
      "            tokenizer = self._tokenizer(model)\n",
      "\n",
      "\n",
      "        self._orig_tokenizer = tokenizer\n",
      "\n",
      "        # build out the set of byte_string tokens\n",
      "        if hasattr(tokenizer, \"byte_decoder\"):\n",
      "            byte_tokens = []\n",
      "            for i in range(len(tokenizer)):\n",
      "                byte_coded = bytes([tokenizer.byte_decoder[c] for c in tokenizer.convert_ids_to_tokens(i)])\n",
      "                byte_tokens.append(byte_coded)\n",
      "        elif hasattr(tokenizer, \"convert_tokens_to_string\"):\n",
      "            byte_tokens = []\n",
      "            for i in range(len(tokenizer)):\n",
      "                s = tokenizer.convert_tokens_to_string(['a', tokenizer.convert_ids_to_tokens(i)])\n",
      "                if s[0] == 'a':\n",
      "                    s = s[1:]\n",
      "                elif s[1] == 'a':\n",
      "                    s = s[2:]\n",
      "                else:\n",
      "                    raise Exception(\"Can't determine tokenstring representation!\")\n",
      "                byte_tokens.append(bytes(s, encoding=\"utf8\"))\n",
      "        else:\n",
      "            raise Exception(\"Invalid tokenizer object!\")\n",
      "\n",
      "        # the superclass does most of the work once we have the tokens\n",
      "        super().__init__(\n",
      "            byte_tokens,\n",
      "            None if ignore_bos_token else tokenizer.bos_token_id,\n",
      "            tokenizer.eos_token_id,\n",
      "        )\n",
      "    \n",
      "\n",
      "    def _tokenizer(self, model, **kwargs):\n",
      "        # intantiate the tokenizer\n",
      "        if isinstance(model, str):\n",
      "            # make sure transformers is installed\n",
      "            try:\n",
      "                import transformers\n",
      "            except:\n",
      "                raise Exception(\"Please install transformers with `pip install transformers` in order to use guidance.models.togetherai!\")\n",
      "\n",
      "            try:\n",
      "                tokenizer = transformers.AutoTokenizer.from_pretrained(model, use_fast=False, **kwargs)\n",
      "                # This is here because some tokenizers are bad and don't have all the bytes (I'm looking at you, microsoft/phi2)\n",
      "                if hasattr(tokenizer, \"byte_decoder\"):\n",
      "                    all_bytes = set()\n",
      "                    for x in tokenizer.get_vocab().keys():\n",
      "                        [all_bytes.add(y) for y in x]\n",
      "                    assert set(tokenizer.byte_decoder.keys()).intersection(all_bytes) == all_bytes\n",
      "            except:\n",
      "                tokenizer = transformers.AutoTokenizer.from_pretrained(model, use_fast=True, **kwargs) # fall back to the fast tokenizer\n",
      "        \n",
      "        assert tokenizer is not None, \"You must give a model name when you provide a tokenizer object!\"\n",
      "            \n",
      "        return tokenizer\n",
      "\n",
      "class TransformersEngine(Engine):\n",
      "    def __init__(self, model, tokenizer, compute_log_probs, **kwargs):\n",
      "        # fill in default model value\n",
      "        if model is None:\n",
      "            model = os.environ.get(\"TRANSFORMERS_MODEL\", None)\n",
      "        if model is None:\n",
      "            try:\n",
      "                with open(os.path.expanduser('~/.transformers_model'), 'r') as file:\n",
      "                    model = file.read().replace('\\n', '')\n",
      "            except:\n",
      "                pass\n",
      "\n",
      "        self.model_obj = self._model(model, **kwargs)\n",
      "\n",
      "        if not isinstance(model, str):\n",
      "            self.model = model.__class__.__name__\n",
      "        self.device = self.model_obj.device # otherwise note the current device\n",
      "\n",
      "        self._past_key_values = None\n",
      "        self._cached_logits = None\n",
      "        self._cached_token_ids = []\n",
      "\n",
      "        super().__init__(\n",
      "            TransformersTokenizer(model, tokenizer),\n",
      "            compute_log_probs=compute_log_probs\n",
      "        )\n",
      "\n",
      "    def _model(self, model, **kwargs):\n",
      "        # intantiate the model if needed\n",
      "        if isinstance(model, str):\n",
      "\n",
      "            # make sure transformers is installed\n",
      "            try:\n",
      "                import transformers\n",
      "            except:\n",
      "                raise Exception(\"Please install transformers with `pip install transformers` in order to use guidance.models.Transformers!\")\n",
      "            model = transformers.AutoModelForCausalLM.from_pretrained(model, **kwargs)\n",
      "        return model\n",
      "\n",
      "    def _joint_tokenize(self, token_ids):\n",
      "        # first_decode = self.tokenizer._orig_tokenizer.decode(token_ids)\n",
      "        first_decode = b''.join([self.tokenizer.tokens[id] for id in token_ids]).decode(\"utf8\")\n",
      "        new_ids = self.tokenizer._orig_tokenizer(first_decode, add_special_tokens=False)[\"input_ids\"]\n",
      "\n",
      "        # HACK: check for a bug in the HuggingFace tokenizer (that will just add extra spaces during an encode-decode cycle)\n",
      "        second_decode = self.tokenizer._orig_tokenizer.decode(new_ids)\n",
      "        if second_decode != first_decode and len(second_decode) == len(first_decode) + 1 and second_decode.startswith(\"<s>  \"):\n",
      "            new_ids = new_ids[0:1] + new_ids[2:]\n",
      "        \n",
      "        return new_ids\n",
      "    \n",
      "    def get_logits(self, token_ids, forced_bytes, current_temp):\n",
      "        '''Computes the logits for the given token state.\n",
      "        \n",
      "        This overrides a method from the LocalEngine class that is used to get\n",
      "        inference results from the model.\n",
      "        '''\n",
      "\n",
      "        # make sure we don't run off the end of the model\n",
      "        if len(token_ids) >= getattr(self.model_obj.config, \"max_position_embeddings\", 1e10):\n",
      "            raise Exception(f\"Attempted to run a transformers model past its maximum context window size of {self.model_obj.config.max_position_embeddings}!\")\n",
      "\n",
      "        # get the number of cache positions we are using\n",
      "        cache_token_ids = self._cached_token_ids\n",
      "        num_cached = 0\n",
      "        for id in cache_token_ids:\n",
      "            if num_cached >= len(cache_token_ids) or num_cached >= len(token_ids) or token_ids[num_cached] != id:\n",
      "                break\n",
      "            num_cached += 1\n",
      "\n",
      "        # reset the cache length according to that number of positions\n",
      "        past_key_values = self._past_key_values\n",
      "        past_length = past_key_values[0][0].size(-2) if past_key_values is not None else 0\n",
      "        if past_length > num_cached:\n",
      "            past_length = max(0, num_cached - 1) # note we recompute the last token because we don't bother to handle the special case of just computing logits\n",
      "            self._past_key_values = tuple(tuple(p[..., :past_length, :] for p in v) for v in past_key_values)\n",
      "        cache_token_ids[past_length:] = []\n",
      "        \n",
      "        # call the model\n",
      "        new_token_ids = token_ids[past_length:]\n",
      "        if len(new_token_ids) > 0:\n",
      "            with torch.no_grad():\n",
      "                model_out = self.model_obj(\n",
      "                    input_ids=torch.tensor(new_token_ids).unsqueeze(0).to(self.device),\n",
      "                    past_key_values=self._past_key_values,\n",
      "                    use_cache=True,\n",
      "                    position_ids=torch.arange(past_length, past_length+len(new_token_ids)).unsqueeze(0).to(self.device),\n",
      "                    attention_mask=torch.ones(1, past_length + len(new_token_ids)).to(self.device),\n",
      "                    return_dict=True,\n",
      "                    output_attentions=False,\n",
      "                    output_hidden_states=False\n",
      "                )\n",
      "\n",
      "            # save the results\n",
      "            self._past_key_values = model_out.past_key_values\n",
      "            cache_token_ids.extend(new_token_ids)\n",
      "            self._cached_logits = model_out.logits[0, -1, :].cpu().numpy()\n",
      "        \n",
      "        return self._cached_logits\n",
      "\n",
      "\n",
      "class Transformers(Model):\n",
      "    def __init__(self, model=None, tokenizer=None, echo=True, compute_log_probs=False, **kwargs):\n",
      "        '''Build a new Transformers model object that represents a model in a given state.'''\n",
      "        super().__init__(\n",
      "            TransformersEngine(model, tokenizer, compute_log_probs, **kwargs),\n",
      "            echo=echo\n",
      "        )\n",
      "\n",
      "class TransformersChat(Transformers, Chat):\n",
      "    pass\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/transformers/_llama.py\n",
      "```py\n",
      "from ._transformers import Transformers, TransformersChat\n",
      "\n",
      "class Llama(Transformers):\n",
      "    pass\n",
      "\n",
      "class LlamaChat(TransformersChat, Llama):\n",
      "\n",
      "    def system(self):\n",
      "        \"\"\"Patch up the system command to convert normal system role structure into Llama structure (nested in the first user message).\"\"\"\n",
      "        self._system_prefex = \"[INST] \" if str(self) == \"\" else \"\"\n",
      "        out = super().system()\n",
      "        delattr(self, \"_system_prefex\")\n",
      "        return out\n",
      "    \n",
      "    def get_role_start(self, role_name, **kwargs):\n",
      "        if role_name == \"system\":\n",
      "            return self._system_prefex + \"<<SYS>>\\n\"\n",
      "        elif role_name == \"user\":\n",
      "            if str(self).endswith(\"\\n<</SYS>>\\n\\n\"):\n",
      "                return \"\" # we don't need to start anything if we are starting with a top level unnested system tag\n",
      "            else:\n",
      "                return \"[INST] \"\n",
      "        else:\n",
      "            return \" \"\n",
      "    \n",
      "    def get_role_end(self, role_name=None):\n",
      "        if role_name == \"system\":\n",
      "            return \"\\n<</SYS>>\\n\\n\"\n",
      "        elif role_name == \"user\":\n",
      "            return \" [/INST]\"\n",
      "        else:\n",
      "            return \" \"\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/llama_cpp/_mistral.py\n",
      "```py\n",
      "from .._model import Instruct\n",
      "from ._llama_cpp import LlamaCpp, LlamaCppChat\n",
      "\n",
      "class MistralChat(LlamaCppChat):\n",
      "    def get_role_start(self, role_name, **kwargs):\n",
      "        if role_name == \"user\":\n",
      "            return \"[INST] \"\n",
      "        \n",
      "        elif role_name == \"assistant\":\n",
      "            return \"\"\n",
      "        \n",
      "        elif role_name == \"system\":\n",
      "            raise Exception(\"MistralChat does not support a sytem role!\")\n",
      "    \n",
      "    def get_role_end(self, role_name=None):\n",
      "        if role_name == \"user\":\n",
      "            return \" [/INST]\"\n",
      "        elif role_name == \"assistant\":\n",
      "            return \"</s>\"\n",
      "        elif role_name == \"system\":\n",
      "            raise Exception(\"MistralChat does not support a sytem role!\")\n",
      "        \n",
      "class MistralInstruct(LlamaCpp, Instruct):\n",
      "    def get_role_start(self, role_name, **kwargs):\n",
      "        if role_name == \"instruction\":\n",
      "            return \"[INST] \"\n",
      "    \n",
      "    def get_role_end(self, role_name=None):\n",
      "        if role_name == \"instruction\":\n",
      "            return \" [/INST]\"\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/llama_cpp/_llama_cpp.py\n",
      "```py\n",
      "import os\n",
      "from pathlib import Path\n",
      "from itertools import takewhile\n",
      "import operator\n",
      "import sys\n",
      "import logging\n",
      "import numpy as np\n",
      "\n",
      "from .._model import Tokenizer, Engine, Model, Chat\n",
      "from .._remote import RemoteEngine\n",
      "from ..._utils import normalize_notebook_stdout_stderr\n",
      "\n",
      "try:\n",
      "    import llama_cpp\n",
      "    is_llama_cpp = True\n",
      "except ImportError:\n",
      "    is_llama_cpp = False\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "class _LlamaBatchContext:\n",
      "    def __init__(self, n_batch, n_ctx):\n",
      "        self._llama_batch_free = llama_cpp.llama_batch_free\n",
      "        self.batch = llama_cpp.llama_batch_init(n_batch, 0, n_ctx)\n",
      "        if self.batch is None:\n",
      "            raise Exception(\"call to llama_cpp.llama_batch_init returned NULL.\")\n",
      "\n",
      "    def __del__(self):\n",
      "        llama_batch_free = getattr(self, \"_llama_batch_free\", None)\n",
      "        batch = getattr(self, \"batch\", None)\n",
      "        if batch is not None and llama_batch_free is not None:\n",
      "            self._llama_batch_free = None\n",
      "            self.batch = None\n",
      "            llama_batch_free(batch)\n",
      "\n",
      "class LlamaCppTokenizer(Tokenizer):\n",
      "    def __init__(self, model_obj):\n",
      "        self._model_obj = model_obj\n",
      "\n",
      "        tokenizer = llama_cpp.LlamaTokenizer(model_obj)\n",
      "        if not hasattr(tokenizer, 'llama'):\n",
      "            tokenizer.llama = tokenizer._model\n",
      "\n",
      "        # get the bytes strings for all the tokens\n",
      "        tokens = []\n",
      "        for i in range(tokenizer.llama.n_vocab()):\n",
      "            tok = tokenizer.llama.detokenize([i]) # note that detokenize returns bytes directly\n",
      "            if tok == b'':\n",
      "                tok = llama_cpp.llama_token_get_text(model_obj.model, i) # get text rep of special tokens\n",
      "            tokens.append(tok)\n",
      "\n",
      "        super().__init__(\n",
      "            tokens,\n",
      "            tokenizer.llama.token_bos(),\n",
      "            tokenizer.llama.token_eos()\n",
      "        )\n",
      "\n",
      "    def __call__(self, byte_string):\n",
      "        return self._model_obj.tokenize(byte_string, add_bos=False, special=True)\n",
      "\n",
      "class LlamaCppEngine(Engine):\n",
      "    '''The core class that runs inference using llama.cpp.'''\n",
      "\n",
      "    def __init__(self, model, compute_log_probs, **kwargs):\n",
      "        if not is_llama_cpp:\n",
      "            raise Exception(\"Please install llama-cpp-python with `pip install llama-cpp-python` in order to use guidance.models.LlamaCpp!\")\n",
      "\n",
      "        if isinstance(model, Path):\n",
      "            model = str(model)\n",
      "        if model is None or isinstance(model, str) and len(model.strip()) == 0:\n",
      "            model = os.environ.get(\"LLAMA_CPP_MODEL\", \"\")\n",
      "            if len(model.strip()) == 0:\n",
      "                try:\n",
      "                    with open(os.path.expanduser('~/.llama_cpp_model'), 'r') as file:\n",
      "                        model = file.read().replace('\\n', '')\n",
      "                except:\n",
      "                    pass\n",
      "                if len(model.strip()) == 0:\n",
      "                    raise ValueError(\"If model is None then a model file must be specified in either the LLAMA_CPP_MODEL environment variable or in the ~/.llama_cpp_model file.\")\n",
      "\n",
      "        if isinstance(model, str):\n",
      "            self.model = model\n",
      "            if \"verbose\" not in kwargs:\n",
      "                kwargs[\"verbose\"] = False\n",
      "\n",
      "            # patch over https://github.com/abetlen/llama-cpp-python/issues/729\n",
      "            try:\n",
      "                sys.stdout.fileno()\n",
      "            except:\n",
      "                logger.warn(\"Cannot use verbose=True in this context (probably CoLab). See https://github.com/abetlen/llama-cpp-python/issues/729\")\n",
      "                kwargs[\"verbose\"] = True # llama-cpp-python can't hide output in this case\n",
      "\n",
      "            with normalize_notebook_stdout_stderr():\n",
      "                self.model_obj = llama_cpp.Llama(model_path=model, **kwargs)\n",
      "        elif isinstance(model, llama_cpp.Llama):\n",
      "            self.model = model.__class__.__name__\n",
      "            self.model_obj = model\n",
      "        else:\n",
      "            raise TypeError(\"model must be None, a file path string, or a llama_cpp.Llama object.\")\n",
      "\n",
      "        self._context = _LlamaBatchContext(self.model_obj.n_batch, self.model_obj.n_ctx())\n",
      "        self._cache_token_ids = []\n",
      "\n",
      "        super().__init__(\n",
      "            LlamaCppTokenizer(self.model_obj),\n",
      "            compute_log_probs=compute_log_probs\n",
      "        )\n",
      "\n",
      "        self._n_vocab = len(self.tokenizer.tokens)\n",
      "\n",
      "    def _joint_tokenize(self, token_ids):\n",
      "        '''What a full joint tokenizer would give for a given byte string'''\n",
      "        byte_string = b\"\".join([self.tokenizer.tokens[t] for t in token_ids])\n",
      "        return self.tokenizer(byte_string)\n",
      "\n",
      "    def get_logits(self, token_ids, forced_bytes, current_temp):\n",
      "        '''Computes the logits for the given token state.\n",
      "        \n",
      "        This overrides a method from the LocalEngine class that is used to get\n",
      "        inference results from the model.\n",
      "        '''\n",
      "\n",
      "        if len(token_ids) == 0:\n",
      "            raise ValueError(\"token_ids must contain some tokens.\")\n",
      "\n",
      "        # check what we have already cached\n",
      "        cache_token_ids = self._cache_token_ids\n",
      "        num_cached = sum(takewhile(operator.truth, map(operator.eq, token_ids, cache_token_ids)))\n",
      "        if num_cached == len(token_ids):\n",
      "            if num_cached == len(cache_token_ids):\n",
      "                return self._cached_logits\n",
      "            num_cached = num_cached - 1 # llama_cpp doesn't like it when we pass in 0 new tokens, so re-input one\n",
      "        \n",
      "        # make sure we don't run off the end of the model's context\n",
      "        if self.model_obj.n_ctx() <= len(token_ids):\n",
      "            raise Exception(f\"Attempted to use a context length of {len(token_ids)} tokens, but this LlamaCpp model is only configured to support up to {self.model_obj.n_ctx()}!\")\n",
      "\n",
      "        self._cache_token_ids = token_ids.copy()\n",
      "\n",
      "        # clear obsolete parts of kv cache\n",
      "        llama_cpp.llama_kv_cache_seq_rm(self.model_obj.ctx, -1, num_cached, -1)\n",
      "\n",
      "        # eval the model\n",
      "        n_batch = self.model_obj.n_batch\n",
      "        batch = self._context.batch\n",
      "        for i in range(num_cached, len(token_ids), n_batch):\n",
      "            n_tokens = min(i + n_batch, len(token_ids)) - i\n",
      "            batch.n_tokens = n_tokens\n",
      "            for j in range(n_tokens):\n",
      "                batch.token[j] = token_ids[i + j]\n",
      "                batch.pos[j] = i + j\n",
      "                batch.seq_id[j][0] = 0\n",
      "                batch.n_seq_id[j] = 1\n",
      "                batch.logits[j] = False\n",
      "\n",
      "            if i + n_tokens == len(token_ids):\n",
      "                batch.logits[n_tokens - 1] = True\n",
      "\n",
      "            ret = llama_cpp.llama_decode(self.model_obj.ctx, batch)\n",
      "            if ret != 0:\n",
      "                raise Exception(f\"Call to llama_cpp.llama_decode returned {ret}.\")\n",
      "\n",
      "        # get the logits\n",
      "        logits = llama_cpp.llama_get_logits(self.model_obj.ctx)\n",
      "        logits = logits[(n_tokens - 1) * self._n_vocab : n_tokens * self._n_vocab]\n",
      "        logits = np.ctypeslib.as_array(logits, shape=(self._n_vocab,)).copy()\n",
      "\n",
      "        self._cached_logits = logits\n",
      "\n",
      "        return logits\n",
      "\n",
      "class LlamaCpp(Model):\n",
      "    def __init__(self, model=None, echo=True, compute_log_probs=False, api_key=None, **llama_cpp_kwargs):\n",
      "        '''Build a new LlamaCpp model object that represents a model in a given state.'''\n",
      "\n",
      "        if isinstance(model, str) and model.startswith(\"http\"):\n",
      "            engine = RemoteEngine(model, api_key=api_key, **llama_cpp_kwargs)\n",
      "        else:\n",
      "            engine = LlamaCppEngine(model, compute_log_probs=compute_log_probs, **llama_cpp_kwargs)\n",
      "\n",
      "        super().__init__(\n",
      "            engine,\n",
      "            echo=echo\n",
      "        )\n",
      "\n",
      "\n",
      "class LlamaCppChat(LlamaCpp, Chat):\n",
      "    def get_role_start(self, role_name, **kwargs):\n",
      "        if role_name == \"user\":\n",
      "\n",
      "            # if we follow an auto-nested system role then we are done\n",
      "            if self._current_prompt().endswith(\"\\n<</SYS>>\\n\\n\"):\n",
      "                return \"\"\n",
      "            else:\n",
      "                return \"[INST] \"\n",
      "        \n",
      "        elif role_name == \"assistant\":\n",
      "            return \" \"\n",
      "        \n",
      "        elif role_name == \"system\":\n",
      "            \n",
      "            # check if we are already embedded at the top of a user role\n",
      "            if self._current_prompt().endswith(\"[INST] \"):\n",
      "                return \"<<SYS>>\\n\"\n",
      "\n",
      "            # if not then we auto nest ourselves\n",
      "            else:\n",
      "                return \"[INST] <<SYS>>\\n\"\n",
      "    \n",
      "    def get_role_end(self, role_name=None):\n",
      "        if role_name == \"user\":\n",
      "            return \" [/INST]\"\n",
      "        elif role_name == \"assistant\":\n",
      "            return \" \"\n",
      "        elif role_name == \"system\":\n",
      "            return \"\\n<</SYS>>\\n\\n\"\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/models/llama_cpp/__init__.py\n",
      "```py\n",
      "from ._llama_cpp import LlamaCpp, LlamaCppChat\n",
      "from ._mistral import MistralChat, MistralInstruct\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/_cpp/byte_trie.cpp\n",
      "```cpp\n",
      "#include <pybind11/stl.h>\n",
      "\n",
      "class ByteTrie : public std::enable_shared_from_this<ByteTrie> { // enable_shared_from_this allows use to return a raw pointer to the parent\n",
      "private:\n",
      "    ByteTrie* _parent = nullptr; // Raw pointer since we are not owning the parent\n",
      "\n",
      "public:\n",
      "    int match_version = -1;\n",
      "    bool match = false;\n",
      "    bool partial_match = false;\n",
      "    double prob = 0;\n",
      "    int value = -1;\n",
      "    std::unordered_map<char, std::shared_ptr<ByteTrie>> children;\n",
      "\n",
      "    ByteTrie(std::vector<std::string> byte_strings) {\n",
      "        for (size_t i = 0; i < byte_strings.size(); ++i) {\n",
      "            insert(byte_strings[i], 0);\n",
      "        }\n",
      "    }\n",
      "\n",
      "    ByteTrie(std::vector<std::string> byte_strings, std::vector<int> values) {\n",
      "        for (size_t i = 0; i < byte_strings.size(); ++i) {\n",
      "            insert(byte_strings[i], values[i]);\n",
      "        }\n",
      "    }\n",
      "\n",
      "    ByteTrie(ByteTrie* parent) : _parent(parent) {}\n",
      "\n",
      "    std::vector<char> keys() const {\n",
      "        std::vector<char> keys;\n",
      "        for (const auto& pair : children) {\n",
      "            keys.push_back(pair.first);\n",
      "        }\n",
      "        return keys;\n",
      "    }\n",
      "\n",
      "    bool has_child(char byte) {\n",
      "        return children.count(byte) > 0;\n",
      "    }\n",
      "\n",
      "    std::shared_ptr<ByteTrie> child(char byte) {\n",
      "        return children[byte];\n",
      "    }\n",
      "\n",
      "    ByteTrie *parent() {\n",
      "        return this->_parent;\n",
      "    }\n",
      "\n",
      "    size_t size() {\n",
      "        return children.size();\n",
      "    }\n",
      "\n",
      "    void insert(const std::string& s, int value, unsigned int pos = 0) {\n",
      "        if (s.size() <= pos) {\n",
      "            if (this->value < 0) {\n",
      "                this->value = value;\n",
      "            }\n",
      "        } else {\n",
      "            uint8_t first_byte = s[pos];\n",
      "            if (children.find(first_byte) == children.end()) {\n",
      "                children[first_byte] = std::make_shared<ByteTrie>(this);\n",
      "            }\n",
      "            children[first_byte]->insert(s, value, pos + 1);\n",
      "        }\n",
      "    }\n",
      "\n",
      "    // we could save a lot of work if we assume the top node has prob 1.0 and then only explore the subtree we care about\n",
      "    void compute_probs(const std::vector<double>& probs) {\n",
      "        prob = 0.0;\n",
      "        \n",
      "        if (value != -1) {\n",
      "            prob += probs[value];\n",
      "        }\n",
      "\n",
      "        if (!children.empty()) {\n",
      "            for (auto& pair : children) {\n",
      "                pair.second->compute_probs(probs);\n",
      "                prob += pair.second->prob;\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "};\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/_cpp/__init__.py\n",
      "```py\n",
      "from .byte_trie import ByteTrie\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/_cpp/main.cpp\n",
      "```cpp\n",
      "#include <pybind11/pybind11.h>\n",
      "#include <pybind11/stl.h>\n",
      "// #include <pybind11/pytypes.h>\n",
      "// #include <pybind11/complex.h>\n",
      "// #include <pybind11/functional.h>\n",
      "// #include <pybind11/chrono.h>\n",
      "#include <any>\n",
      "#include \"byte_trie.cpp\"\n",
      "\n",
      "namespace py = pybind11;\n",
      "\n",
      "PYBIND11_MODULE(cpp, m) {\n",
      "    m.doc() = \"Performance sensitive parts of guidance that have been written in C++.\";\n",
      "\n",
      "    py::class_<ByteTrie, std::shared_ptr<ByteTrie>>(m, \"ByteTrie\")\n",
      "        .def(py::init<std::vector<std::string>>())\n",
      "        .def(py::init<std::vector<std::string>, std::vector<int>>())\n",
      "        .def(\"insert\", &ByteTrie::insert)\n",
      "        .def(\"has_child\", &ByteTrie::has_child)\n",
      "        .def(\"child\", &ByteTrie::child)\n",
      "        .def(\"parent\", &ByteTrie::parent)\n",
      "        .def(\"__len__\", &ByteTrie::size) \n",
      "        .def(\"keys\", [](const ByteTrie& self) {\n",
      "            auto byte_strings = self.keys();\n",
      "            py::list py_byte_strings;\n",
      "            for (size_t i = 0; i < byte_strings.size(); i++) {\n",
      "                py_byte_strings.append(py::bytes(&byte_strings[i], 1));\n",
      "            }\n",
      "            return py_byte_strings;\n",
      "        })\n",
      "        .def(\"compute_probs\", &ByteTrie::compute_probs)\n",
      "        .def_readwrite(\"match_version\", &ByteTrie::match_version)\n",
      "        .def_readwrite(\"match\", &ByteTrie::match)\n",
      "        .def_readwrite(\"partial_match\", &ByteTrie::partial_match)\n",
      "        .def_readwrite(\"prob\", &ByteTrie::prob)\n",
      "        .def_readwrite(\"value\", &ByteTrie::value)\n",
      "        .def_readwrite(\"children\", &ByteTrie::children);\n",
      "}\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/guidance/_cpp/byte_trie.py\n",
      "```py\n",
      "\n",
      "\n",
      "class ByteTrie:\n",
      "    \"\"\"A python implementation mirroring the C++ ByteTrie class.\"\"\"\n",
      "    def __init__(self, byte_strings=None, values=None, parent=None):\n",
      "        self._parent = parent\n",
      "        self.match_version = -1\n",
      "        self.match = False\n",
      "        self.partial_match = False\n",
      "        self.prob = 0\n",
      "        self.value = -1\n",
      "        self.children = {}\n",
      "\n",
      "        if byte_strings is not None:\n",
      "            if values is None:\n",
      "                for s in byte_strings:\n",
      "                    self.insert(s, 0)\n",
      "            else:\n",
      "                for i,s in enumerate(byte_strings):\n",
      "                    self.insert(s, values[i])\n",
      "\n",
      "    def keys(self):\n",
      "        return self.children.keys()\n",
      "    \n",
      "    def has_child(self, byte):\n",
      "        return byte in self.children\n",
      "\n",
      "    def child(self, byte):\n",
      "        return self.children[byte]\n",
      "    \n",
      "    def parent(self):\n",
      "        return self._parent\n",
      "\n",
      "    def size(self):\n",
      "        return len(self.children)\n",
      "    def __len__(self):\n",
      "        return self.size()\n",
      "    \n",
      "    def insert(self, s, value, pos=0):\n",
      "        if len(s) <= pos:\n",
      "            if self.value < 0:\n",
      "                self.value = value\n",
      "        else:\n",
      "            first_byte = s[pos:pos+1]\n",
      "            if first_byte not in self.children:\n",
      "                self.children[first_byte] = ByteTrie(parent=self)\n",
      "            self.children[first_byte].insert(s, value, pos + 1)\n",
      "\n",
      "    def compute_probs(self, probs):\n",
      "        self.prob = 0.0\n",
      "        \n",
      "        if self.value != -1:\n",
      "            self.prob += probs[self.value]\n",
      "\n",
      "        if self.children:\n",
      "            for k in self.children:\n",
      "                child = self.children[k]\n",
      "                child.compute_probs(probs)\n",
      "                self.prob += child.prob\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/README.md\n",
      "```md\n",
      "<div align=\"right\"><a href=\"https://guidance.readthedocs.org\"><img src=\"https://readthedocs.org/projects/guidance/badge/?version=latest&style=flat\" /></a></div>\n",
      "<div align=\"center\"><picture>\n",
      "  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/figures/guidance_logo_blue_dark.svg\">\n",
      "  <img alt=\"guidance\" src=\"docs/figures/guidance_logo_blue.svg\" width=300\">\n",
      "</picture></div>\n",
      "<br/>\n",
      "\n",
      "\n",
      "> *Note that v0.1 is a dramatically new version developed while releases had to be paused over the summer. If you are looking for the old version based on handlebars, you can use v0.0.64, but you should instead try porting over to the much better new version :)*\n",
      "\n",
      "**`guidance`** is a programming paradigm that offers superior control and efficiency compared to conventional prompting and chaining. It allows users to constrain generation (e.g. with regex and CFGs) as well as to interleave control (conditional, loops) and generation seamlessly. Here are some important features: \n",
      "\n",
      "1. **Pure, beautiful python** with additional LM functionality. E.g. here is [basic generation](#basic-generation):\n",
      "```python\n",
      "from guidance import models, gen\n",
      "\n",
      "# load a model (could be Transformers, LlamaCpp, VertexAI, OpenAI...)\n",
      "llama2 = models.LlamaCpp(path) \n",
      "\n",
      "# append text or generations to the model\n",
      "llama2 + f'Do you want a joke or a poem? ' + gen(stop='.')\n",
      "```\n",
      "<img alt=\"Do you want a joke or a poem? I'll give you a poem\" src=\"docs/figures/simple_gen_llama2_7b.png\" width=\"354\">\n",
      "\n",
      "2. [**Constrained generation**](#constrained-generation) with [selects](#select-basic), [regular expressions](#regular-expressions), and [context-free grammars](#context-free-grammars).\n",
      "```python\n",
      "from guidance import select\n",
      "\n",
      "# a simple select between two options\n",
      "llama2 + f'Do you want a joke or a poem? A ' + select(['joke', 'poem'])\n",
      "```\n",
      "<img alt=\"Do you want a joke or a poem? A poem\" src=\"docs/figures/simple_select_llama2_7b.png\" width=\"277\">\n",
      "\n",
      "3. **Rich templates with f-strings**:\n",
      "```python\n",
      "llama2 + f'''\\\n",
      "Do you want a joke or a poem? A {select(['joke', 'poem'])}.\n",
      "Okay, here is a one-liner: \"{gen(stop='\"')}\"\n",
      "'''\n",
      "```\n",
      "<img width=\"358\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/486ca968-89b1-4c02-b914-3b9714fe5890\"><br>\n",
      "\n",
      "4. [**Stateful control + generation**](#stateful-control--generation) makes it easy to interleave prompting / logic / generation, no need for intermediate parsers:\n",
      "```python\n",
      "# capture our selection under the name 'answer'\n",
      "lm = llama2 + f\"Do you want a joke or a poem? A {select(['joke', 'poem'], name='answer')}.\\n\"\n",
      "\n",
      "# make a choice based on the model's previous selection\n",
      "if lm[\"answer\"] == \"joke\":\n",
      "    lm += f\"Here is a one-line joke about cats: \" + gen('output', stop='\\n')\n",
      "else:\n",
      "    lm += f\"Here is a one-line poem about dogs: \" + gen('output', stop='\\n')\n",
      "```\n",
      "<img width=\"393\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/66d47ce7-1d5a-4dbd-b676-66b9c1094184\"><br>\n",
      "\n",
      "\n",
      "5. **Abstract chat interface** that uses the correct special tokens for any chat model:\n",
      "```python\n",
      "from guidance import user, assistant\n",
      "\n",
      "# load a chat model\n",
      "chat_lm = models.LlamaCppChat(path)\n",
      "\n",
      "# wrap with chat block contexts\n",
      "with user():\n",
      "    lm = chat_lm + 'Do you want a joke or a poem?'\n",
      "\n",
      "with assistant():\n",
      "    lm += f\"A {select(['joke', 'poem'])}.\"`\n",
      "```\n",
      "<img width=\"331\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/89c3e0e2-ed0a-4715-8366-2efca74b7b71\"><br>\n",
      "\n",
      "6. **Easy to write reusable components**\n",
      "```python\n",
      "import guidance\n",
      "\n",
      "@guidance\n",
      "def one_line_thing(lm, thing, topic):\n",
      "    lm += f'Here is a one-line {thing} about {topic}: ' + gen(stop='\\n')\n",
      "    return lm # return our updated model\n",
      "\n",
      "# pick either a joke or a poem\n",
      "lm = llama2 + f\"Do you want a joke or a poem? A {select(['joke', 'poem'], name='thing')}.\\n\"\n",
      "\n",
      "# call our guidance function\n",
      "lm += one_line_thing(lm['thing'], 'cats')\n",
      "```\n",
      "<img width=\"386\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/60071680-8bbb-4fa5-a298-613d4fd55fa7\"><br>\n",
      "\n",
      "7. **A library of pre-built components**, e.g. substring:\n",
      "```python\n",
      "from guidance import substring\n",
      "\n",
      "# define a set of possible statements\n",
      "text = 'guidance is awesome. guidance is so great. guidance is the best thing since sliced bread.'\n",
      "\n",
      "# force the model to make an exact quote\n",
      "llama2 + f'Here is a true statement about the guidance library: \"{substring(text)}\"'\n",
      "```\n",
      "<img width=\"589\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/9a7178ad-ed73-4e6b-b418-f9d2a3a76b88\"><br>\n",
      "\n",
      "8. [**Easy tool use**](#automatic-interleaving-of-control-and-generation-tool-use), where the model stops generation when a tool is called, calls the tool, then resumes generation. For example, here is a simple version of a calculator, via four separate 'tools':\n",
      "```python\n",
      "@guidance\n",
      "def add(lm, input1, input2):\n",
      "    lm += f' = {int(input1) + int(input2)}'\n",
      "    return lm\n",
      "@guidance\n",
      "def subtract(lm, input1, input2):\n",
      "    lm += f' = {int(input1) - int(input2)}'\n",
      "    return lm\n",
      "@guidance\n",
      "def multiply(lm, input1, input2):\n",
      "    lm += f' = {float(input1) * float(input2)}'\n",
      "    return lm\n",
      "@guidance\n",
      "def divide(lm, input1, input2):\n",
      "    lm += f' = {float(input1) / float(input2)}'\n",
      "    return lm\n",
      "```\n",
      "Now we call `gen` with these tools as options. Notice how generation is stopped and restarted automatically:\n",
      "```python\n",
      "lm = llama2 + '''\\\n",
      "1 + 1 = add(1, 1) = 2\n",
      "2 - 3 = subtract(2, 3) = -1\n",
      "'''\n",
      "lm + gen(max_tokens=15, tools=[add, subtract, multiply, divide])\n",
      "```\n",
      "<img width=\"201\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/646e1a7d-0206-419b-8206-1d835c3a0e0a\"><br>\n",
      "\n",
      "9. **Speed**: In contrast to chaining, `guidance` programs are the equivalent of a single LLM call. More so, whatever non-generated text that gets appended is batched, so that `guidance` programs are **faster** than having the LM generate intermediate text when you have a set structure.\n",
      "\n",
      "10. **Token healing**: Users deal with text (or bytes) rather than tokens, and thus don't have to worry about [perverse token boundaries issues](https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38) such as 'prompt ending in whitespace'.\n",
      "\n",
      "11. **Streaming support**, also integrated with jupyter notebooks:\n",
      "```python\n",
      "lm = llama2 + 'Here is a cute 5-line poem about cats and dogs:\\n'\n",
      "for i in range(5):\n",
      "    lm += f\"LINE {i+1}: \" + gen(temperature=0.8, suffix=\"\\n\")\n",
      "```\n",
      "<img src=\"docs/figures/simple_streaming_example.gif\" width=\"337\">\n",
      "\n",
      "13. **High compatibility:** works with Transformers, llama.cpp, VertexAI, OpenAI. Users can write one guidance program and execute it on many backends. (note that the most powerful control features require endpoint integration, and for now work best with Transformers and llama.cpp).\n",
      "```python\n",
      "gpt = models.OpenAI(\"gpt-3.5-turbo\")\n",
      "\n",
      "with user():\n",
      "    lm = gpt + \"What is the capital of France?\"\n",
      "\n",
      "with assistant():\n",
      "    lm += gen(\"capital\")\n",
      "\n",
      "with user():\n",
      "    lm += \"What is one short surprising fact about it?\"\n",
      "\n",
      "with assistant():\n",
      "    lm += gen(\"fact\")\n",
      "```\n",
      "<img width=\"645\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/f31ed7b8-1868-44d2-b14c-4842b0a40e5c\"><br>\n",
      "\n",
      "14. **Multi-modal support.**\n",
      "```python\n",
      "from guidance import image\n",
      "\n",
      "gemini = models.VertexAI(\"gemini-pro-vision\")\n",
      "\n",
      "with user():\n",
      "    lm = gemini + \"What is this a picture of?\" + image(\"longs_peak.jpg\")\n",
      "\n",
      "with assistant():\n",
      "    lm += gen(\"answer\")\n",
      "```\n",
      "<img width=\"673\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/6450d05d-52e9-4ef5-b280-8b57e733d46d\">\n",
      "\n",
      "\n",
      "\n",
      "## Table of Contents\n",
      "   * [Install](#install)\n",
      "   * [Loading models](#loading-models)\n",
      "      * [llama.cpp](#llamacpp)\n",
      "      * [transformers](#transformers)\n",
      "      * [Vertex](#vertex-ai)\n",
      "      * [OpenAI](#openai)\n",
      "   * [Example notebooks](#example-notebooks)\n",
      "   * [Basic generation](#basic-generation)\n",
      "   * [Constrained Generation](#constrained-generation)\n",
      "      * [Select (basic)](#select-basic)\n",
      "      * [Regular expressions](#regular-expressions)\n",
      "         * [Regex to constrain generation](#regex-to-constrain-generation)\n",
      "         * [Regex as stopping criterion](#regex-as-stopping-criterion)\n",
      "      * [Context-free grammars](#context-free-grammars)\n",
      "   * [Stateful control + generation](#stateful-control--generation)\n",
      "      * [State in immutable objects](#state-in-immutable-objects)\n",
      "      * [Stateful guidance functions](#stateful-guidance-functions)\n",
      "      * [Example: ReAct](#example-react)\n",
      "      * [Example: Changing intermediate step of a Chat session](#example-changing-intermediate-step-of-a-chat-session)\n",
      "      * [Automatic interleaving of control and generation: tool use](#automatic-interleaving-of-control-and-generation-tool-use)\n",
      "      * [Gsm8k example](#gsm8k-example)\n",
      "      * [Automatic call grammar for @guidance functions](#automatic-call-grammar-for-guidance-functions)\n",
      "   * [Text, not tokens](#text-not-tokens)\n",
      "   * [Fast](#fast)\n",
      "      * [Integrated stateful control is faster](#integrated-stateful-control-is-faster)\n",
      "      * [Guidance acceleration](#guidance-acceleration)\n",
      "\n",
      "## Install\n",
      "```bash\n",
      "pip install guidance\n",
      "```\n",
      "## Loading models\n",
      "### llama.cpp\n",
      "Install the python bindings:\n",
      "```bash\n",
      "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
      "```\n",
      "Loading the model:\n",
      "```python\n",
      "from guidance import models\n",
      "lm = models.LlamaCpp(path_to_model, n_gpu_layers=-1)\n",
      "```\n",
      "\n",
      "### Transformers\n",
      "Install transformers:\n",
      "```python\n",
      "from guidance import models\n",
      "lm = models.Transformers(model_name_or_path)\n",
      "```\n",
      "\n",
      "### Vertex AI\n",
      "Remote endpoints that don't have explicit guidance integration are run \"optimistically\". This means that all the text that can be forced is given to the model as a prompt (or chat context) and then the model is run in streaming mode without hard constrants (since the remote API doesn't support them). If the model ever violates the contraints then the model stream is stopped and we optionally try it again at that point. This means that all the API-supported control work as expected, and more complex controls/parsing that is not supported by the API work if the model stays consistent with the program.\n",
      "```python\n",
      "palm2 = models.VertexAI(\"text-bison@001\")\n",
      "\n",
      "with instruction():\n",
      "    lm = palm2 + \"What is one funny fact about Seattle?\"\n",
      "\n",
      "lm + gen(\"fact\", max_tokens=100)\n",
      "```\n",
      "<img width=\"635\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/693ae08f-68f7-4368-bd25-19afc9bfc0a5\"><br>\n",
      "\n",
      "### OpenAI\n",
      "OpenAI endpoint don't have direct support for guidance grammars, but through optimistic running we can still control them in ways that match the model type:\n",
      "\n",
      "*Legacy completion models:*\n",
      "```python\n",
      "curie = models.OpenAI(\"text-curie-001\")\n",
      "\n",
      "curie + \"The smallest cats are\" + gen(stop=\".\")\n",
      "```\n",
      "<img width=\"263\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/116a906c-ea77-4a13-a83a-682029d5e5c8\"><br>\n",
      "\n",
      "*Instruct tuned models:*\n",
      "```python\n",
      "gpt_instruct = models.OpenAI(\"gpt-3.5-turbo-instruct\")\n",
      "\n",
      "with instruction():\n",
      "    lm = gpt_instruct + \"What are the smallest cats?\"\n",
      "    \n",
      "lm += gen(stop=\".\")\n",
      "```\n",
      "<img width=\"574\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/56a53ce1-89f5-4e9d-bdb8-86fb3eebf309\"><br>\n",
      "\n",
      "*Chat models:*\n",
      "```python\n",
      "gpt = models.OpenAI(\"gpt-3.5-turbo\")\n",
      "\n",
      "with system():\n",
      "    lm = gpt + \"You are a cat expert.\"\n",
      "\n",
      "with user():\n",
      "    lm += \"What are the smallest cats?\"\n",
      "\n",
      "with assistant():\n",
      "    lm += gen(\"answer\", stop=\".\")\n",
      "```\n",
      "<img width=\"367\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/46102f0f-37dc-4bb1-99b7-e5895bdee772\"><br>\n",
      "\n",
      "\n",
      "\n",
      "## Example notebooks\n",
      "We are working on updating our example notebooks. The following ones have been updated:\n",
      "- [Basic tutorial](notebooks/tutorials/intro_to_guidance.ipynb)\n",
      "- [Chatbot with search](notebooks/chat_with_search.ipynb)  \n",
      "\n",
      "More coming soon\n",
      "\n",
      "## Basic generation\n",
      "An `lm` object is immutable, so you change it by creating new copies of it. By default, when you append things to `lm`, it creates a copy, e.g.:\n",
      "```python\n",
      "from guidance import models, gen, select\n",
      "llama2 = models.LlamaCpp(model)\n",
      "\n",
      "# llama2 is not modified, `lm` is a copy of `llama2` with 'This is a prompt' appended to its state\n",
      "lm = llama2 + 'This is a prompt'\n",
      "```\n",
      "<img width=\"124\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/c1e96b2b-8f4a-44ee-a8f4-a694a8d7784b\"><br>\n",
      "\n",
      "You can append _generation_ calls to model objects, e.g.\n",
      "```python\n",
      "lm = llama2 + 'This is a prompt' + gen(max_tokens=10)\n",
      "```\n",
      "<img width=\"267\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/d2e5ed34-ba9d-4bdd-872d-2b76f8e3cf85\"><br>\n",
      "\n",
      "You can also interleave generation calls with plain text, or control flows:\n",
      "```python\n",
      "# Note how we set stop tokens\n",
      "lm = llama2 + 'I like to play with my ' + gen(stop=' ') + ' in' + gen(stop=['\\n', '.', '!'])\n",
      "```\n",
      "<img width=\"279\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/2d47fd65-1982-4dd8-9ba9-a01e62fba455\"><br>\n",
      "\n",
      "## Constrained Generation\n",
      "### Select (basic)\n",
      "`select` constrains generation to a set of options:\n",
      "```python\n",
      "lm = llama2 + 'I like the color ' + select(['red', 'blue', 'green'])\n",
      "```\n",
      "<img width=\"137\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/f0b97629-78a9-439d-90b2-06af31fdc40e\"><br>\n",
      "\n",
      "### Regular expressions\n",
      "`gen` has optional arguments `regex` and `stop_regex`, which allow generation (and stopping, respectively) to be controlled by a regex. \n",
      "\n",
      "#### Regex to constrain generation\n",
      "Unconstrained:\n",
      "\n",
      "```python\n",
      "lm = llama2 + 'Question: Luke has ten balls. He gives three to his brother.\\n'\n",
      "lm += 'How many balls does he have left?\\n'\n",
      "lm += 'Answer: ' + gen(stop='\\n')\n",
      "```\n",
      "<img width=\"405\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/55fb66ea-a717-417a-8a70-14c46eba4c66\"><br>\n",
      "\n",
      "Constrained by regex:\n",
      "\n",
      "```python\n",
      "lm = llama2 + 'Question: Luke has ten balls. He gives three to his brother.\\n'\n",
      "lm += 'How many balls does he have left?\\n'\n",
      "lm += 'Answer: ' + gen(regex='\\d+')\n",
      "```\n",
      "<img width=\"404\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/b45a5a79-55e0-4c15-884a-fba830c0a153\"><br>\n",
      "\n",
      "\n",
      "#### Regex as stopping criterion\n",
      "Unconstrained:\n",
      "```python\n",
      "lm = llama2 + '19, 18,' + gen(max_tokens=50)\n",
      "```\n",
      "<img width=\"359\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/5dd13454-cc42-4e27-a52c-19a31237891c\"><br>\n",
      "\n",
      "Stop with traditional stop text, whenever the model generates the number 7:\n",
      "```python\n",
      "lm = llama2 + '19, 18,' + gen(max_tokens=50, stop='7')\n",
      "```\n",
      "<img width=\"73\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/fc96d7c3-381d-4766-8bee-c930669f518a\"><br>\n",
      "\n",
      " \n",
      "Stop whenever the model generates the character `7` without any numbers around it: \n",
      "```python\n",
      "lm = llama2 + '19, 18,' + gen(max_tokens=50, stop_regex='[^\\d]7[^\\d]')\n",
      "```\n",
      "<img width=\"293\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/a657e566-b1a4-447a-82a5-b88977b5fedf\"><br>\n",
      "\n",
      "\n",
      "### Context-free grammars\n",
      "We expose a variety of operators that make it easy to define CFGs, which in turn can be used to constrain generation.\n",
      "For example, we can use the `select` operator (it accepts CFGs as options), `zero_or_more` and `one_or_more` to define a grammar for mathematical expressions:\n",
      "```python\n",
      "import guidance\n",
      "from guidance import one_or_more, select, zero_or_more\n",
      "# stateless=True indicates this function does not depend on LLM generations\n",
      "@guidance(stateless=True)\n",
      "def number(lm):\n",
      "    n = one_or_more(select(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']))\n",
      "    # Allow for negative or positive numbers\n",
      "    return lm + select(['-' + n, n])\n",
      "\n",
      "@guidance(stateless=True)\n",
      "def operator(lm):\n",
      "    return lm + select(['+' , '*', '**', '/', '-'])\n",
      "\n",
      "@guidance(stateless=True)\n",
      "def expression(lm):\n",
      "    # Either\n",
      "    # 1. A number (terminal)\n",
      "    # 2. two expressions with an operator and optional whitespace\n",
      "    # 3. An expression with parentheses around it\n",
      "    return lm + select([\n",
      "        number(),\n",
      "        expression() + zero_or_more(' ') +  operator() + zero_or_more(' ') +  expression(),\n",
      "        '(' + expression() + ')'\n",
      "    ])\n",
      "```\n",
      "\n",
      "The `@guidance(stateless=True)` decorator makes it such that a function (e.g. `expression`) lives as a stateless grammar that does not get 'executed' until we call call `lm + expression()` or `lm += expression()`. For example, here is an example of _unconstrained_ generation:\n",
      "```python\n",
      "# Without constraints\n",
      "lm = llama2 + 'Problem: Luke has a hundred and six balls. He then loses thirty six.\\n'\n",
      "lm += 'Equivalent arithmetic expression: ' + gen(stop='\\n') + '\\n'\n",
      "```\n",
      "<img width=\"462\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/54af1909-cad4-4fb1-8987-dfdfc02f8f42\"><br>\n",
      "\n",
      "Notice how the model wrote the right equation but solved it (incorrectly). If we wanted to constrain the model such that it only writes valid expressions (without trying to solve them), we can just append our grammar to it:\n",
      "```python\n",
      "grammar = expression()\n",
      "lm = llama2 + 'Problem: Luke has a hundred and six balls. He then loses thirty six.\\n'\n",
      "lm += 'Equivalent arithmetic expression: ' + grammar + '\\n'\n",
      "```\n",
      "<img width=\"460\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/dbda0ff8-8edd-4384-b63d-fc98792e0689\"><br>\n",
      "\n",
      "Grammars are very easy to compose. For example, let's say we want a grammar that generates either a mathematical expression or an expression followed by a solution followed by another expression. Creating this grammar is easy:\n",
      "\n",
      "```python\n",
      "from guidance import regex\n",
      "grammar = select([expression(), expression() +  regex(' = \\d+; ') + expression()])\n",
      "```\n",
      "We can generate according to it:\n",
      "```python\n",
      "llama2 + 'Here is a math expression for two plus two: ' + grammar\n",
      "```\n",
      "<img width=\"346\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/283e6973-0b8d-4153-a82b-9f5db1460da9\"><br>\n",
      "\n",
      "```python\n",
      "llama2 + '2 + 2 = 4; 3+3\\n' + grammar\n",
      "```\n",
      "<img width=\"109\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/d584a93c-bf24-43d5-8f8d-501e7eb88422\"><br>\n",
      "\n",
      "Even if you don't like thinking in terms of recursive grammars, this formalism makes it easy to constrain generation. For example, let's say we have the following one-shot prompt:\n",
      "```python\n",
      "@guidance(stateless=True)\n",
      "def ner_instruction(lm, input):\n",
      "    lm += f'''\\\n",
      "    Please tag each word in the input with PER, ORG, LOC, or nothing\n",
      "    ---\n",
      "    Input: John worked at Apple.\n",
      "    Output:\n",
      "    John: PER\n",
      "    worked: \n",
      "    at: \n",
      "    Apple: ORG\n",
      "    .: \n",
      "    ---\n",
      "    Input: {input}\n",
      "    Output:\n",
      "    '''\n",
      "    return lm\n",
      "input = 'Julia never went to Morocco in her life!!'\n",
      "llama2 + ner_instruction(input) + gen(stop='---')\n",
      "```\n",
      "<img width=\"465\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/8ecf5ad4-68b8-4e7a-b107-b1a5613e4c68\"><br>\n",
      "\n",
      "Notice that the model did not spell the word 'Morocco' correctly. Sometimes the model might also hallucinate a tag that doesn't exist. We can improve this by adding more few-shot examples, etc, but we can also constrain generation to the exact format we want:\n",
      "```python\n",
      "import re\n",
      "\n",
      "@guidance(stateless=True)\n",
      "def constrained_ner(lm, input):\n",
      "    # Split into words\n",
      "    words = [x for x in re.split('([^a-zA-Z0-9])', input) if x and not re.match('\\s', x)]\n",
      "    ret = ''\n",
      "    for x in words:\n",
      "        ret += x + ': ' + select(['PER', 'ORG', 'LOC', '']) + '\\n'\n",
      "    return lm + ret\n",
      "llama2 + ner_instruction(input) + constrained_ner(input)\n",
      "```\n",
      "<img width=\"462\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/72545093-ef16-479a-b666-bd97c54a5dc7\">\n",
      "\n",
      "While `constrained_ner(input)` **is** a grammar that constrains the model generation, it _feels_ like you're just writing normal imperative python code with `+=` and `selects`.\n",
      "\n",
      "\n",
      "## Stateful control + generation\n",
      "### State in immutable objects\n",
      "Whenever you do `lm + grammar` or `lm + gen`, `lm + select`, etc, you return a new lm object with additional state. For example:\n",
      "\n",
      "```python\n",
      "lm = llama2 + 'This is a prompt' + gen(name='test', max_tokens=10)\n",
      "lm += select(['this', 'that'], name='test2')\n",
      "lm['test'], lm['test2']\n",
      "```\n",
      "<img width=\"296\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/f0f9d180-6209-40df-9401-40da35d46e1a\"><br>\n",
      "\n",
      "### Stateful `guidance` functions\n",
      "The guidance decorator is `@guidance(stateless=False)` by default, meaning that a function with this decorator depends on the lm state to execute (either prior state or state generated within the function). For example:\n",
      "```python\n",
      "@guidance(stateless=False)\n",
      "def test(lm):\n",
      "    lm += 'Should I say \"Scott\"?\\n' + select(['yes', 'no'], name='answer') + '\\n'\n",
      "    if lm['answer'] == 'yes':\n",
      "        lm += 'Scott'\n",
      "    else:\n",
      "        lm += 'Not Scott'\n",
      "    return lm\n",
      "llama2 + test()\n",
      "```\n",
      "<img width=\"159\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/5a55496b-aea0-46e9-8de6-b63655027653\"><br>\n",
      "\n",
      "\n",
      "### Example: ReAct\n",
      "A big advantage of stateful control is that you don't have to write any intermediate parsers, and adding follow-up 'prompting' is easy, even if the follow up depends on what the model generates.\n",
      "For example, let's say we want to implement the first example of ReAct prompt in [this](https://www.promptingguide.ai/techniques/react), and let's say the valid acts are only 'Search' or 'Finish'. We might write it like this:\n",
      "```python\n",
      "@guidance\n",
      "def react_prompt_example(lm, question, max_rounds=10):\n",
      "    lm += f'Question: {question}\\n'\n",
      "    i = 1\n",
      "    while True:\n",
      "        lm += f'Thought {i}: ' + gen(suffix='\\n')\n",
      "        lm += f'Act {i}: ' + select(['Search', 'Finish'], name='act') \n",
      "        lm += '[' + gen(name='arg', suffix=']') + '\\n'\n",
      "        if lm['act'] == 'Finish' or i == max_rounds:\n",
      "            break\n",
      "        else:\n",
      "            lm += f'Observation {i}: ' + search(lm['arg']) + '\\n'\n",
      "        i += 1\n",
      "    return lm\n",
      "```\n",
      "Notice how we don't have to write a parser for Act and argument and hope that the model generates something valid: we enforce it. Notice also that the loop only stops once the model chooses to act with 'Finish' (or once we hit a maximum number of rounds).\n",
      "\n",
      "### Example: Changing intermediate step of a Chat session\n",
      "We can also hide or change some of what the model generates. For example, below we get a Chat model (notice we use special `role` blocks) to name some experts to answer a question, but we always remove 'Ferriss' from the list if he is mentioned:\n",
      "```python\n",
      "from guidance import user, system, assistant\n",
      "lm = llama2\n",
      "query = 'How can I be more productive?'\n",
      "with system():\n",
      "    lm += 'You are a helpful and terse assistant.'\n",
      "with user():\n",
      "    lm += f'I want a response to the following question:\\n{query}\\n'\n",
      "    lm += 'Name 3 world-class experts (past or present) who would be great at answering this.'\n",
      "with assistant():\n",
      "    temp_lm = lm\n",
      "    for i in range(1, 4):\n",
      "        # This regex only allows strings that look like names (where every word is capitalized)\n",
      "        # list_append appends the result to a list\n",
      "        temp_lm += f'{i}. ' + gen(regex='([A-Z][a-z]*\\s*)+', suffix='\\n',\n",
      "                                  name='experts', list_append=True)\n",
      "    experts = [x for x in temp_lm['experts'] if 'Ferriss' not in x]\n",
      "    # Notice that even if the model generates 'Ferriss' above,\n",
      "    # it doesn't get added to `lm`, only to `temp_lm`\n",
      "    lm += ', '.join(experts)\n",
      "with user():\n",
      "    lm += 'Please answer the question as if these experts had collaborated in writing an anonymous answer.'\n",
      "with assistant():\n",
      "    lm += gen(max_tokens=100)\n",
      "```\n",
      "<img width=\"688\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/d274f8b8-52e7-41a5-9635-b34f70ed50e0\"><br>\n",
      "\n",
      "### Automatic interleaving of control and generation: tool use\n",
      "Tool use is a common case of stateful control. To make it easy to do so, `gen` calls take `tools` as an optional argument, where each tool is defined by (1) a grammar that triggers its call and captures the arguments (if any), and (2) the actual tool call. Then, as generation unrolls, whenever the model generates something that matches the grammar of a tool call, it (1) stops generation, (2) calls the tool (which can append whatever it wants to the LM session), and (3) continues generation.\n",
      "\n",
      "For example, here is how we might implement a calculator tool, leveraging our `expression` grammar above:\n",
      "```python\n",
      "from guidance import capture, Tool\n",
      "@guidance(stateless=True)\n",
      "def calculator_call(lm):\n",
      "    # capture just 'names' the expression, to be saved in the LM state\n",
      "    return lm + 'calculator(' + capture(expression(), 'tool_args') + ')'\n",
      "\n",
      "@guidance\n",
      "def calculator(lm):\n",
      "    expression = lm['tool_args']\n",
      "    # You typically don't want to run eval directly for save reasons\n",
      "    # Here we are guaranteed to only have mathematical expressions\n",
      "    lm += f' = {eval(expression)}'\n",
      "    return lm\n",
      "calculator_tool = Tool(calculator_call(), calculator)\n",
      "lm = llama2 + 'Here are five expressions:\\ncalculator(3 *3) = 33\\ncalculator(2 + 1 * 3) = 5\\n'\n",
      "lm += gen(max_tokens=30, tools=[calculator_tool], stop='\\n\\n')\n",
      "```\n",
      "<img width=\"201\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/2d9b840a-4fad-4dab-b3e7-20887539b447\"><br>\n",
      "\n",
      "\n",
      "### Gsm8k example\n",
      "Notice that the calculator is just called seamlessly during generation. Here is a more realistic exampe of the model solving a gsm8k question:\n",
      "\n",
      "```python\n",
      "@guidance\n",
      "def math_with_calc(lm, question):\n",
      "    # Two-shot example\n",
      "    lm += '''\\\n",
      "    Question: John starts with 2 balls. He then quintupled his number of balls. Then he lost half of them. He then gave 3 to his brother. How many does he have left?\n",
      "    Reasoning:\n",
      "    1. He quintupled his balls. So he has calculator(2 * 5) = 10 balls.\n",
      "    1. He lost half. So he has calculator(10 / 2) = 5 balls.\n",
      "    3. He gave 3 to his brother. So he has calculator(5 - 3) = 2 balls.\n",
      "    Answer: 2\n",
      "\n",
      "    Question: Jill get 7 dollars a day in allowance. She uses 1 each day to by a bus pass, then gives half away. How much does she have left each day?\n",
      "    Reasoning:\n",
      "    1. She gets 7 dollars a day.\n",
      "    1. She spends 1 on a bus pass. So she has calculator(5 - 1) = 6.\n",
      "    3. She gives half away. So that makes calculator(6 / 2) = 3.\n",
      "    Answer: 3\n",
      "\n",
      "    '''\n",
      "    lm += f'Question: {question}\\n'\n",
      "    lm += 'Reasoning:\\n' + gen(max_tokens=200, tools=[calculator_tool], stop='Answer')\n",
      "    # Only numbers or commas\n",
      "    lm += 'Answer: ' + gen(regex='[-\\d,]+')\n",
      "    return lm\n",
      "\n",
      "question = '''Janets ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?'''\n",
      "llama2 + math_with_calc(question)\n",
      "```\n",
      "<img width=\"685\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/0c7b8da0-b295-46cd-a312-604ecfba7b33\"><br>\n",
      "\n",
      "### Automatic call grammar for @guidance functions\n",
      "You can also initialize a `Tool` with any `@guidance`-decorated function, and the default call grammar will be like a python call. Here is an example of using multiple such tools in the same `gen` call:\n",
      "```python\n",
      "@guidance\n",
      "def say_scott(lm, n):\n",
      "    lm += '\\n'\n",
      "    for _ in range(int(n)):\n",
      "        lm += 'Scott\\n'\n",
      "    return lm\n",
      "\n",
      "@guidance\n",
      "def say_marco(lm, n):\n",
      "    lm += '\\n'\n",
      "    for _ in range(int(n)):\n",
      "        lm += 'marco\\n'\n",
      "    return lm\n",
      "\n",
      "tools = [Tool(callable=say_scott), Tool(callable=say_marco)]\n",
      "llama2 + '''\\\n",
      "I am going to call say_scott and say_marco a few times:\n",
      "say_scott(1)\n",
      "Scott\n",
      "''' + gen(max_tokens=20, tools=tools)\n",
      "```\n",
      "<img width=\"395\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/8025699b-59a1-4a3f-8b1e-a895a54924e2\"><br>\n",
      "\n",
      "\n",
      "## Text, not tokens\n",
      "The standard greedy tokenizations used by most language models introduce a variety of subtle and powerful biases, which that can have all kinds of unintended consequences for your prompts.\n",
      "For example, take the following prompt, given to gpt-2 (standard greedy tokenization):\n",
      "\n",
      "hf_gen(prompt, max_tokens=10)\n",
      "```python\n",
      "from transformers import pipeline\n",
      "pipe = pipeline(\"text-generation\", model=\"gpt2\")\n",
      "def hf_gen(prompt, max_tokens=100):\n",
      "    return pipe(prompt, do_sample=False, max_length=max_tokens, return_full_text=False)[0]['generated_text']\n",
      "\n",
      "prompt = 'http:'\n",
      "hf_gen(prompt, max_tokens=10)\n",
      "```\n",
      "<img width=\"198\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/a0fe3e81-89e0-4b4a-8981-edf8b1a8a723\"><br>\n",
      "\n",
      " \n",
      " Notice how the output generated by the LLM does not complete the URL with the obvious next characters (two forward slashes). It instead creates an invalid URL string with a space in the middle. Why? Because the string `://` is its own token, and so once the model sees a colon by itself, it assumes that the next characters cannot be `//`; otherwise, the tokenizer would not have used `:`, and instead would have used `://`. This is why there are warnings about ending prompts in whitespace, but the problem is way more pervasive than that: any boundary that may span multiple tokens will cause problems, e.g. notice how a partial word causes incorrect completion:\n",
      "\n",
      " ```python\n",
      "prompt = 'John is a'\n",
      "hf_gen(prompt, max_tokens=5)\n",
      "```\n",
      "<img width=\"133\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/44906e57-c4ca-4dc3-a1c3-2fdba040259b\"><br>\n",
      "\n",
      "\n",
      " ```python\n",
      "prompt = 'John is a fo'\n",
      "hf_gen(prompt, max_tokens=5)\n",
      "```\n",
      "<img width=\"52\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/df649320-ec8e-468a-bb2f-e1994f16c9b6\"><br>\n",
      "\n",
      "While problematic enough for normal prompts, these problems would be a disaster in the kinds of prompts we wrote in this readme, where there is interleaving of prompting and generation happening multiple times (and thus multiple opportunities for problems). This is why `guidance` implements [token healing](https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38), a feature that deals with prompt boundaries automatically, allowing users to just think in terms of **text** rather than tokens. For example:\n",
      "\n",
      "```python\n",
      "from guidance import models\n",
      "gpt = models.Transformers('gpt2')\n",
      "prompt = 'http:'\n",
      "gpt + prompt + gen(max_tokens=10)\n",
      "```\n",
      "<img width=\"244\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/c9f26a58-52f2-457c-958a-e048f68eb388\"><br>\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "prompt = 'John is a fo'\n",
      "gpt + prompt + gen(max_tokens=2)\n",
      "```\n",
      "<img width=\"186\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/bc5e4cd4-9b82-4c09-9db2-9e890dad1d69\"><br>\n",
      "\n",
      "## Fast\n",
      "### Integrated stateful control is faster\n",
      "We have full control of the decoding loop in our integration with `transformers` and `llamacpp`, allowing us to add control and additional prompt without any extra cost.  \n",
      "If instead we're calling a server, we pay the extra cost of making additional requests, which might be ok if the server has caching, but quickly becomes impractical if the server does not have fine-grained caching. For example, note again the output from the [gsm8k example with calculator](#gsm8k-example) above:\n",
      "\n",
      "<img width=\"624\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/2c75b0f2-6997-43d9-b10e-cb9f6f2e2de5\">\n",
      "\n",
      "Every time we call `calculator`, we have to stop generation, append the result to the prompt, and resume generation. To avoid slowing down after the first call, a server would need to keep the KV cache up to '3 for breakfast. So she has calculator(16 - 3)', then roll forward generation from that point on. Even servers that _do_ have caching often don't have a way to guarantee state is preserved at each stop and start, and so user's pay a significant overhead at each interruption. The normal approach of considering everything as a new prompt would cause significant slow downs every time `calculator` is called.\n",
      "\n",
      "### Guidance acceleration\n",
      "In addition to the benefit above, `guidance` calls are often **faster** than running equivalent prompts the traditional way, because we can batch any additional text that is added by the user as execution unrolls (rather than generating it). Take the example below, where we generate a json with a GGUF compressed `llama2` 7B executed using llama.cpp:\n",
      "```python\n",
      "@guidance\n",
      "def character_maker(lm, id, description, valid_weapons):\n",
      "    lm += f\"\"\"\\\n",
      "    The following is a character profile for an RPG game in JSON format.\n",
      "    ```json\n",
      "    {{\n",
      "        \"id\": \"{id}\",\n",
      "        \"description\": \"{description}\",\n",
      "        \"name\": \"{gen('name', stop='\"')}\",\n",
      "        \"age\": {gen('age', regex='[0-9]+', stop=',')},\n",
      "        \"armor\": \"{select(options=['leather', 'chainmail', 'plate'], name='armor')}\",\n",
      "        \"weapon\": \"{select(options=valid_weapons, name='weapon')}\",\n",
      "        \"class\": \"{gen('class', stop='\"')}\",\n",
      "        \"mantra\": \"{gen('mantra', stop='\"')}\",\n",
      "        \"strength\": {gen('strength', regex='[0-9]+', stop=',')},\n",
      "        \"items\": [\"{gen('item', list_append=True, stop='\"')}\", \"{gen('item', list_append=True, stop='\"')}\", \"{gen('item', list_append=True, stop='\"')}\"]\n",
      "    }}```\"\"\"\n",
      "    return lm\n",
      "a = time.time()\n",
      "lm = llama2 + character_maker(1, 'A nimble fighter', ['axe', 'sword', 'bow'])\n",
      "time.time() - a\n",
      "```\n",
      "<img width=\"480\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/85b5a181-6e6a-4582-9203-730f49353aeb\"><br>\n",
      "\n",
      "Everything that is not green is not actually generated by the model, and is thus batched (much faster). This prompt takes about 1.2 seconds on an A100 GPU. Now, if we let the model generate everything (as in the roughly equivalent prompt below), it takes roughly `2.6` seconds (not only is it slower, we also have less control over generation). \n",
      "```python\n",
      "@guidance\n",
      "def character_maker2(lm, id, description):\n",
      "    lm += f\"\"\"\\\n",
      "    The following is a character profile for an RPG game in JSON format. It has fields 'id', 'description', 'name', 'age', 'armor', weapon', 'class', 'mantra', 'strength', and 'items (just the names of 3 items)'\n",
      "    please set description to '{description}'\n",
      "    ```json\"\"\" + gen(stop='```')\n",
      "    return lm\n",
      "a = time.time()\n",
      "lm = llama2 + character_maker2(1, 'A nimble fighter')\n",
      "time.time() - a\n",
      "```\n",
      "<img width=\"586\" alt=\"image\" src=\"https://github.com/guidance-ai/guidance/assets/3740613/9c55500d-4c90-4f42-9343-43aa2a25efa4\"><br>\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/notebooks/api_examples/models/OpenAI.ipynb\n",
      "```ipynb\n",
      "# `OpenAI` API examples\n",
      "\n",
      "This notebook contains examples of how to use the `OpenAI` LLM.\n",
      "\n",
      "## Instruct usage\n",
      "\n",
      "```python\n",
      "from guidance import models, gen, instruction\n",
      "\n",
      "# this relies on the environment variable OPENAI_API_KEY being set\n",
      "gpt35_instruct = models.OpenAI('gpt-3.5-turbo-instruct')\n",
      "\n",
      "lm = gpt35_instruct\n",
      "with instruction():\n",
      "    lm += \"What is a popular flavor?\"\n",
      "lm += gen('flavor', max_tokens=10, stop=\".\")\n",
      "```\n",
      "\n",
      "## Chat usage\n",
      "\n",
      "```python\n",
      "from guidance import system, user, assistant\n",
      "\n",
      "# this relies on the environment variable OPENAI_API_KEY being set\n",
      "gpt35 = models.OpenAI('gpt-3.5-turbo')\n",
      "\n",
      "lm = gpt35\n",
      "\n",
      "with system():\n",
      "    lm += \"You only speak in ALL CAPS.\"\n",
      "\n",
      "with user():\n",
      "    lm += \"What is the captial of Greenland?\"\n",
      "\n",
      "with assistant():\n",
      "    lm += gen('answer', max_tokens=20)\n",
      "```\n",
      "\n",
      "<hr style=\"height: 1px; opacity: 0.5; border: none; background: #cccccc;\">\n",
      "<div style=\"text-align: center; opacity: 0.5\">Have an idea for more helpful examples? Pull requests that add to this documentation notebook are encouraged!</div>\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/notebooks/tutorials/intro_to_guidance.ipynb\n",
      "```ipynb\n",
      "# Introduction to `guidance`\n",
      "\n",
      "This notebook is a terse tutorial walkthrough of the syntax of `guidance`.\n",
      "\n",
      "## Models\n",
      "\n",
      "At the core of any guidance program are the immutable model objects. You can create an initial model object using any of the constructors under `guidance.models`: \n",
      "\n",
      "```python\n",
      "from guidance import models\n",
      "\n",
      "# For LlamaCpp, you need to provide the path on disk to a .gguf model\n",
      "# A sample model can be downloaded from\n",
      "# https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/blob/main/mistral-7b-instruct-v0.2.Q8_0.gguf\n",
      "mistral = models.LlamaCpp(\"/home/scottlundberg_google_com/models/mistral-7b-instruct-v0.2.Q8_0.gguf\", n_gpu_layers=-1, n_ctx=4096)\n",
      "\n",
      "#llama2 = models.Transformers(\"meta-llama/Llama-2-7b-hf\")\n",
      "#gpt3 = models.OpenAI(\"text-davinci-003\")\n",
      "#palm2 = models.VertexAI(\"text-bison@001\")\n",
      "```\n",
      "\n",
      "## Simple generation\n",
      "\n",
      "Once you have an initial model object you can append text to it with the addition operator. This creates a new model object that has the same context (prompt) as the original model, but with the text appended at the end (just like what would happen if you add two strings together).\n",
      "\n",
      "```python\n",
      "lm = mistral + \"Who won the last Kentucky derby and by how much?\"\n",
      "```\n",
      "\n",
      "Once you have added some text to the model you can then ask the model to generate unconstrained text using the `gen` guidance function. Guidance functions represent executable components that can be appended to a model. When you append a guidance function to a model the model extends its state by executing the guidance function.\n",
      "\n",
      "```python\n",
      "from guidance import gen\n",
      "\n",
      "lm + gen(max_tokens=10)\n",
      "```\n",
      "\n",
      "Note that while the `lm` and `mistral` objects are semantically separate, for performance purposes they share the same model weights and KV cache, so the incremental creation of new lm objects is very cheap and reuses all the computation from prior objects.\n",
      "\n",
      "We can add the text and the `gen` function in one statement to follow the traditional prompt-then-generate pattern:\n",
      "\n",
      "```python\n",
      "mistral + '''\\\n",
      "Q: Who won the last Kentucky derby and by how much?\n",
      "A:''' + gen(stop=\"Q:\")\n",
      "```\n",
      "\n",
      "## Simple templates\n",
      "\n",
      "You can define a template in `guidance` (v0.1+) using f-strings. You can interpolate both standard variables and also guidance functions. Note that in Python 3.12 you can put anything into f-string slots, but in python 3.11 and below there are a few disallowed characters (like backslash).\n",
      "\n",
      "```python\n",
      "query = \"Who won the last Kentucky derby and by how much?\"\n",
      "mistral + f'''\\\n",
      "Q: {query}\n",
      "A: {gen(stop=\"Q:\")}'''\n",
      "```\n",
      "\n",
      "## Capturing variables\n",
      "\n",
      "Often when you are building a guidance program you will want to capture specific portions of the output generated by the model. You can do this by giving a name to the element you wish to capture.\n",
      "\n",
      "```python\n",
      "query = \"Who won the last Kentucky derby and by how much?\"\n",
      "lm = mistral + f'''\\\n",
      "Q: {query}\n",
      "A: {gen(name=\"answer\", stop=\"Q:\")}'''\n",
      "```\n",
      "\n",
      "Then we can access the variable by indexing into the final model object.\n",
      "\n",
      "```python\n",
      "lm[\"answer\"]\n",
      "```\n",
      "\n",
      "## Function encapsulation\n",
      "\n",
      "When you have a set of model operations you want to group together, you can place them into a custom guidance function. To do this you define a decorated python function that takes a model as the first positional argument and returns a new updated model. You can add this guidance function to a model to execute it, just like with the built-in guidance functions like `gen`.\n",
      "\n",
      "```python\n",
      "import guidance\n",
      "\n",
      "@guidance\n",
      "def qa_bot(lm, query):\n",
      "    lm += f'''\\\n",
      "    Q: {query}\n",
      "    A: {gen(name=\"answer\", stop=\"Q:\")}'''\n",
      "    return lm\n",
      "\n",
      "query = \"Who won the last Kentucky derby and by how much?\"\n",
      "mistral + qa_bot(query) # note we don't pass the `lm` arg here (that will get passed during execution when it gets added to the model)\n",
      "```\n",
      "\n",
      "Note that one atypical feature of guidance functions is that multi-line string literals defined inside a guidance function respect the python indentation structure. This means that the whitespace before \"Q:\" and \"A:\" in the prompt above is stripped (but if they were indented 6 spaces instead of 4 spaces then only the first 4 spaces would be stripped, since that is the current python indentation level). This allows us to define multi-line templates inside guidance functions while retaining indentation readability (if you ever want to disable this behavior you can use `@guidance(dedent=False)`).\n",
      "\n",
      "## Selecting among alternatives\n",
      "\n",
      "Guidance has lots of ways to constrain model generation, but the most basic buliding block is the `select` function that forces the model to choose between a set of options (either strings or full grammars).\n",
      "\n",
      "```python\n",
      "from guidance import select\n",
      "\n",
      "mistral + f'''\\\n",
      "Q: {query}\n",
      "Now I will choose to either SEARCH the web or RESPOND.\n",
      "Choice: {select([\"SEARCH\", \"RESPOND\"], name=\"choice\")}'''\n",
      "```\n",
      "\n",
      "Note that since guidance is smart about when tokens are forced by the program (and so don't need to be predicted by the model) only one token was generated in the program above (the beginning of \"SEARCH\" that is highlighted in green).\n",
      "\n",
      "## Interleaved generation and control\n",
      "\n",
      "Because guidance is pure Python code you can interleave (constrained) generation commands with traditional python control statements. In the example below we first ask the model to decide if it should search the web or respond directly, then act accordingly.\n",
      "\n",
      "```python\n",
      "@guidance\n",
      "def qa_bot(lm, query):\n",
      "    lm += f'''\\\n",
      "    Q: {query}\n",
      "    Now I will choose to either SEARCH the web or RESPOND.\n",
      "    Choice: {select([\"SEARCH\", \"RESPOND\"], name=\"choice\")}\n",
      "    '''\n",
      "    if lm[\"choice\"] == \"SEARCH\":\n",
      "        lm += \"A: I don't know, Google it!\"\n",
      "    else:\n",
      "        lm += f'A: {gen(stop=\"Q:\", name=\"answer\")}'\n",
      "    return lm\n",
      "\n",
      "mistral + qa_bot(query)\n",
      "```\n",
      "\n",
      "## Generating lists\n",
      "\n",
      "Whenever you want to generate a list of items you can use the `list_append` parameter which will cause the captured value to be appended to a list instead of overwriting previous values.\n",
      "\n",
      "```python\n",
      "lm = mistral + f'''\\\n",
      "Q: {query}\n",
      "Now I will choose to either SEARCH the web or RESPOND.\n",
      "Choice: {select([\"SEARCH\", \"RESPOND\"], name=\"choice\")}\n",
      "'''\n",
      "if lm[\"choice\"] == \"SEARCH\":\n",
      "    lm += \"Here are 3 search queries:\\n\"\n",
      "    for i in range(3):\n",
      "        lm += f'''{i+1}. \"{gen(stop='\"', name=\"queries\", temperature=1.0, list_append=True)}\"\\n'''\n",
      "```\n",
      "\n",
      "```python\n",
      "lm[\"queries\"]\n",
      "```\n",
      "\n",
      "## Chat\n",
      "\n",
      "You can control chat models using special `with` context blocks that wrap whatever is inside them with the special formats needed for the chat model you are using. This allows you express chat programs without tying yourself to a single model backend.\n",
      "\n",
      "```python\n",
      "# to use role based chat tags you need a chat model, here we use gpt-3.5-turbo but you can use 'gpt-4' as well\n",
      "gpt35 = models.OpenAI(\"gpt-3.5-turbo\")\n",
      "```\n",
      "\n",
      "```python\n",
      "from guidance import system, user, assistant\n",
      "\n",
      "with system():\n",
      "    lm = gpt35 + \"You are a helpful assistant.\"\n",
      "\n",
      "with user():\n",
      "    lm += \"What is the meaning of life?\"\n",
      "\n",
      "with assistant():\n",
      "    lm += gen(\"response\")\n",
      "```\n",
      "\n",
      "Multistep\n",
      "\n",
      "```python\n",
      "# you can create and guide multi-turn conversations by using a series of role tags\n",
      "@guidance\n",
      "def experts(lm, query):\n",
      "    with system():\n",
      "        lm += \"You are a helpful assistant.\"\n",
      "\n",
      "    with user():\n",
      "        lm += f\"\"\"\\\n",
      "        I want a response to the following question:\n",
      "        {query}\n",
      "        Who are 3 world-class experts (past or present) who would be great at answering this?\n",
      "        Please don't answer the question or comment on it yet.\"\"\"\n",
      "\n",
      "    with assistant():\n",
      "        lm += gen(name='experts', max_tokens=300)\n",
      "    \n",
      "    with user():\n",
      "        lm += f\"\"\"\\\n",
      "        Great, now please answer the question as if these experts had collaborated in writing a joint anonymous answer.\n",
      "        In other words, their identity is not revealed, nor is the fact that there is a panel of experts answering the question.\n",
      "        If the experts would disagree, just present their different positions as alternatives in the answer itself (e.g. 'some might argue... others might argue...').\n",
      "        Please start your answer with ANSWER:\"\"\"\n",
      "    \n",
      "    with assistant():\n",
      "        lm += gen(name='answer', max_tokens=500)\n",
      "\n",
      "    return lm\n",
      "                   \n",
      "gpt35 + experts(query='What is the meaning of life?')\n",
      "```\n",
      "\n",
      "## Streaming\n",
      "\n",
      "Often you want to get the results of a generation as it is happening so you update an interface. You can do this programmatically using the `.stream()` method of model objects. This creates a `ModelStream` that you can use to accumulate updates. These updates don't get executed until you interate over then `ModelStream` object. When you iterate over the object you get lots of partially completed model objects as the guidance program is executed.\n",
      "\n",
      "```python\n",
      "for part in mistral.stream() + qa_bot(query):\n",
      "    part # do something with the partially executed lm\n",
      "```\n",
      "\n",
      "<hr style=\"height: 1px; opacity: 0.5; border: none; background: #cccccc;\">\n",
      "<div style=\"text-align: center; opacity: 0.5\">Have an idea for more helpful examples? Pull requests that add to this documentation notebook are encouraged!</div>\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/notebooks/anachronism.ipynb\n",
      "```ipynb\n",
      "# Anachronism example\n",
      "\n",
      "This example takes a <a href=\"https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/anachronisms\">simple task from BigBench</a>, where the goal is to identify whether a given sentence contains an anachronism (i.e. states something that is impossibile due to time periods).\n",
      "\n",
      "```python\n",
      "import datasets\n",
      "\n",
      "# load the data\n",
      "data = datasets.load_dataset('bigbench', 'anachronisms')\n",
      "inputs = [x.split('\\n')[0] for x in data['validation']['inputs']]\n",
      "labels = [x[0] for x in data['validation']['targets']]\n",
      "\n",
      "print(f\"Loaded {len(labels)} data items\")\n",
      "```\n",
      "\n",
      "Now, let us load a model into `guidance`:\n",
      "\n",
      "```python\n",
      "import os\n",
      "\n",
      "import guidance\n",
      "\n",
      "# define the model we will use\n",
      "# MODEL_PATH should point at the gguf file which you wish to use\n",
      "target_model_path = os.getenv(\"MODEL_PATH\")\n",
      "print(f\"Attempting to load {target_model_path}\")\n",
      "\n",
      "lm = guidance.models.LlamaCpp(target_model_path, n_gpu_layers=-1)\n",
      "```\n",
      "\n",
      "We can now define an `anachronism_query()` function.\n",
      "This is a function, decorated with `@guidance` and which contains guidance instructions.\n",
      "The first argument to a function decorated like this is always a language model, and the function returns the same model after appending whatever strings and `guidance` instructions are required.\n",
      "\n",
      "In this case, we're going to take some few-shot examples in addition to the desired query, and build them into a prompt.\n",
      "We then provide `guidance` commands to step through some chain-of-thought (CoT) reasoning.\n",
      "Notice how we use the `stop` keyword to limit the generation before the next stage in the CoT (the model may go off the rails and generate more than we want in the first 'entity' generation call otherwise).\n",
      "In the final step, we use `guidance.select` to force the model to generate a 'Yes' or 'No' answer:\n",
      "\n",
      "```python\n",
      "@guidance\n",
      "def anachronism_query(llm, query, examples):\n",
      "    prompt_string = \"\"\"Given a sentence tell me whether it contains an anachronism (i.e. whether it could have happened or\n",
      "not based on the time periods associated with the entities).\n",
      "\n",
      "Here are some examples:\n",
      "\"\"\"\n",
      "    for ex in examples:\n",
      "        prompt_string += f\"Sentence: { ex['input'] }\" + \"\\n\"\n",
      "        prompt_string += \"Entities and dates:\\n\"\n",
      "        for en in ex['entities']:\n",
      "            prompt_string += f\"{en['entity']} : {en['time']}\" + \"\\n\"\n",
      "        prompt_string += f\"Reasoning: {ex['reasoning']}\" + \"\\n\"\n",
      "        prompt_string += f\"Anachronism: {ex['answer']}\" + \"\\n\"\n",
      "\n",
      "    llm += f'''{prompt_string}\n",
      "Now determine whether the following is an anachronism:\n",
      "    \n",
      "Sentence: { query }\n",
      "Entities and dates:\n",
      "{ guidance.gen(name=\"entities\", max_tokens=100, stop=\"Reason\") }'''\n",
      "    llm += \"Reasoning :\"\n",
      "    llm += guidance.gen(name=\"reason\", max_tokens=100, stop=\"\\n\")\n",
      "    llm += f'''\\nAnachronism: { guidance.select([\"Yes\", \"No\"], name=\"answer\") }'''\n",
      "    return llm\n",
      "```\n",
      "\n",
      "We can now invoke our function with a query string and some examples.\n",
      "Again, note how when we call `anachronism_query()` we _don't_ pass in the language model itself; the `@guidance` decorator takes care of that.\n",
      "\n",
      "```python\n",
      "# define the few shot examples\n",
      "fewshot_examples = [\n",
      "    {'input': 'I wrote about shakespeare',\n",
      "    'entities': [{'entity': 'I', 'time': 'present'}, {'entity': 'Shakespeare', 'time': '16th century'}],\n",
      "    'reasoning': 'I can write about Shakespeare because he lived in the past with respect to me.',\n",
      "    'answer': 'No'},\n",
      "    {'input': 'Shakespeare wrote about me',\n",
      "    'entities': [{'entity': 'Shakespeare', 'time': '16th century'}, {'entity': 'I', 'time': 'present'}],\n",
      "    'reasoning': 'Shakespeare cannot have written about me, because he died before I was born',\n",
      "    'answer': 'Yes'}\n",
      "]\n",
      "\n",
      "# Invoke the model\n",
      "generate = lm + anachronism_query(\"The T-Rex bit my dog\", fewshot_examples)\n",
      "\n",
      "# Show the extracted generations\n",
      "print(\"entities:\\n{0}\".format(generate['entities']))\n",
      "print(f\"reasoning: {generate['reason']}\")\n",
      "print(f\"answer: {generate['answer']}\")\n",
      "```\n",
      "\n",
      "For comparison purposes, we can also define a zero-shot function:\n",
      "\n",
      "```python\n",
      "@guidance\n",
      "def anachronism_query_zeroshot(llm, query):\n",
      "    llm += f'''Given a sentence tell me whether it contains an anachronism (i.e. whether it could have happened or\n",
      "not based on the time periods associated with the entities).\n",
      "\n",
      "Sentence: {query}\n",
      "Anachronism: { guidance.select([\"Yes\", \"No\"], name=\"answer\") }\n",
      "'''\n",
      "    return llm\n",
      "\n",
      "generate_zero = lm + anachronism_query_zeroshot(\"The T-Rex bit my dog\")\n",
      "\n",
      "# Show the extracted generations\n",
      "print(f\"answer: {generate_zero['answer']}\")\n",
      "```\n",
      "\n",
      "### Compute accuracy\n",
      "\n",
      "We compute accuracy on the validation set, and compare it to using the same two-shot examples above without the output structure, as well as to the best reported result <a href=\"https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/anachronisms\">here</a>. We hope that a simple output structure will improve the accuracy of the results:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "fews = []\n",
      "zero_shot = []\n",
      "count = 0\n",
      "for input, label in zip(inputs, labels):\n",
      "    print(f\"Working on item {count}\")\n",
      "    f = lm + anachronism_query(input, fewshot_examples)\n",
      "    f = 'Yes' if 'Yes' in f['answer'] else 'No'\n",
      "    fews.append(f)\n",
      "    g = lm + anachronism_query_zeroshot(input)\n",
      "    g = 'Yes' if 'Yes' in g['answer'] else 'No'\n",
      "    zero_shot.append(g)\n",
      "    count += 1\n",
      "fews = np.array(fews)\n",
      "zero_shot = np.array(zero_shot)\n",
      "```\n",
      "\n",
      "Now, we can compute the accuracy for each of the approaches:\n",
      "\n",
      "```python\n",
      "print('Few-shot', (np.array(labels) == fews).mean())\n",
      "print('Zero-shot', (np.array(labels) == zero_shot).mean())\n",
      "```\n",
      "\n",
      "<hr style=\"height: 1px; opacity: 0.5; border: none; background: #cccccc;\">\n",
      "<div style=\"text-align: center; opacity: 0.5\">Have an idea for more helpful examples? Pull requests that add to this documentation notebook are encouraged!</div>\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/notebooks/art_of_prompt_design/prompt_boundaries_and_token_healing.ipynb\n",
      "```ipynb\n",
      "`The Art of Prompt Design`\n",
      "\n",
      "\n",
      "# Prompt Boundaries and Token Healing\n",
      "\n",
      "This (written jointly with <a href=\"https://medium.com/@marcotcr\">Marco Tulio Ribeiro</a>) is part 2 of a series on <b>the art of prompt design</b> (part 1 <a href=\"https://medium.com/towards-data-science/the-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5\">here</a>), where we talk about controlling large language models (LLMs) with <a href=\"https://github.com/microsoft/guidance\">`guidance`</a>.\n",
      "\n",
      "In this post, we'll discuss how the greedy tokenization methods used by language models can introduce unintended token splits into your prompts, leading to puzzling generations.\n",
      "\n",
      "Language models are not trained on raw text, but rather on tokens, which are chunks of text that often occur together, similar to words. This impacts how language models 'see' text, including prompts (since prompts are just sets of tokens). GPT-style models utilize tokenization methods like [Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) (BPE), which map all input bytes to token ids in an optimized/greedy manner. This is fine for training, but it can lead to subtle issues during inference, as shown in the example below.\n",
      "\n",
      "<!-- TODO\n",
      "Standard greedy token mapping works well during training, but it can lead to subtle issues during prompting and inference. These issues arise because the greedy token boundaries often don't line up with the end of the prompt, especially when considering the generated tokens that will come next. While the end of a prompt will always align with a token boundary in practice, as the prompt is tokenized before being extended by the model, there may be instances where the first characters of the completion are part of a longer token that would span the prompt boundary. In such cases, the longer token cannot be used even though the model would expect it based on the training data.\n",
      "\n",
      "The inability to use tokens that span prompt boundaries can lead to subtle yet important biases in the model's output. -->\n",
      "\n",
      "\n",
      "## An example of a prompt boundary problem\n",
      "Consider the following example, where we are trying to generate an HTTP URL string:\n",
      "\n",
      "```python\n",
      "import transformers\n",
      "\n",
      "# we use StableLM as an example, but these issues impact all models to varying degrees\n",
      "generator = transformers.pipeline('text-generation', model='stabilityai/stablelm-base-alpha-3b')\n",
      "\n",
      "def raw_gen(prompt, temp=0):\n",
      "    kwargs = {}\n",
      "    if temp > 0:\n",
      "        kwargs[\"temperature\"] = temp\n",
      "        kwargs[\"do_sample\"] = True\n",
      "    return generator(prompt, max_new_tokens=10, pad_token_id=0, **kwargs)[0][\"generated_text\"]\n",
      "raw_gen('The link is <a href=\"http:')\n",
      "```\n",
      "\n",
      "```python\n",
      "import transformers\n",
      "\n",
      "# we use StableLM as an example, but these issues impact all models to varying degrees\n",
      "generator = transformers.pipeline('text-generation', model='stabilityai/stablelm-base-alpha-3b')\n",
      "\n",
      "def raw_gen(prompt):\n",
      "    return generator(prompt, max_new_tokens=10, pad_token_id=0)[0][\"generated_text\"]\n",
      "raw_gen('The link is <a href=\"http:')\n",
      "```\n",
      "\n",
      "Note that the output generated by the LLM does not complete the url with the obvious next characters (two forward slashes). It instead creates an invalid URL string with a space in the middle. This is surprising, because the `//` completion is extremely obvious after `http:`. To understand why this happens, let's change our prompt boundary so that our prompt does not include the colon character:\n",
      "\n",
      "```python\n",
      "raw_gen('The link is <a href=\"http')\n",
      "```\n",
      "\n",
      "Now the language model generates a valid url string like we expect. To understand why the  `:` matters, we need to look at the tokenized representation of the prompts. Below is the tokenization of the prompt that ends in a colon (the prompt without the colon has the same tokenization, except for the last token):\n",
      "\n",
      "```python\n",
      "def print_tokens(tokens):\n",
      "    print(\"len = \" + str(len(tokens)))\n",
      "    for i in tokens:\n",
      "        print(str(i) + \"\\t`\" + generator.tokenizer.decode([i]) + \"`\")\n",
      "\n",
      "print_tokens(generator.tokenizer.encode('The link is <a href=\"http:'))\n",
      "```\n",
      "\n",
      "Now note what the tokenization of a valid URL looks like, paying careful attention to token `1358`, right after `http`:\n",
      "\n",
      "```python\n",
      "print_tokens(generator.tokenizer.encode('The link is <a href=\"http://www.google.com/search?q'))\n",
      "```\n",
      "\n",
      "This particular LLM uses a greedy/optimized tokenization method, almost always preferring the longest possible token, i.e. `://` will be preferred over `:` in full text (e.g. in training).\n",
      "\n",
      "While URLs in training are encoded with token 1358 (`://`), our prompt makes the LLM see token `27` (`:`) instead, which throws off completion by artificially splitting `://`.\n",
      "In fact, the model can be pretty sure that seeing token `27` (`:`) means what comes next is very unlikely to be anything that could have been encoded together with the colon using a \"longer token\" like `://`, since in the model's training data those characters would have been encoded together with the colon (an exception to this that we will discuss later is <a href=\"https://arxiv.org/abs/1804.10959\">subword regularization</a> during training). The fact that seeing a token means both seeing the embedding of that token **and also** that whatever comes next wasn't compressed by the greedy tokenizer is easy to forget, but it is important in prompt boundaries.\n",
      "\n",
      "Let's search over the string representation of all the tokens in the model's vocabulary, to see which ones start with a colon:\n",
      "\n",
      "```python\n",
      "tokens = generator.tokenizer.convert_ids_to_tokens(range(generator.tokenizer.vocab_size))\n",
      "colon_tokens = [i for i,t in enumerate(tokens) if t.startswith(\":\")]\n",
      "print_tokens(colon_tokens)\n",
      "```\n",
      "\n",
      "Note that there are **34** different tokens starting with a colon, and thus ending a prompt with a colon means the model will likely not generate completions with any of these 34 token strings. *This subtle and powerful bias can have all kinds of unintended consequences.* And this applies to **any** string that could be potentially extended to make a longer single token (not just `:`).  Even our \"fixed\" prompt ending with \"http\" has a built in bias as well, as it communicates to the model that what comes after \"http\" is likely not \"s\" (otherwise \"http\" would not have been encoded as a separate token):\n",
      "\n",
      "```python\n",
      "http_tokens = [i for i,t in enumerate(tokens) if t.startswith(\"http\")]\n",
      "print_tokens(http_tokens)\n",
      "```\n",
      "\n",
      "Lest you think this is an arcane problem that only touches URLs, remember that most tokenizers treat tokens differently depending on whether they start with a space, punctuation, quotes, etc, and thus **ending a prompt with any of these can lead to wrong token boundaries**, and break things:\n",
      "\n",
      "```python\n",
      "# Accidentally adding a space, will lead to weird generation\n",
      "raw_gen('I read a book about ')\n",
      "```\n",
      "\n",
      "```python\n",
      "# No space, works as expected\n",
      "raw_gen('I read a book about')\n",
      "```\n",
      "\n",
      "Another example of this is the \"[\" character. Consider the following prompt and completion:\n",
      "\n",
      "```python\n",
      "# guidance('''An example [\"like this\"] and another example [{{gen max_tokens=10 token_healing=False}}''', caching=False)()\n",
      "raw_gen('An example [\"like this\"] and another example [')\n",
      "```\n",
      "\n",
      "Why is the second string not quoted? Because by ending our prompt with the ' [' token, we are telling the model that it should not generate completions that match the following 27 longer tokens (one of which adds the quote character, `15640`):\n",
      "\n",
      "```python\n",
      "space_bracket_tokens = [i for i,t in enumerate(tokens) if t .startswith(\"[\")] # note the  is converted to a space by the tokenizer\n",
      "print_tokens(space_bracket_tokens)\n",
      "```\n",
      "\n",
      "Token boundary bias happens everywhere. *About 70% of the 10k most common tokens for the StableLM model used above are prefixes of longer possible tokens, and so cause token boundary bias when they are the last token in a prompt.* Keeping track of all these possible extension biases during prompt design is impractical so most people just ignore them.\n",
      "\n",
      "```python\n",
      "# count the number of tokens that have longer extensions\n",
      "count = 0\n",
      "for i in range(10000):\n",
      "    m = 0\n",
      "    for j in range(generator.tokenizer.vocab_size):\n",
      "        if tokens[j].startswith(tokens[i]):\n",
      "            m += 1\n",
      "        if m > 1:\n",
      "            break\n",
      "    # m = guidance.llm.prefix_matches(guidance.llm.decode([i]))\n",
      "    if m > 1:\n",
      "        count += 1\n",
      "print(str(100*count/10000)+\"%\")\n",
      "```\n",
      "\n",
      "## Fixing unintended bias with \"token healing\"\n",
      "\n",
      "What can we do to avoid these unintended biases? One option is to always end our prompts with tokens that cannot be extended into longer tokens (for example a role tag for chat-based models), but this is a severe limitation.  \n",
      "\n",
      "Instead, `guidance` has a feature called \"token healing\", which automatically backs up the generation process by one token before the end of the prompt, then constrains the first token generated to have a prefix that matches the last token in the prompt. In our URL example, this would mean removing the `:`, and forcing generation of the first token to have a `:` prefix.   \n",
      "Token healing allows users to express prompts however they wish, without worrying about token boundaries.\n",
      "\n",
      "For example, let's re-run some of the URL examples above with token healing turned on (it's on by default for Transformer models, so we remove `token_healing=False`):\n",
      "\n",
      "```python\n",
      "from guidance import models, gen\n",
      "\n",
      "# load StableLM from huggingface\n",
      "lm = models.Transformers(\"stabilityai/stablelm-base-alpha-3b\", device=0)\n",
      "\n",
      "# With token healing we generate valid URLs, even when the prompt ends with a colon:\n",
      "lm + 'The link is <a href=\"http:' + gen(max_tokens=10)\n",
      "```\n",
      "\n",
      "```python\n",
      "[str(lm + 'The link is <a href=\"http' + gen(max_tokens=10, temperature=1)) for i in range(10)]\n",
      "```\n",
      "\n",
      "Similarly, we don't have to worry about extra spaces:\n",
      "\n",
      "```python\n",
      "\n",
      "# Accidentally adding a space will not impact generation\n",
      "lm + 'I read a book about ' + gen(max_tokens=5)\n",
      "```\n",
      "\n",
      "```python\n",
      "# This will generate the same text as above \n",
      "lm + 'I read a book about' + gen(max_tokens=6)\n",
      "```\n",
      "\n",
      "Similarly, we now get quoted strings even when the prompt ends with a \" [\" token:\n",
      "\n",
      "```python\n",
      "lm + 'An example [\"like this\"] and another example [' + gen(max_tokens=10)\n",
      "```\n",
      "\n",
      "## What about subword regularization?\n",
      "\n",
      "If you are familiar with how language models are trained, you may be wondering how <a href=\"https://arxiv.org/abs/1804.10959\">subword regularization</a> fits into all this. Subword regularization is a technique where during training sub-optimial tokenizations are randomly introduced to increase the model's robustness to token boundary issues. This means that the model does not always see the best tokenization. Subword regularization is great at helping the model be more robust to token boundaries, but it does not remove the bias that the model has towards the standard optimized (near greedy) tokenization. This means that while depending on the amount of subword regularization during training models may exhibit more or less token boundaries bias, all models still have this bias. And as shown above it can still have a powerful and unexpected impact on the model output.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "When you write prompts, remember that greedy tokenization can have a significant impact on how language models interpret your prompts, particularly when the prompt ends with a token that could be extended into a longer token. This easy-to-miss source of bias can impact your results in surprising and unintended ways.\n",
      "\n",
      "To address to this, either end your prompt with a non-extendable token, or use something like `guidance`'s \"token healing\" feature so you can to express your prompts however you wish, without worrying about token boundary artifacts. \n",
      "\n",
      "## Appendix: Did we just get unlucky with the link example?\n",
      "\n",
      "No, and random sampling can verify that:\n",
      "\n",
      "```python\n",
      "# with the colon we almost always get an invalid link\n",
      "[raw_gen('The link is <a href=\"http:', temp=1) for _ in range(5)]\n",
      "```\n",
      "\n",
      "```python\n",
      "# without the colon we always get a valid link\n",
      "[raw_gen('The link is <a href=\"http', temp=1) for _ in range(5)]\n",
      "```\n",
      "\n",
      "<hr style=\"height: 1px; opacity: 0.5; border: none; background: #cccccc;\">\n",
      "<div style=\"text-align: center; opacity: 0.5\">Have an idea for more helpful examples? Pull requests that add to this documentation notebook are encouraged!</div>\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "## File: /Users/nicholasking/code/ms/guidance/notebooks/proverb.ipynb\n",
      "```ipynb\n",
      "# GPT Proverb\n",
      "\n",
      "This is a fun example of using GPT-3 to adapt proverbs to a new domain.\n",
      "\n",
      "Guidance programs have a well defined linear execution order that directly corresponds to the token order as processed by the language model. This means that at any point during execution the language model can be used to generate text (using the `gen()` command) or make logical control flow decisions. This interleaving of generation and prompting allows for precise output structure that produces clear and parsable results.\n",
      "\n",
      "```python\n",
      "import guidance\n",
      "from guidance import models, gen, newline\n",
      "\n",
      "# set the default language model used to execute guidance programs\n",
      "gpt3 = models.OpenAI(\"text-davinci-003\")\n",
      "\n",
      "# Define a guidance program that adapts proverbs.\n",
      "@guidance\n",
      "def program(lm, proverb, book, chapter, verse):\n",
      "    lm += f\"\"\"\\\n",
      "    Tweak this proverb to apply to model instructions instead.\n",
      "\n",
      "    {proverb}\n",
      "    - {book} {chapter}:{verse}\n",
      "\n",
      "    UPDATED\n",
      "    Where there is no guidance{gen('rewrite', stop=newline + \"-\")}\n",
      "    - GPT {gen('chapter', stop=\":\")}:{gen('verse', regex=\"[0-9]+\")}\"\"\"\n",
      "    return lm\n",
      "\n",
      "# execute the program on a specific proverb\n",
      "lm = gpt3 + program(\n",
      "    proverb=\"Where there is no guidance, a people falls,\\nbut in an abundance of counselors there is safety.\",\n",
      "    book=\"Proverbs\",\n",
      "    chapter=11,\n",
      "    verse=14\n",
      ")\n",
      "```\n",
      "\n",
      "```python\n",
      "lm[\"rewrite\"]\n",
      "```\n",
      "\n",
      "<hr style=\"height: 1px; opacity: 0.5; border: none; background: #cccccc;\">\n",
      "<div style=\"text-align: center; opacity: 0.5\">Have an idea for more helpful examples? Pull requests that add to this documentation notebook are encouraged!</div>\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
