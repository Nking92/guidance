{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "import os\n",
    "from guidance import models, gen, any_char, any_char_but, regex, substring, substring_no_empty, with_temperature, system, user, assistant\n",
    "from typing import Optional\n",
    "import nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral = models.LlamaCpp(\"/Users/nicholasking/code/models/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf\", n_gpu_layers=-1, n_ctx=4096)\n",
    "\n",
    "model = os.getenv(\"AZUREAI_CHAT_MODEL\", \"Please set the model\")\n",
    "azure_endpoint = os.getenv(\"AZUREAI_CHAT_ENDPOINT\", \"Please set the endpoint\")\n",
    "api_key=os.getenv(\"AZUREAI_CHAT_KEY\", \"Please set API key\")\n",
    "\n",
    "gpt4 = models.AzureOpenAI(\n",
    "    model=model,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import guidance\n",
    "\n",
    "def extract_text_from_ipynb(notebook_file):\n",
    "    nb = nbformat.read(notebook_file, as_version=4)\n",
    "    extracted_text = \"\"\n",
    "    for cell in nb['cells']:\n",
    "        if cell['cell_type'] == 'code':\n",
    "            extracted_text += \"```python\\n\" + cell['source'] + \"\\n```\\n\\n\"\n",
    "        elif cell['cell_type'] == 'markdown':\n",
    "            extracted_text += cell['source'] + \"\\n\\n\"\n",
    "    return extracted_text\n",
    "\n",
    "def walk_and_match_files(start_path, include_file_regex=None, exclude_file_regex=None):\n",
    "    \"\"\"Walk through directories starting from start_path and collect files that match include_file_regex and don't match exclude_file_regex.\"\"\"\n",
    "    matched_files = []\n",
    "    for root, _, files in os.walk(start_path):\n",
    "        for file_name in files:\n",
    "            if include_file_regex is None or re.match(include_file_regex, file_name):\n",
    "                if exclude_file_regex is None or not re.match(exclude_file_regex, file_name):\n",
    "                    matched_files.append(os.path.join(root, file_name))\n",
    "    return matched_files\n",
    "\n",
    "def read_files(file_paths):\n",
    "    \"\"\"Read the contents of the matched files.\"\"\"\n",
    "    file_contents = {}\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            if file_path.endswith('.ipynb'):\n",
    "                file_contents[file_path] = extract_text_from_ipynb(file_path)\n",
    "            else:\n",
    "                file_contents[file_path] = f.read()\n",
    "    return file_contents\n",
    "\n",
    "def format_for_analysis(file_contents):\n",
    "    \"\"\"Format the contents for model prompting.\"\"\"\n",
    "    formatted_string = \"\"\n",
    "    for file_path, content in file_contents.items():\n",
    "        formatted_string += f\"## File: {file_path}\\n```{file_path.split('.')[-1]}\\n{content}\\n```\\n\\n\"\n",
    "    return formatted_string\n",
    "\n",
    "# Orchestrator\n",
    "def build_code_prompt(repo_paths, include_file_regex=None, exclude_file_regex=None):\n",
    "    \"\"\"Orchestrate the analysis of a repository.\"\"\"\n",
    "    all_file_paths = []\n",
    "    for start_path in repo_paths:\n",
    "        all_file_paths.extend(walk_and_match_files(start_path, include_file_regex, exclude_file_regex))\n",
    "    \n",
    "    all_file_contents = read_files(all_file_paths)\n",
    "    formatted_code = format_for_analysis(all_file_contents)\n",
    "    prompt = f\"\"\"# Code Analysis\n",
    "Please analyze the code provided below.\n",
    "\n",
    "{formatted_code}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "@guidance\n",
    "def analyze_code(lm, code_prompt: str, system_prompt: Optional[str] = None, user_message: Optional[str] = None, **kwargs):\n",
    "    kwargs.setdefault('temperature', 0.8)\n",
    "    kwargs.setdefault('max_tokens', 1000)\n",
    "    if isinstance(lm, models.Chat):\n",
    "        if system_prompt is None:\n",
    "            system_prompt = \"Provide insights into code quality, potential issues, and suggestions for improvement. Answer any questions the user has.\"\n",
    "        with system():\n",
    "            lm += system_prompt\n",
    "        with user():\n",
    "            lm += code_prompt\n",
    "            if user_message is not None:\n",
    "                lm += f\"\\n# User Message\\n{user_message}\"\n",
    "        with assistant():\n",
    "            lm += gen(**kwargs)\n",
    "    else:\n",
    "        lm += code_prompt\n",
    "        if system_prompt is not None:\n",
    "            lm += f\"\\n# Instructions\\n{system_prompt}\"\n",
    "        if user_message is not None:\n",
    "            lm += f\"\\n# User Message\\n{user_message}\"\n",
    "        # Set default temperature and max_tokens in kwargs\n",
    "        lm += gen(**kwargs)\n",
    "    return lm\n",
    "\n",
    "# Example usage\n",
    "repo_paths = ['/path/to/repo1', '/path/to/repo2']\n",
    "file_regex = r'\\.(py)$'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
